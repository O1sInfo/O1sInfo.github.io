{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/eas.png","path":"images/eas.png","modified":0,"renderable":0},{"_id":"source/images/my_image.jpg","path":"images/my_image.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.jpeg","path":"images/avatar.jpeg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"source/images/res1.PNG","path":"images/res1.PNG","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"source/images/LlayerNN.png","path":"images/LlayerNN.png","modified":0,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"source/images/动态规划.jpg","path":"images/动态规划.jpg","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"source/images/minibatch.png","path":"images/minibatch.png","modified":0,"renderable":0},{"_id":"source/images/partition.png","path":"images/partition.png","modified":0,"renderable":0},{"_id":"source/images/shuffle.png","path":"images/shuffle.png","modified":0,"renderable":0},{"_id":"source/images/sgd.png","path":"images/sgd.png","modified":0,"renderable":0},{"_id":"source/images/momentum.png","path":"images/momentum.png","modified":0,"renderable":0},{"_id":"source/images/adam.PNG","path":"images/adam.PNG","modified":0,"renderable":0}],"Cache":[{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1533602185347},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1533602185348},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1533602185347},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1533602185352},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1533602185353},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1533602185355},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1533602185354},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1533602185356},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1533602185356},{"_id":"themes/next/README.cn.md","hash":"23e92a2599725db2f8dbd524fbef2087c6d11c7b","modified":1533602185357},{"_id":"themes/next/README.md","hash":"50abff86ffe4113051a409c1ed9261195d2aead0","modified":1533602185358},{"_id":"themes/next/_config.yml","hash":"70a30c95b33b0833feebf579410f2043e1a8fec4","modified":1533602185359},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1533602185359},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1533602185360},{"_id":"themes/next/package.json","hash":"3963ad558a24c78a3fd4ef23cf5f73f421854627","modified":1533602185438},{"_id":"source/_posts/Evolutionary-Algorithms.md","hash":"ed4b9fa58dce005de7894dfc88b27cdb0cd4a0c7","modified":1533602185220},{"_id":"source/_posts/Genetic-Algorithms.md","hash":"dcdf9134d8883b1761660673c4727eb14fca42ac","modified":1533602185221},{"_id":"source/_posts/DNN应用1-识别猫.md","hash":"f5c73273f9457bdcf657cada51e295c0e4c30008","modified":1533602185220},{"_id":"source/_posts/How-to-set-up-a-blog-with-hexo-on-github-io.md","hash":"b2d62d7d9a8e27d598ac19144990ec4b6e184ab7","modified":1533602185222},{"_id":"source/_posts/dropout.md","hash":"b2d5d8715432d5129c3aa25dbcca12091e0321a5","modified":1533602185223},{"_id":"source/_posts/hello-world.md","hash":"b4f283c1f275e64526f5fc48cbe315056937d25b","modified":1531932593871},{"_id":"source/_posts/python内置小工具.md","hash":"2b00d8961b498c4dcd52620514fe44bd4ec32fae","modified":1533602185225},{"_id":"source/_posts/人生-路遥.md","hash":"c7fb84ff2d39595f07edd0fb8279d7f7f1341c05","modified":1533602185225},{"_id":"source/_posts/初始化参数.md","hash":"e7a9718f850bbb2118f7bb2dd7c081d0053cc075","modified":1533602185226},{"_id":"source/_posts/动态规划.md","hash":"6808f1f158916d3969678bb88c1f18bf48e30aa7","modified":1533602185226},{"_id":"source/_posts/十个策略故事.md","hash":"b2a623b9b7c150872b3c66dd7a7b0a9942eed343","modified":1533602185227},{"_id":"source/_posts/恋爱领域中普遍存在的贬低倾向.md","hash":"ce44fbf0731011e1435134d99e367f77c324ba92","modified":1533602185230},{"_id":"source/_posts/数据划分.md","hash":"c2ea3bca5e73449cd52d02d335ca78835dfc4625","modified":1533602185231},{"_id":"source/_posts/数据的加载-预处理-可视化.md","hash":"b933bf8b8b675b14bc00df01d5e3ba182b54c21f","modified":1533602185232},{"_id":"source/_posts/标准化输入.md","hash":"a897c83d52ba2740e83c65c03d18e7178c67e5dc","modified":1533602185233},{"_id":"source/_posts/梯度检验.md","hash":"fb3e6dc11787ba9b9fb344ee7d0ea5a85743feea","modified":1533602185234},{"_id":"source/_posts/梯度消失和梯度爆炸.md","hash":"87f54770525d2624c43f5eb85f8dfd305374f7fb","modified":1533602185234},{"_id":"source/_posts/模型估计.md","hash":"7d352603141e287845ad68d99266fae98eef5f9d","modified":1533602185235},{"_id":"source/_posts/正则化.md","hash":"fcd221c3db2b25ffd29738358399fec85ac3bedf","modified":1533602185236},{"_id":"source/_posts/github使用手册.md","hash":"0a47a341a124c82322d4ad91955e92a6f71856b3","modified":1533602185223},{"_id":"source/_posts/男人的对象选择中的一种特殊类型.md","hash":"8fd8d3a299b09b23e87f349de61fe7e90c4886c1","modified":1533602185238},{"_id":"source/_posts/短诗三首.md","hash":"dd52d5906a235c18f78bf1a8476ede9dfcdea976","modified":1533602185238},{"_id":"source/_posts/布雷默曼极限.md","hash":"d70eba8626ab837bbd1cd2061cd8682fe319ba69","modified":1533602185228},{"_id":"source/_posts/贞洁禁忌.md","hash":"fbcb884e5930f5a93c21a803befbc1b0e83fb49d","modified":1533602185240},{"_id":"source/categories/index.md","hash":"a8ee8e11f9d453a17eb3467620d7b43ff97856b3","modified":1533602185241},{"_id":"source/_posts/深度学习中的优化算法.md","hash":"f16e813ad88a268779cc2010e4f1be080e326aa4","modified":1533602185237},{"_id":"source/_posts/神经网络中的通用函数代码.md","hash":"16d9af986fa36cb422e9e7e439b5b2325b409835","modified":1533602185239},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1533602185352},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1533602185349},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"a0a82dbfabdef9a9d7c17a08ceebfb4052d98d81","modified":1533602185350},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1533602185350},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1523677104000},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1533602185361},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1533602185361},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1533602185363},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1533602185362},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1533602185364},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1533602185363},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1533602185365},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1533602185365},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1533602185366},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1533602185367},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1533602185367},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1533602185371},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1533602185368},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1533602185374},{"_id":"themes/next/languages/zh-Hans.yml","hash":"66b9b42f143c3cb2f782a94abd4c4cbd5fd7f55f","modified":1533602185372},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1533602185375},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1533602185378},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1533602185433},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1533602185434},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1533602185434},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1533602185435},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1533602185436},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1533602185436},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1533602185437},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1533602185439},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1533602185440},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1533602185655},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1533602185656},{"_id":"source/images/eas.png","hash":"9d12e21703b5d1dab464bd5f8f0d2f194145b43c","modified":1533461374355},{"_id":"source/images/my_image.jpg","hash":"931bed91b176c20eee98be0a6f0af356d37b2330","modified":1531227952704},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1533602185654},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1533602185375},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1533602185376},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1533602185379},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1533602185380},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1533602185381},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1533602185382},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9c7343fd470e0943ebd75f227a083a980816290b","modified":1533602185382},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1533602185383},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1533602185384},{"_id":"themes/next/layout/_partials/footer.swig","hash":"26e93336dc57a39590ba8dc80564a1d2ad5ff93b","modified":1533602185385},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1533602185386},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1533602185388},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1533602185389},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1533602185390},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1533602185390},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1533602185400},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1533602185401},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1533602185405},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1533602185423},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1533602185424},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1533602185424},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1533602185425},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1533602185426},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1533602185426},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1533602185427},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1533602185441},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1533602185441},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1533602185442},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1533602185443},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1533602185445},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1533602185446},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1533602185448},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1533602185453},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1533602185454},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1533602185543},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1533602185544},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1523677104000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1523677104000},{"_id":"themes/next/source/images/avatar.jpeg","hash":"f1aa09fc418f5d6f05af58e81a9ca95790a075b0","modified":1531930547907},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1533602185547},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1533602185547},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1533602185548},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1533602185549},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1533602185549},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1533602185550},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1533602185551},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1523677104000},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1523677104000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523677104000},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1533602185553},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1523677104000},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1533602185555},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1533602185555},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1523677104000},{"_id":"source/images/res1.PNG","hash":"f528eca6822e71539139aa1b2795847135dc2b07","modified":1533260349819},{"_id":"source/tags/index.md","hash":"a75687420abbf3ff738d0fea7fe2634cbd4340e9","modified":1533602185251},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1523677104000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1533602185387},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1533602185391},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1533602185392},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1533602185393},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1533602185394},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1533602185394},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1533602185397},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1533602185402},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1533602185403},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1533602185404},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1533602185406},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1533602185407},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1533602185408},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1533602185408},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1533602185409},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1533602185410},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1533602185410},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1533602185411},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1533602185412},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1533602185413},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1533602185413},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1533602185414},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1533602185415},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1533602185415},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1533602185416},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1533602185417},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1533602185418},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1533602185419},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1533602185419},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1533602185420},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1533602185421},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1533602185422},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1533602185430},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1533602185431},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1533602185431},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1533602185432},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1533602185392},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1533602185388},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1533602185517},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1533602185518},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1533602185514},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1533602185538},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1533602185539},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1533602185542},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1533602185557},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1533602185558},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1533602185559},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1533602185558},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1533602185560},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1533602185561},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1533602185561},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1533602185562},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1533602185564},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1533602185564},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1533602185565},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1523677104000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1523677104000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1533602185582},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1523677104000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1533602185595},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1533602185596},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1533602185597},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1533602185598},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1533602185600},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1533602185601},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1533602185602},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1533602185602},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1533602185603},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1533602185515},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1533602185541},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1533602185620},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1533602185622},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1533602185623},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1533602185621},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1533602185624},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1533602185624},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1533602185626},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1533602185628},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1523677104000},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1533602185638},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1533602185639},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1533602185640},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1533602185641},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1533602185626},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1533602185648},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1533602185649},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1533602185652},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1533602185653},{"_id":"source/images/LlayerNN.png","hash":"939d9808c2d757e7097f22cbb4b4083c40f3da9c","modified":1531227952667},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1533602185654},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1533602185619},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1533602185618},{"_id":"source/images/动态规划.jpg","hash":"771e3ceb5404dcb7ef349c6aa6e7853a7d61934e","modified":1532186630097},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1533602185428},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1533602185456},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1533602185457},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1533602185459},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1533602185473},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1533602185493},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1533602185459},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1533602185460},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1533602185509},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1533602185510},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1533602185511},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1533602185512},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1533602185513},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1533602185508},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1533602185519},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1533602185511},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1533602185520},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1533602185521},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1533602185522},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1533602185524},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1533602185525},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1533602185526},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1533602185527},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1533602185529},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1533602185530},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e695e58f714129ca292c2e54cd62c251aca7f7fe","modified":1533602185530},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1533602185531},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1533602185532},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1533602185533},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1533602185534},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1533602185536},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"ad2dcedf393ed1f3f5afd2508d24969c916d02fc","modified":1533602185536},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1533602185537},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1533602185535},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1533602185563},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1533602185571},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1533602185573},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1533602185575},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1533602185593},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1533602185594},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1533602185595},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1533602185599},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1533602185605},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1523677104000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1533602185604},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1533602185606},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1533602185647},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1533602185646},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1533602185429},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1533602185572},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1523677104000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1523677104000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1533602185651},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1533602185463},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1533602185465},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1533602185465},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1533602185461},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1533602185462},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1533602185466},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1533602185467},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1533602185468},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1533602185469},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1533602185470},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1533602185471},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1533602185472},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1533602185472},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1533602185485},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1533602185486},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1533602185487},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1533602185488},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1533602185489},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1533602185489},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1533602185490},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1533602185491},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1533602185491},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1533602185492},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1533602185474},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1533602185475},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1533602185476},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1533602185476},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1533602185477},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1533602185478},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1533602185479},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1533602185479},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1533602185480},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1533602185481},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1533602185482},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1533602185482},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1533602185483},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1533602185484},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1533602185484},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1533602185494},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1533602185494},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1533602185495},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1533602185496},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1533602185497},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1533602185497},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1533602185498},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1533602185499},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1533602185500},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1533602185500},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1533602185501},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1533602185502},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1533602185503},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1533602185503},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1533602185504},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1533602185505},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1533602185506},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1533602185506},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1533602185507},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1533602185527},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1533602185528},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1533602185533},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1523677104000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1523677104000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1523677104000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1523677104000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1533602185589},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1533602185590},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1533602185591},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1533602185592},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1523677104000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1523677104000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1533602185592},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1533602185580},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1523677104000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1533602185463},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1533602185645},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1533602185613},{"_id":"public/categories/index.html","hash":"b38c73975965a30beb6a5986f4e78b3be6a23f77","modified":1533605178035},{"_id":"public/tags/index.html","hash":"92e7a0fbb00dd77d02b608c2def906b5376a8075","modified":1533607181492},{"_id":"public/tags/DNN应用/index.html","hash":"ddc9dd9ce9e762b8ed689c38d63aa7916231f3f0","modified":1533605178037},{"_id":"public/tags/hexo/index.html","hash":"5f8ecb51f828bb3e9f3fb6c25629a7ae3fa64866","modified":1533605178037},{"_id":"public/tags/dropout/index.html","hash":"ea043c21a2058d4dedb128b225ca7e76bfe6b4ee","modified":1533605178037},{"_id":"public/tags/程序员实用工具/index.html","hash":"e0ab632991b8f6b3b02feef623f49c53e05cd727","modified":1533605178037},{"_id":"public/tags/路遥-人生/index.html","hash":"7c5dcbc547c7eeddf1f170f9c9e076da0939ab31","modified":1533605178037},{"_id":"public/tags/初始化参数/index.html","hash":"2c224f0f64f870243310f6af15659cf7e570b139","modified":1533605178037},{"_id":"public/tags/动态规划/index.html","hash":"a24e915fb227261f24b91c0e70e94a0a67e982b7","modified":1533605178037},{"_id":"public/tags/策略游戏/index.html","hash":"d5a8b071392532989fde1652d79e419efe9afc21","modified":1533605178037},{"_id":"public/tags/爱情心理学/index.html","hash":"51ae7300548cccca0a77ba6634b073b0fa9d5287","modified":1533605178037},{"_id":"public/tags/数据划分/index.html","hash":"3d1ec6759dd38b9806172ac8dcc64d39a4c6bbfa","modified":1533605178038},{"_id":"public/tags/通用数据操作/index.html","hash":"9739808f19e5ad7a87a56419849e74a9ef483623","modified":1533605178038},{"_id":"public/tags/标准化输入/index.html","hash":"97d461c9cf208c2ea939e0cc1898f8e337e14230","modified":1533605178038},{"_id":"public/tags/梯度检验/index.html","hash":"a3218710b86d00860ba6456c3b0096d600e5db1e","modified":1533605178038},{"_id":"public/tags/梯度消失和梯度爆炸/index.html","hash":"f2bcbc59c430d72942eb4be8cf5af03be261f54e","modified":1533605178038},{"_id":"public/tags/模型估计/index.html","hash":"f6eb0960f2d0fb1efccae61f1455d48026b28fae","modified":1533605178038},{"_id":"public/tags/正则化/index.html","hash":"0a84f5c79faca6afe24f82db814bc447a1d23da3","modified":1533605178038},{"_id":"public/tags/git/index.html","hash":"8f9865090cc377d759fa13a28cc4a43ef1c6d55a","modified":1533605178038},{"_id":"public/tags/现代诗/index.html","hash":"645144380befe339f41238bfddc398dff1669e7e","modified":1533605178038},{"_id":"public/tags/计算限制/index.html","hash":"adf62f030e1ebbfb67b2608576f05378693f7d45","modified":1533605178038},{"_id":"public/tags/优化算法/index.html","hash":"009544eef1964d3afd01edb2edbeec33240a7b0c","modified":1533607181493},{"_id":"public/tags/神经网络/index.html","hash":"5bf465480406bd01c040f399c64dfae35e50928a","modified":1533605178038},{"_id":"public/tags/通用函数代码/index.html","hash":"56292bc0491d1e7a3688666ac7f77f394964e65d","modified":1533605178038},{"_id":"public/archives/page/3/index.html","hash":"8352298080e0662032dc60e687c1028e8786427a","modified":1533605178040},{"_id":"public/archives/2018/page/3/index.html","hash":"9f9d18bf66073e8ebd49f17998012dfaf786b254","modified":1533605178041},{"_id":"public/categories/深度学习/index.html","hash":"7e2c579d90c516eaf24f9d7c15a86fcb6c6f7480","modified":1533605178036},{"_id":"public/categories/web/index.html","hash":"f4ea14ab7b829a21c66e3642f9a5267fbfc74861","modified":1533605178036},{"_id":"public/categories/深度学习的实用层面/index.html","hash":"3c77046a24a5bc1acca3c02920d61ce5c72f4e5d","modified":1533605178036},{"_id":"public/categories/程序员实用工具/index.html","hash":"c1910f9eefc8b007da7a49e16f52a2479a5f30e0","modified":1533605178036},{"_id":"public/categories/文学/index.html","hash":"acd400823f52378a08537144be2f096616a5e1e2","modified":1533605178036},{"_id":"public/categories/算法导论/index.html","hash":"1ea7ace7aab5f7d4110f8f2a76497613e84dc63a","modified":1533605178036},{"_id":"public/categories/爱情心理学/index.html","hash":"557961ee2b05b2683fcc28e402f3697a5206bb33","modified":1533605178037},{"_id":"public/categories/数据集/index.html","hash":"245ff89e17525090a9a36e00a4a62f3641d2c395","modified":1533605178037},{"_id":"public/categories/现代诗/index.html","hash":"342a84e72c2768ca1f1d32134f703a9fda886553","modified":1533605178037},{"_id":"public/categories/计算机科学/index.html","hash":"95785b2a5bce3b8181d129767bd5700700456e95","modified":1533605178037},{"_id":"public/2018/08/05/布雷默曼极限/index.html","hash":"3b328add810b0146e9184d87b99bfebf6c7b0808","modified":1533605178039},{"_id":"public/2018/08/05/Evolutionary-Algorithms/index.html","hash":"694d84476a3fc24a838b97562dafd44f1ee9ef14","modified":1533605178039},{"_id":"public/2018/08/05/github使用手册/index.html","hash":"3db1d987cc3bdc5a4b01d5210a8a050d3f9150d5","modified":1533605178039},{"_id":"public/2018/08/05/hello-world/index.html","hash":"0449f8844cc6483ef60885af8d3e569ad6e0f34f","modified":1533515893972},{"_id":"public/2018/08/05/python内置小工具/index.html","hash":"f3e91948108f8cfbdd3241e356824038f38cc8cb","modified":1533605178039},{"_id":"public/2018/08/04/深度学习中的优化算法/index.html","hash":"3b3a865d1188ab398b2ea15ba0fbaf4e10d3a710","modified":1533605178039},{"_id":"public/2018/08/03/DNN应用1-识别猫/index.html","hash":"33e8c9cbec68b3f357149fd8f3eb9b6fbaa60a65","modified":1533605178039},{"_id":"public/2018/07/22/初始化参数/index.html","hash":"1da78d279fdf6b6cf3ad40439472c36aae8976e7","modified":1533605178039},{"_id":"public/2018/07/21/动态规划/index.html","hash":"921961f71edc0a041134528808309565ff285ccc","modified":1533605178039},{"_id":"public/2018/07/21/数据的加载-预处理-可视化/index.html","hash":"4ff216dd31228cf488d0ffaa069f75f4488ef1ad","modified":1533605178039},{"_id":"public/2018/07/21/神经网络中的通用函数代码/index.html","hash":"4528682b334bcd75de53e44e3e7d92cbbb8c5fe2","modified":1533605178039},{"_id":"public/2018/07/21/贞洁禁忌/index.html","hash":"34ea66cede3c545b71c8eb834f8a6edd69dc7a25","modified":1533605178039},{"_id":"public/2018/07/20/梯度检验/index.html","hash":"26846d1157a4d4baeeafd1661433a08c2af2b7d3","modified":1533605178039},{"_id":"public/2018/07/20/标准化输入/index.html","hash":"5f3d61b95e7b82107320646633389a3a3a6d7420","modified":1533605178039},{"_id":"public/2018/07/20/梯度消失和梯度爆炸/index.html","hash":"3543c14abfcd880fc6b09231ae0375fb72a68fb6","modified":1533605178039},{"_id":"public/2018/07/20/dropout/index.html","hash":"d1a42fa076718a9af3c940796251b75ec784f1d1","modified":1533605178040},{"_id":"public/2018/07/20/正则化/index.html","hash":"0050c8899fe94fd0b21a77fe3b252626958e0649","modified":1533605178040},{"_id":"public/2018/07/20/模型估计/index.html","hash":"b3682fd0b7a774302913353676fde4fd8718c9ba","modified":1533605178040},{"_id":"public/2018/07/20/数据划分/index.html","hash":"d41eade50e5d52d8acf4e5159934c12184b13e2e","modified":1533605178040},{"_id":"public/2018/07/20/十个策略故事/index.html","hash":"5ad08712e193f8f038450dffb99f8aac7eb2d1d6","modified":1533605178040},{"_id":"public/2018/07/19/人生-路遥/index.html","hash":"1ea9c90dcf6587edb4001a7e2f76c6a68d90802c","modified":1533605178040},{"_id":"public/2018/07/19/恋爱领域中普遍存在的贬低倾向/index.html","hash":"e99a4f50016ca9d30cad82b2f0728a79593d94e5","modified":1533605178040},{"_id":"public/2018/07/19/男人的对象选择中的一种特殊类型/index.html","hash":"9e2cbf18296ebcb8ec6d4e6c5ac4f3850daafe04","modified":1533605178040},{"_id":"public/2018/07/19/短诗三首/index.html","hash":"f9f07f9a2a1e2e4b3852832f1fe4d429b5b9a5bb","modified":1533605178040},{"_id":"public/2018/07/18/How-to-set-up-a-blog-with-hexo-on-github-io/index.html","hash":"4d20146b13c7a5e4929d5618be0feee4cbd99d6c","modified":1533605178040},{"_id":"public/archives/index.html","hash":"d20613d668b66809b873893ea69f0d68927992e3","modified":1533605178040},{"_id":"public/archives/page/2/index.html","hash":"e5a407817214050d7e0939538e662187a729eabb","modified":1533605178040},{"_id":"public/archives/2018/index.html","hash":"d8e265cd687dcae28ab810f19d9171bad68a42db","modified":1533605178040},{"_id":"public/archives/2018/page/2/index.html","hash":"1f0477e87d083c5ecd37c1f73782b5f1395f55f9","modified":1533605178041},{"_id":"public/archives/2018/07/index.html","hash":"7cb4a5d294a3dc068093dfeeebe8f6a9cb40f3be","modified":1533605178041},{"_id":"public/archives/2018/07/page/2/index.html","hash":"3ce1a7b27dcf9a97a92cbd80dcfdb5eec9e40340","modified":1533605178041},{"_id":"public/archives/2018/08/index.html","hash":"48e923d7d3e55ed9958c56ef47b1fd22b6dbf1ad","modified":1533605178041},{"_id":"public/index.html","hash":"3ee6dc12325bf01eb7be199efb6bcfc3c05531c2","modified":1533605178041},{"_id":"public/page/2/index.html","hash":"96d1500a4c8116fa00cf34f5a811bda9b90bc4f0","modified":1533605178041},{"_id":"public/page/3/index.html","hash":"781b46894c4e3b5b9c74bd5759ffe89cd3ef1d0d","modified":1533605178041},{"_id":"public/tags/进化算法/index.html","hash":"7b92798188e099b700a010942ed572db21c2aa34","modified":1533605178037},{"_id":"public/tags/遗传算法/index.html","hash":"3f8f17268ed2c341441c5e84d85e0383202e6412","modified":1533605178037},{"_id":"public/categories/进化计算/index.html","hash":"83905c9741603bfcf27e551d8654b87ec4cbe85a","modified":1533605178036},{"_id":"public/2018/08/05/Genetic-Algorithms/index.html","hash":"137dc606640b9b3d61293dd882f5098c76178a2d","modified":1533605178038},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1533515893987},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1533515893987},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1533515893987},{"_id":"public/images/avatar.jpeg","hash":"f1aa09fc418f5d6f05af58e81a9ca95790a075b0","modified":1533515893987},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1533515893987},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1533515893987},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1533515893988},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1533515893988},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1533515893988},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1533515893988},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1533515893988},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1533515893988},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1533515893988},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1533515893988},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1533515893988},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1533515893988},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1533515893988},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1533515893988},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1533515893988},{"_id":"public/images/res1.PNG","hash":"f528eca6822e71539139aa1b2795847135dc2b07","modified":1533515893988},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1533515893988},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1533515893988},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1533515893988},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1533515893988},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1533515893988},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1533515893988},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1533515893988},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1533515893989},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1533515893989},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1533515893989},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1533515893989},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1533515893989},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1533515893989},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1533515893989},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1533515893989},{"_id":"public/images/eas.png","hash":"9d12e21703b5d1dab464bd5f8f0d2f194145b43c","modified":1533515894725},{"_id":"public/images/my_image.jpg","hash":"931bed91b176c20eee98be0a6f0af356d37b2330","modified":1533515894726},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1533515894735},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1533515894735},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1533515894744},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1533515894744},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1533515894744},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1533515894744},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1533515894744},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1533515894744},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1533515894744},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1533515894744},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1533515894745},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1533515894745},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1533515894745},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1533515894745},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1533515894745},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1533515894745},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1533515894745},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1533515894745},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1533515894745},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1533515894745},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1533515894745},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1533515894745},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1533515894745},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1533515894745},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1533515894746},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1533515894746},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1533515894746},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1533515894746},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1533515894746},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1533515894746},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1533515894746},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1533515894746},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1533515894746},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1533515894746},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1533515894746},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1533515894746},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1533515894746},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1533515894746},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1533515894746},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1533515894746},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1533515894746},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1533515894747},{"_id":"public/css/main.css","hash":"9489c3436207c351d0c52fca2dba9764269039ef","modified":1533515894747},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1533515894747},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1533515894747},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1533515894747},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1533515894747},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1533515894747},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1533515894747},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1533515894747},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1533515894747},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1533515894747},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1533515894747},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1533515894747},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1533515894747},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1533515894747},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1533515894747},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1533515894747},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1533515894747},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1533515894748},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1533515894748},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1533515894748},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1533515894748},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1533515894748},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1533515894748},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1533515894748},{"_id":"public/images/LlayerNN.png","hash":"939d9808c2d757e7097f22cbb4b4083c40f3da9c","modified":1533515894756},{"_id":"public/images/动态规划.jpg","hash":"771e3ceb5404dcb7ef349c6aa6e7853a7d61934e","modified":1533515894758},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1533515894762},{"_id":"source/_posts/Gradient-Descent-Famliy.md","hash":"3ecf4969d0bf93f93bd5ba75f37bf54429e3c0d7","modified":1533608359385},{"_id":"source/images/minibatch.png","hash":"1705433ef878be5688ab474c7f3461c94b224dde","modified":1531227952848},{"_id":"source/images/partition.png","hash":"1c29a499253b64f0834f8253ea85efcb716872ac","modified":1531227952849},{"_id":"source/images/sgd.png","hash":"43fac22e9aa802e1fae6e25d791d23d3f82c616f","modified":1531227952850},{"_id":"source/images/shuffle.png","hash":"e844a5da26c91ccd40f9c5186a7fd5db7fe4f2b7","modified":1531227952852},{"_id":"public/2018/08/07/Gradient-Descent-Famliy/index.html","hash":"cf8efb521a8487e982debb6d187e49dbd7f86c19","modified":1533608375437},{"_id":"public/2018/08/07/hello-world/index.html","hash":"2d3a38ee84012892264364744ee30ce1888ead7d","modified":1533605178038},{"_id":"public/images/minibatch.png","hash":"1705433ef878be5688ab474c7f3461c94b224dde","modified":1533604415499},{"_id":"public/images/partition.png","hash":"1c29a499253b64f0834f8253ea85efcb716872ac","modified":1533604415499},{"_id":"public/images/sgd.png","hash":"43fac22e9aa802e1fae6e25d791d23d3f82c616f","modified":1533604415499},{"_id":"public/images/shuffle.png","hash":"e844a5da26c91ccd40f9c5186a7fd5db7fe4f2b7","modified":1533604415500},{"_id":"public/tags/梯度下降/index.html","hash":"c937a75f4ead394f85d822087a971adf70209d82","modified":1533605178043},{"_id":"source/images/momentum.png","hash":"97d12c75f9b2ecd40f0a6d0d47a21f83672ede97","modified":1531227952842},{"_id":"public/images/momentum.png","hash":"97d12c75f9b2ecd40f0a6d0d47a21f83672ede97","modified":1533607554699},{"_id":"source/images/adam.PNG","hash":"f2adc403d4012046172c37175835fefb8bec932f","modified":1533608219366},{"_id":"public/images/adam.PNG","hash":"f2adc403d4012046172c37175835fefb8bec932f","modified":1533608375441}],"Category":[{"name":"进化计算","_id":"cjkhjrl9k00023bcph8rood3t"},{"name":"深度学习","_id":"cjkhjrlap000c3bcp889b57c7"},{"name":"web","_id":"cjkhjrlat000h3bcpkv1d5xiu"},{"name":"深度学习的实用层面","_id":"cjkhjrlav000m3bcp1n1uywc0"},{"name":"程序员实用工具","_id":"cjkhjrlay000t3bcpj7ycv82m"},{"name":"文学","_id":"cjkhjrlaz000y3bcpb4oojgoj"},{"name":"算法导论","_id":"cjkhjrlg800173bcpfmwcv2mi"},{"name":"爱情心理学","_id":"cjkhjrlgc001e3bcpq2l5dsk5"},{"name":"数据集","_id":"cjkhjrlgf001k3bcp8rz52r0y"},{"name":"现代诗","_id":"cjkhjrlgn00253bcp64338gxo"},{"name":"计算机科学","_id":"cjkhjrlgu002e3bcp7rxrgmt1"}],"Data":[],"Page":[{"title":"categories","date":"2018-07-18T16:23:02.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-07-19 00:23:02\ntype: \"categories\"\n---\n","updated":"2018-08-07T00:36:25.241Z","path":"categories/index.html","_id":"cjkhjrlfz00133bcpdukobay4","comments":1,"layout":"page","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-07-18T16:21:54.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-07-19 00:21:54\ntype: \"tags\"\n---\n","updated":"2018-08-07T00:36:25.251Z","path":"tags/index.html","_id":"cjkhjrlk000363bcpqrlbvkt7","comments":1,"layout":"page","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Evolutionary Algorithms","date":"2018-08-05T09:12:46.000Z","_content":"\n>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. -- Charles Darwin\n\n## Motivation of EAs\n\n1. What can EAs do for us?\n\t- Optimization\n\t- Help people understand the evolution in nature.\n\n2. What is optimizatin?\n\t- The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain **performance criteria**\n\n3. Produce maximum yields given litmited resources.\n\n## Key Concepts\n\n- Population-Based Stochastic Optimization Methods\n- Inherently Parallel\n- A Good Example of Bionics in Engineering\n- Survival of the Fittest\n- Chromosome, Crossover, Mutation\n- Metaheuristics\n- Bio-/Nature Inspired Computing\n\n## The Big Picture\n\n![](/images/eas.png)\n\n## EA Family\n\n- GA: Genetic Algorithm\n- GP: Genetic Programming\n- ES: Evolution Strategies\n- EP: Evolution Programming\n- EDA: Estimation of Distribution Algorithm\n- PSO: Particle Swarm Optimization\n- ACO: Ant Colony Optimization\n- DE: Differential Evolution\n\n## Optimization Problem Set\n\n- Portfolio Optimization\n- Travelling Salesman Problem\n- Knapsack Problem\n- Machine Learing Problems\n\n![](/images/local_optima.png)\n\nMany interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.\n\n## Solution: Parallel Search\n\n- Conduct searching in different areas simultaneously.\n\t- Population Based\n\t- Avoid unfortunate starting positions.\n- Employ heuristic methods to effectively explore the space.\n\t- Focus on promising areas.\n\t- Also keep an eye on other regions.\n\t- More than random restart strategies.\n\n## Publications\n\nTop Journals:\n- IEEE Transactions On Evolutionary Computation.\n- Evolutionary Compution Journal\n\nMajor Conference:\n- IEEE Congress On Evolution Computation(CEC)\n- Genetic and Evolution Computation Conference(GECCO)\n- Parallel Problem Solving from Nature(PPSN)\n\nGame:\n\t- Blondie24: Playing at the Edge of AI\n\nBook:\n\t- How to Solve It: Modern Heuristics\n","source":"_posts/Evolutionary-Algorithms.md","raw":"---\ntitle: Evolutionary Algorithms\ndate: 2018-08-05 17:12:46\ntags: 进化算法\ncategories: 进化计算\n---\n\n>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. -- Charles Darwin\n\n## Motivation of EAs\n\n1. What can EAs do for us?\n\t- Optimization\n\t- Help people understand the evolution in nature.\n\n2. What is optimizatin?\n\t- The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain **performance criteria**\n\n3. Produce maximum yields given litmited resources.\n\n## Key Concepts\n\n- Population-Based Stochastic Optimization Methods\n- Inherently Parallel\n- A Good Example of Bionics in Engineering\n- Survival of the Fittest\n- Chromosome, Crossover, Mutation\n- Metaheuristics\n- Bio-/Nature Inspired Computing\n\n## The Big Picture\n\n![](/images/eas.png)\n\n## EA Family\n\n- GA: Genetic Algorithm\n- GP: Genetic Programming\n- ES: Evolution Strategies\n- EP: Evolution Programming\n- EDA: Estimation of Distribution Algorithm\n- PSO: Particle Swarm Optimization\n- ACO: Ant Colony Optimization\n- DE: Differential Evolution\n\n## Optimization Problem Set\n\n- Portfolio Optimization\n- Travelling Salesman Problem\n- Knapsack Problem\n- Machine Learing Problems\n\n![](/images/local_optima.png)\n\nMany interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.\n\n## Solution: Parallel Search\n\n- Conduct searching in different areas simultaneously.\n\t- Population Based\n\t- Avoid unfortunate starting positions.\n- Employ heuristic methods to effectively explore the space.\n\t- Focus on promising areas.\n\t- Also keep an eye on other regions.\n\t- More than random restart strategies.\n\n## Publications\n\nTop Journals:\n- IEEE Transactions On Evolutionary Computation.\n- Evolutionary Compution Journal\n\nMajor Conference:\n- IEEE Congress On Evolution Computation(CEC)\n- Genetic and Evolution Computation Conference(GECCO)\n- Parallel Problem Solving from Nature(PPSN)\n\nGame:\n\t- Blondie24: Playing at the Edge of AI\n\nBook:\n\t- How to Solve It: Modern Heuristics\n","slug":"Evolutionary-Algorithms","published":1,"updated":"2018-08-07T00:36:25.220Z","_id":"cjkhjrl9c00003bcpyg6ta5u9","comments":1,"layout":"post","photos":[],"link":"","content":"<blockquote>\n<p>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. – Charles Darwin</p>\n</blockquote>\n<h2 id=\"Motivation-of-EAs\"><a href=\"#Motivation-of-EAs\" class=\"headerlink\" title=\"Motivation of EAs\"></a>Motivation of EAs</h2><ol>\n<li><p>What can EAs do for us?</p>\n<ul>\n<li>Optimization</li>\n<li>Help people understand the evolution in nature.</li>\n</ul>\n</li>\n<li><p>What is optimizatin?</p>\n<ul>\n<li>The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain <strong>performance criteria</strong></li>\n</ul>\n</li>\n<li><p>Produce maximum yields given litmited resources.</p>\n</li>\n</ol>\n<h2 id=\"Key-Concepts\"><a href=\"#Key-Concepts\" class=\"headerlink\" title=\"Key Concepts\"></a>Key Concepts</h2><ul>\n<li>Population-Based Stochastic Optimization Methods</li>\n<li>Inherently Parallel</li>\n<li>A Good Example of Bionics in Engineering</li>\n<li>Survival of the Fittest</li>\n<li>Chromosome, Crossover, Mutation</li>\n<li>Metaheuristics</li>\n<li>Bio-/Nature Inspired Computing</li>\n</ul>\n<h2 id=\"The-Big-Picture\"><a href=\"#The-Big-Picture\" class=\"headerlink\" title=\"The Big Picture\"></a>The Big Picture</h2><p><img src=\"/images/eas.png\" alt=\"\"></p>\n<h2 id=\"EA-Family\"><a href=\"#EA-Family\" class=\"headerlink\" title=\"EA Family\"></a>EA Family</h2><ul>\n<li>GA: Genetic Algorithm</li>\n<li>GP: Genetic Programming</li>\n<li>ES: Evolution Strategies</li>\n<li>EP: Evolution Programming</li>\n<li>EDA: Estimation of Distribution Algorithm</li>\n<li>PSO: Particle Swarm Optimization</li>\n<li>ACO: Ant Colony Optimization</li>\n<li>DE: Differential Evolution</li>\n</ul>\n<h2 id=\"Optimization-Problem-Set\"><a href=\"#Optimization-Problem-Set\" class=\"headerlink\" title=\"Optimization Problem Set\"></a>Optimization Problem Set</h2><ul>\n<li>Portfolio Optimization</li>\n<li>Travelling Salesman Problem</li>\n<li>Knapsack Problem</li>\n<li>Machine Learing Problems</li>\n</ul>\n<p><img src=\"/images/local_optima.png\" alt=\"\"></p>\n<p>Many interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.</p>\n<h2 id=\"Solution-Parallel-Search\"><a href=\"#Solution-Parallel-Search\" class=\"headerlink\" title=\"Solution: Parallel Search\"></a>Solution: Parallel Search</h2><ul>\n<li>Conduct searching in different areas simultaneously.<ul>\n<li>Population Based</li>\n<li>Avoid unfortunate starting positions.</li>\n</ul>\n</li>\n<li>Employ heuristic methods to effectively explore the space.<ul>\n<li>Focus on promising areas.</li>\n<li>Also keep an eye on other regions.</li>\n<li>More than random restart strategies.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><p>Top Journals:</p>\n<ul>\n<li>IEEE Transactions On Evolutionary Computation.</li>\n<li>Evolutionary Compution Journal</li>\n</ul>\n<p>Major Conference:</p>\n<ul>\n<li>IEEE Congress On Evolution Computation(CEC)</li>\n<li>Genetic and Evolution Computation Conference(GECCO)</li>\n<li>Parallel Problem Solving from Nature(PPSN)</li>\n</ul>\n<p>Game:</p>\n<pre><code>- Blondie24: Playing at the Edge of AI\n</code></pre><p>Book:</p>\n<pre><code>- How to Solve It: Modern Heuristics\n</code></pre>","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. – Charles Darwin</p>\n</blockquote>\n<h2 id=\"Motivation-of-EAs\"><a href=\"#Motivation-of-EAs\" class=\"headerlink\" title=\"Motivation of EAs\"></a>Motivation of EAs</h2><ol>\n<li><p>What can EAs do for us?</p>\n<ul>\n<li>Optimization</li>\n<li>Help people understand the evolution in nature.</li>\n</ul>\n</li>\n<li><p>What is optimizatin?</p>\n<ul>\n<li>The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain <strong>performance criteria</strong></li>\n</ul>\n</li>\n<li><p>Produce maximum yields given litmited resources.</p>\n</li>\n</ol>\n<h2 id=\"Key-Concepts\"><a href=\"#Key-Concepts\" class=\"headerlink\" title=\"Key Concepts\"></a>Key Concepts</h2><ul>\n<li>Population-Based Stochastic Optimization Methods</li>\n<li>Inherently Parallel</li>\n<li>A Good Example of Bionics in Engineering</li>\n<li>Survival of the Fittest</li>\n<li>Chromosome, Crossover, Mutation</li>\n<li>Metaheuristics</li>\n<li>Bio-/Nature Inspired Computing</li>\n</ul>\n<h2 id=\"The-Big-Picture\"><a href=\"#The-Big-Picture\" class=\"headerlink\" title=\"The Big Picture\"></a>The Big Picture</h2><p><img src=\"/images/eas.png\" alt=\"\"></p>\n<h2 id=\"EA-Family\"><a href=\"#EA-Family\" class=\"headerlink\" title=\"EA Family\"></a>EA Family</h2><ul>\n<li>GA: Genetic Algorithm</li>\n<li>GP: Genetic Programming</li>\n<li>ES: Evolution Strategies</li>\n<li>EP: Evolution Programming</li>\n<li>EDA: Estimation of Distribution Algorithm</li>\n<li>PSO: Particle Swarm Optimization</li>\n<li>ACO: Ant Colony Optimization</li>\n<li>DE: Differential Evolution</li>\n</ul>\n<h2 id=\"Optimization-Problem-Set\"><a href=\"#Optimization-Problem-Set\" class=\"headerlink\" title=\"Optimization Problem Set\"></a>Optimization Problem Set</h2><ul>\n<li>Portfolio Optimization</li>\n<li>Travelling Salesman Problem</li>\n<li>Knapsack Problem</li>\n<li>Machine Learing Problems</li>\n</ul>\n<p><img src=\"/images/local_optima.png\" alt=\"\"></p>\n<p>Many interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.</p>\n<h2 id=\"Solution-Parallel-Search\"><a href=\"#Solution-Parallel-Search\" class=\"headerlink\" title=\"Solution: Parallel Search\"></a>Solution: Parallel Search</h2><ul>\n<li>Conduct searching in different areas simultaneously.<ul>\n<li>Population Based</li>\n<li>Avoid unfortunate starting positions.</li>\n</ul>\n</li>\n<li>Employ heuristic methods to effectively explore the space.<ul>\n<li>Focus on promising areas.</li>\n<li>Also keep an eye on other regions.</li>\n<li>More than random restart strategies.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><p>Top Journals:</p>\n<ul>\n<li>IEEE Transactions On Evolutionary Computation.</li>\n<li>Evolutionary Compution Journal</li>\n</ul>\n<p>Major Conference:</p>\n<ul>\n<li>IEEE Congress On Evolution Computation(CEC)</li>\n<li>Genetic and Evolution Computation Conference(GECCO)</li>\n<li>Parallel Problem Solving from Nature(PPSN)</li>\n</ul>\n<p>Game:</p>\n<pre><code>- Blondie24: Playing at the Edge of AI\n</code></pre><p>Book:</p>\n<pre><code>- How to Solve It: Modern Heuristics\n</code></pre>"},{"title":"Genetic Algorithms","date":"2018-08-05T14:14:07.000Z","_content":"\n## Biology Background\n\n- Gene: A working subunit of DNA\n- Gene Trait(性状基因): For example colour of eyes\n- Allele(等位基因): Possible settings for a trait\n- Genotype(基因型): The actual genes carried by an individual\n- Phenotype(性状型): The physical characteristics into which genes are translated\n\n## Genetic Algorithms\n\nBy John Holland: \"Adaptation in Natural and Artificial Systems\"\n\n### Inspired by and (loosely) based on Darwin's Theory\n\n- Chromosome(染色体)\n- Crossover(交叉)\n- Mutation(变异)\n- Selection(Survival of the Fittest)\n\n### Basic Ideas\n\n- Each solution to the problem is represented as a chromosome.\n- The initial solutions may be randomly generated.\n- Solution are evolved during generations.\n- Improved gradually based on the principle of natural evolution.\n\n### Basic Components\n\n#### Representation\n\n- How to encode the parameters of the problem?\n- Binary Problems\n- Continuous Problems\n\n1. Individual(Chromosome)\n\nA vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.\n\n2. Population\n\nA set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.\n\n3. Offspring\n\nNew individuals generated via genetic operators. Hopefully contain better solutions.\n\n4. Encoding\n\nBinary vs. Gray. How to encode TSP problems?\n\n#### Genetic Operators\n\n1. Crossover:\n\tExchange genetic materials between two chromosomes.\n\t- One Point Crossover\n\t- Two Point Crossover\n\t- Uniform Crossover\n \n2. Mutation:\n\tRandomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.\n\n#### Selection Strategy\n\n- Which chromosomes should be involved in reproduction?\n- Which offspring should be able to survive?\n\n1. Roulette Wheel Selection: 根据适应度的高低按比例选择\n2. Rank Selection： 根据排名按固定比例选择\n3. Tournament Selection: 两两及以上互相竞争\n4. Elitism: 精英保留直接拷贝到下一代\n5. Offspring Selection: 子代直接进入下一代还是与父代一起竞争\n\n### Selection vs. Crossover vs. Mutation\n\n- Selection:\n\t- Bias the search effort towards promising individuals.\n\t- Loss of genetic diversity\n\n- Cossover:\n\t- Create better individuals by combining genes from good individuals\n\t- Building Block Hypothesis\n\t- Major search power of GAs\n\t- No effect on genetic diversity\n\n- Mutation:\n\t- Increase genetic diversity\n\t- Force the algorithm to search areas other than the current focus.\n\n**It is a trade off about Exploration vs. Exploitation**\n\n## GA Framework\n\n1. Intialization: Generate a random population P of M individuals\n2. Evaluation: Evaluate the fitness f(x) of each individual\n3. Repeat until the stopping criteria are met:\n\t1. Reproduction: Repeat the following steps until all offspring are generated\n\t\t1. Paraent Selection: Select two parents from P\n\t\t2. Crossover: Apply crossover on the parents with probability P_c\n\t\t3. Mutation: Apply mutation on offspring with probability P_m\n\t\t4. Evaluation: Evaluate the newly generated offspring\n\t2. Offspring Selection: Create a new population from oddspring and P\n\t3. Output: Return the best individual found\n\n## Parameters\n\n- Population Size:\n\tToo big: Slow convergence rate. Too small: Premature convergence\n\n- Crossover Rate:\n\tRecommended value: 0.8\n\n- Mutation Rate:\n\tRecommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.\n\n- Selection Strategy:\n\tTournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.\n","source":"_posts/Genetic-Algorithms.md","raw":"---\ntitle: Genetic Algorithms\ndate: 2018-08-05 22:14:07\ntags: 遗传算法\ncategories: 进化计算\n---\n\n## Biology Background\n\n- Gene: A working subunit of DNA\n- Gene Trait(性状基因): For example colour of eyes\n- Allele(等位基因): Possible settings for a trait\n- Genotype(基因型): The actual genes carried by an individual\n- Phenotype(性状型): The physical characteristics into which genes are translated\n\n## Genetic Algorithms\n\nBy John Holland: \"Adaptation in Natural and Artificial Systems\"\n\n### Inspired by and (loosely) based on Darwin's Theory\n\n- Chromosome(染色体)\n- Crossover(交叉)\n- Mutation(变异)\n- Selection(Survival of the Fittest)\n\n### Basic Ideas\n\n- Each solution to the problem is represented as a chromosome.\n- The initial solutions may be randomly generated.\n- Solution are evolved during generations.\n- Improved gradually based on the principle of natural evolution.\n\n### Basic Components\n\n#### Representation\n\n- How to encode the parameters of the problem?\n- Binary Problems\n- Continuous Problems\n\n1. Individual(Chromosome)\n\nA vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.\n\n2. Population\n\nA set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.\n\n3. Offspring\n\nNew individuals generated via genetic operators. Hopefully contain better solutions.\n\n4. Encoding\n\nBinary vs. Gray. How to encode TSP problems?\n\n#### Genetic Operators\n\n1. Crossover:\n\tExchange genetic materials between two chromosomes.\n\t- One Point Crossover\n\t- Two Point Crossover\n\t- Uniform Crossover\n \n2. Mutation:\n\tRandomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.\n\n#### Selection Strategy\n\n- Which chromosomes should be involved in reproduction?\n- Which offspring should be able to survive?\n\n1. Roulette Wheel Selection: 根据适应度的高低按比例选择\n2. Rank Selection： 根据排名按固定比例选择\n3. Tournament Selection: 两两及以上互相竞争\n4. Elitism: 精英保留直接拷贝到下一代\n5. Offspring Selection: 子代直接进入下一代还是与父代一起竞争\n\n### Selection vs. Crossover vs. Mutation\n\n- Selection:\n\t- Bias the search effort towards promising individuals.\n\t- Loss of genetic diversity\n\n- Cossover:\n\t- Create better individuals by combining genes from good individuals\n\t- Building Block Hypothesis\n\t- Major search power of GAs\n\t- No effect on genetic diversity\n\n- Mutation:\n\t- Increase genetic diversity\n\t- Force the algorithm to search areas other than the current focus.\n\n**It is a trade off about Exploration vs. Exploitation**\n\n## GA Framework\n\n1. Intialization: Generate a random population P of M individuals\n2. Evaluation: Evaluate the fitness f(x) of each individual\n3. Repeat until the stopping criteria are met:\n\t1. Reproduction: Repeat the following steps until all offspring are generated\n\t\t1. Paraent Selection: Select two parents from P\n\t\t2. Crossover: Apply crossover on the parents with probability P_c\n\t\t3. Mutation: Apply mutation on offspring with probability P_m\n\t\t4. Evaluation: Evaluate the newly generated offspring\n\t2. Offspring Selection: Create a new population from oddspring and P\n\t3. Output: Return the best individual found\n\n## Parameters\n\n- Population Size:\n\tToo big: Slow convergence rate. Too small: Premature convergence\n\n- Crossover Rate:\n\tRecommended value: 0.8\n\n- Mutation Rate:\n\tRecommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.\n\n- Selection Strategy:\n\tTournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.\n","slug":"Genetic-Algorithms","published":1,"updated":"2018-08-07T00:36:25.221Z","_id":"cjkhjrl9i00013bcp7u68pocb","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Biology-Background\"><a href=\"#Biology-Background\" class=\"headerlink\" title=\"Biology Background\"></a>Biology Background</h2><ul>\n<li>Gene: A working subunit of DNA</li>\n<li>Gene Trait(性状基因): For example colour of eyes</li>\n<li>Allele(等位基因): Possible settings for a trait</li>\n<li>Genotype(基因型): The actual genes carried by an individual</li>\n<li>Phenotype(性状型): The physical characteristics into which genes are translated</li>\n</ul>\n<h2 id=\"Genetic-Algorithms\"><a href=\"#Genetic-Algorithms\" class=\"headerlink\" title=\"Genetic Algorithms\"></a>Genetic Algorithms</h2><p>By John Holland: “Adaptation in Natural and Artificial Systems”</p>\n<h3 id=\"Inspired-by-and-loosely-based-on-Darwin’s-Theory\"><a href=\"#Inspired-by-and-loosely-based-on-Darwin’s-Theory\" class=\"headerlink\" title=\"Inspired by and (loosely) based on Darwin’s Theory\"></a>Inspired by and (loosely) based on Darwin’s Theory</h3><ul>\n<li>Chromosome(染色体)</li>\n<li>Crossover(交叉)</li>\n<li>Mutation(变异)</li>\n<li>Selection(Survival of the Fittest)</li>\n</ul>\n<h3 id=\"Basic-Ideas\"><a href=\"#Basic-Ideas\" class=\"headerlink\" title=\"Basic Ideas\"></a>Basic Ideas</h3><ul>\n<li>Each solution to the problem is represented as a chromosome.</li>\n<li>The initial solutions may be randomly generated.</li>\n<li>Solution are evolved during generations.</li>\n<li>Improved gradually based on the principle of natural evolution.</li>\n</ul>\n<h3 id=\"Basic-Components\"><a href=\"#Basic-Components\" class=\"headerlink\" title=\"Basic Components\"></a>Basic Components</h3><h4 id=\"Representation\"><a href=\"#Representation\" class=\"headerlink\" title=\"Representation\"></a>Representation</h4><ul>\n<li>How to encode the parameters of the problem?</li>\n<li>Binary Problems</li>\n<li>Continuous Problems</li>\n</ul>\n<ol>\n<li>Individual(Chromosome)</li>\n</ol>\n<p>A vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.</p>\n<ol start=\"2\">\n<li>Population</li>\n</ol>\n<p>A set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.</p>\n<ol start=\"3\">\n<li>Offspring</li>\n</ol>\n<p>New individuals generated via genetic operators. Hopefully contain better solutions.</p>\n<ol start=\"4\">\n<li>Encoding</li>\n</ol>\n<p>Binary vs. Gray. How to encode TSP problems?</p>\n<h4 id=\"Genetic-Operators\"><a href=\"#Genetic-Operators\" class=\"headerlink\" title=\"Genetic Operators\"></a>Genetic Operators</h4><ol>\n<li><p>Crossover:<br> Exchange genetic materials between two chromosomes.</p>\n<ul>\n<li>One Point Crossover</li>\n<li>Two Point Crossover</li>\n<li>Uniform Crossover</li>\n</ul>\n</li>\n<li><p>Mutation:<br> Randomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.</p>\n</li>\n</ol>\n<h4 id=\"Selection-Strategy\"><a href=\"#Selection-Strategy\" class=\"headerlink\" title=\"Selection Strategy\"></a>Selection Strategy</h4><ul>\n<li>Which chromosomes should be involved in reproduction?</li>\n<li>Which offspring should be able to survive?</li>\n</ul>\n<ol>\n<li>Roulette Wheel Selection: 根据适应度的高低按比例选择</li>\n<li>Rank Selection： 根据排名按固定比例选择</li>\n<li>Tournament Selection: 两两及以上互相竞争</li>\n<li>Elitism: 精英保留直接拷贝到下一代</li>\n<li>Offspring Selection: 子代直接进入下一代还是与父代一起竞争</li>\n</ol>\n<h3 id=\"Selection-vs-Crossover-vs-Mutation\"><a href=\"#Selection-vs-Crossover-vs-Mutation\" class=\"headerlink\" title=\"Selection vs. Crossover vs. Mutation\"></a>Selection vs. Crossover vs. Mutation</h3><ul>\n<li><p>Selection:</p>\n<ul>\n<li>Bias the search effort towards promising individuals.</li>\n<li>Loss of genetic diversity</li>\n</ul>\n</li>\n<li><p>Cossover:</p>\n<ul>\n<li>Create better individuals by combining genes from good individuals</li>\n<li>Building Block Hypothesis</li>\n<li>Major search power of GAs</li>\n<li>No effect on genetic diversity</li>\n</ul>\n</li>\n<li><p>Mutation:</p>\n<ul>\n<li>Increase genetic diversity</li>\n<li>Force the algorithm to search areas other than the current focus.</li>\n</ul>\n</li>\n</ul>\n<p><strong>It is a trade off about Exploration vs. Exploitation</strong></p>\n<h2 id=\"GA-Framework\"><a href=\"#GA-Framework\" class=\"headerlink\" title=\"GA Framework\"></a>GA Framework</h2><ol>\n<li>Intialization: Generate a random population P of M individuals</li>\n<li>Evaluation: Evaluate the fitness f(x) of each individual</li>\n<li>Repeat until the stopping criteria are met:<ol>\n<li>Reproduction: Repeat the following steps until all offspring are generated<ol>\n<li>Paraent Selection: Select two parents from P</li>\n<li>Crossover: Apply crossover on the parents with probability P_c</li>\n<li>Mutation: Apply mutation on offspring with probability P_m</li>\n<li>Evaluation: Evaluate the newly generated offspring</li>\n</ol>\n</li>\n<li>Offspring Selection: Create a new population from oddspring and P</li>\n<li>Output: Return the best individual found</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"Parameters\"><a href=\"#Parameters\" class=\"headerlink\" title=\"Parameters\"></a>Parameters</h2><ul>\n<li><p>Population Size:<br>  Too big: Slow convergence rate. Too small: Premature convergence</p>\n</li>\n<li><p>Crossover Rate:<br>  Recommended value: 0.8</p>\n</li>\n<li><p>Mutation Rate:<br>  Recommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.</p>\n</li>\n<li><p>Selection Strategy:<br>  Tournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Biology-Background\"><a href=\"#Biology-Background\" class=\"headerlink\" title=\"Biology Background\"></a>Biology Background</h2><ul>\n<li>Gene: A working subunit of DNA</li>\n<li>Gene Trait(性状基因): For example colour of eyes</li>\n<li>Allele(等位基因): Possible settings for a trait</li>\n<li>Genotype(基因型): The actual genes carried by an individual</li>\n<li>Phenotype(性状型): The physical characteristics into which genes are translated</li>\n</ul>\n<h2 id=\"Genetic-Algorithms\"><a href=\"#Genetic-Algorithms\" class=\"headerlink\" title=\"Genetic Algorithms\"></a>Genetic Algorithms</h2><p>By John Holland: “Adaptation in Natural and Artificial Systems”</p>\n<h3 id=\"Inspired-by-and-loosely-based-on-Darwin’s-Theory\"><a href=\"#Inspired-by-and-loosely-based-on-Darwin’s-Theory\" class=\"headerlink\" title=\"Inspired by and (loosely) based on Darwin’s Theory\"></a>Inspired by and (loosely) based on Darwin’s Theory</h3><ul>\n<li>Chromosome(染色体)</li>\n<li>Crossover(交叉)</li>\n<li>Mutation(变异)</li>\n<li>Selection(Survival of the Fittest)</li>\n</ul>\n<h3 id=\"Basic-Ideas\"><a href=\"#Basic-Ideas\" class=\"headerlink\" title=\"Basic Ideas\"></a>Basic Ideas</h3><ul>\n<li>Each solution to the problem is represented as a chromosome.</li>\n<li>The initial solutions may be randomly generated.</li>\n<li>Solution are evolved during generations.</li>\n<li>Improved gradually based on the principle of natural evolution.</li>\n</ul>\n<h3 id=\"Basic-Components\"><a href=\"#Basic-Components\" class=\"headerlink\" title=\"Basic Components\"></a>Basic Components</h3><h4 id=\"Representation\"><a href=\"#Representation\" class=\"headerlink\" title=\"Representation\"></a>Representation</h4><ul>\n<li>How to encode the parameters of the problem?</li>\n<li>Binary Problems</li>\n<li>Continuous Problems</li>\n</ul>\n<ol>\n<li>Individual(Chromosome)</li>\n</ol>\n<p>A vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.</p>\n<ol start=\"2\">\n<li>Population</li>\n</ol>\n<p>A set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.</p>\n<ol start=\"3\">\n<li>Offspring</li>\n</ol>\n<p>New individuals generated via genetic operators. Hopefully contain better solutions.</p>\n<ol start=\"4\">\n<li>Encoding</li>\n</ol>\n<p>Binary vs. Gray. How to encode TSP problems?</p>\n<h4 id=\"Genetic-Operators\"><a href=\"#Genetic-Operators\" class=\"headerlink\" title=\"Genetic Operators\"></a>Genetic Operators</h4><ol>\n<li><p>Crossover:<br> Exchange genetic materials between two chromosomes.</p>\n<ul>\n<li>One Point Crossover</li>\n<li>Two Point Crossover</li>\n<li>Uniform Crossover</li>\n</ul>\n</li>\n<li><p>Mutation:<br> Randomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.</p>\n</li>\n</ol>\n<h4 id=\"Selection-Strategy\"><a href=\"#Selection-Strategy\" class=\"headerlink\" title=\"Selection Strategy\"></a>Selection Strategy</h4><ul>\n<li>Which chromosomes should be involved in reproduction?</li>\n<li>Which offspring should be able to survive?</li>\n</ul>\n<ol>\n<li>Roulette Wheel Selection: 根据适应度的高低按比例选择</li>\n<li>Rank Selection： 根据排名按固定比例选择</li>\n<li>Tournament Selection: 两两及以上互相竞争</li>\n<li>Elitism: 精英保留直接拷贝到下一代</li>\n<li>Offspring Selection: 子代直接进入下一代还是与父代一起竞争</li>\n</ol>\n<h3 id=\"Selection-vs-Crossover-vs-Mutation\"><a href=\"#Selection-vs-Crossover-vs-Mutation\" class=\"headerlink\" title=\"Selection vs. Crossover vs. Mutation\"></a>Selection vs. Crossover vs. Mutation</h3><ul>\n<li><p>Selection:</p>\n<ul>\n<li>Bias the search effort towards promising individuals.</li>\n<li>Loss of genetic diversity</li>\n</ul>\n</li>\n<li><p>Cossover:</p>\n<ul>\n<li>Create better individuals by combining genes from good individuals</li>\n<li>Building Block Hypothesis</li>\n<li>Major search power of GAs</li>\n<li>No effect on genetic diversity</li>\n</ul>\n</li>\n<li><p>Mutation:</p>\n<ul>\n<li>Increase genetic diversity</li>\n<li>Force the algorithm to search areas other than the current focus.</li>\n</ul>\n</li>\n</ul>\n<p><strong>It is a trade off about Exploration vs. Exploitation</strong></p>\n<h2 id=\"GA-Framework\"><a href=\"#GA-Framework\" class=\"headerlink\" title=\"GA Framework\"></a>GA Framework</h2><ol>\n<li>Intialization: Generate a random population P of M individuals</li>\n<li>Evaluation: Evaluate the fitness f(x) of each individual</li>\n<li>Repeat until the stopping criteria are met:<ol>\n<li>Reproduction: Repeat the following steps until all offspring are generated<ol>\n<li>Paraent Selection: Select two parents from P</li>\n<li>Crossover: Apply crossover on the parents with probability P_c</li>\n<li>Mutation: Apply mutation on offspring with probability P_m</li>\n<li>Evaluation: Evaluate the newly generated offspring</li>\n</ol>\n</li>\n<li>Offspring Selection: Create a new population from oddspring and P</li>\n<li>Output: Return the best individual found</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"Parameters\"><a href=\"#Parameters\" class=\"headerlink\" title=\"Parameters\"></a>Parameters</h2><ul>\n<li><p>Population Size:<br>  Too big: Slow convergence rate. Too small: Premature convergence</p>\n</li>\n<li><p>Crossover Rate:<br>  Recommended value: 0.8</p>\n</li>\n<li><p>Mutation Rate:<br>  Recommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.</p>\n</li>\n<li><p>Selection Strategy:<br>  Tournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.</p>\n</li>\n</ul>\n"},{"title":"DNN应用1--识别猫","date":"2018-08-03T00:52:05.000Z","_content":"## 实验目的\n\n使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。\n\n## 实验方案\n\n- 使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。\n- 各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理\n- 损失函数采用交叉熵，并使用L2正则化\n- 网络架构\n ![](/images/LlayerNN.png)\n\n## 详细设计\n\n### 数据预处理\n\n#### 加载数据\n\n```python\nimport numpy as np\nimport h5py\n\ndef load_data():\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n```\n\n```python\ntrain_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n```\n\n#### 数据集形状\n\n>Number of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n\n#### 展示数据图片\n\n```python\nimport matplotlib.pyplot as plt\n\nindex = 7\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\nplt.show()\n```\n\n#### 图像矩阵向量化\n\n```python\n# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n```\n\n#### 数据集最终形状\n\n>train_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)\n\n### 网络设计\n\n1. 初始化参数 / 定义超参数\n2. 迭代循环:\n    a. 前向传播\n    b. 计算代价函数\n    c. 反向传播\n    d. 更新参数 \n3. 使用训练的参数去预测新的数据标签\n\n网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 实验结果\n\n### 训练集结果\n\n```python\nlayers_dims = [12288, 20, 7, 5, 1] #  5-layer model\nparameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n```\n\n![](/images/res1.PNG)\n\n```python\npred_train = predict(train_x, train_y, parameters)\n```\n\n>Accuracy: 0.9856459330143539\n\n### 测试集结果\n\n```python\npred_test = predict(test_x, test_y, parameters)\n```\n\n>Accuracy: 0.8\n\n### 数据集外结果\n\n```python\nfrom scipy import ndimage\nimport scipy.misc\n\nmy_image = \"my_image.jpg\"\nmy_label_y = [0]\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * 3, 1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\nplt.imshow(image)\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)), ].decode(\"utf-8\") + \"\\\" picture.\")\n```\n\n>Accuracy: 1.0\n>y = 1.0, your L-layer model predicts a \"cat\" picture.\n\n![](/images/my_image.jpg)","source":"_posts/DNN应用1-识别猫.md","raw":"---\ntitle: DNN应用1--识别猫\ndate: 2018-08-03 08:52:05\ntags: DNN应用\ncategories: 深度学习\n---\n## 实验目的\n\n使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。\n\n## 实验方案\n\n- 使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。\n- 各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理\n- 损失函数采用交叉熵，并使用L2正则化\n- 网络架构\n ![](/images/LlayerNN.png)\n\n## 详细设计\n\n### 数据预处理\n\n#### 加载数据\n\n```python\nimport numpy as np\nimport h5py\n\ndef load_data():\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n```\n\n```python\ntrain_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n```\n\n#### 数据集形状\n\n>Number of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n\n#### 展示数据图片\n\n```python\nimport matplotlib.pyplot as plt\n\nindex = 7\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\nplt.show()\n```\n\n#### 图像矩阵向量化\n\n```python\n# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n```\n\n#### 数据集最终形状\n\n>train_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)\n\n### 网络设计\n\n1. 初始化参数 / 定义超参数\n2. 迭代循环:\n    a. 前向传播\n    b. 计算代价函数\n    c. 反向传播\n    d. 更新参数 \n3. 使用训练的参数去预测新的数据标签\n\n网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 实验结果\n\n### 训练集结果\n\n```python\nlayers_dims = [12288, 20, 7, 5, 1] #  5-layer model\nparameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n```\n\n![](/images/res1.PNG)\n\n```python\npred_train = predict(train_x, train_y, parameters)\n```\n\n>Accuracy: 0.9856459330143539\n\n### 测试集结果\n\n```python\npred_test = predict(test_x, test_y, parameters)\n```\n\n>Accuracy: 0.8\n\n### 数据集外结果\n\n```python\nfrom scipy import ndimage\nimport scipy.misc\n\nmy_image = \"my_image.jpg\"\nmy_label_y = [0]\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * 3, 1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\nplt.imshow(image)\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)), ].decode(\"utf-8\") + \"\\\" picture.\")\n```\n\n>Accuracy: 1.0\n>y = 1.0, your L-layer model predicts a \"cat\" picture.\n\n![](/images/my_image.jpg)","slug":"DNN应用1-识别猫","published":1,"updated":"2018-08-07T00:36:25.220Z","_id":"cjkhjrlan000a3bcp2eqwt6n0","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h2><p>使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。</p>\n<h2 id=\"实验方案\"><a href=\"#实验方案\" class=\"headerlink\" title=\"实验方案\"></a>实验方案</h2><ul>\n<li>使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。</li>\n<li>各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理</li>\n<li>损失函数采用交叉熵，并使用L2正则化</li>\n<li>网络架构<br><img src=\"/images/LlayerNN.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"详细设计\"><a href=\"#详细设计\" class=\"headerlink\" title=\"详细设计\"></a>详细设计</h2><h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><h4 id=\"加载数据\"><a href=\"#加载数据\" class=\"headerlink\" title=\"加载数据\"></a>加载数据</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> h5py</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:])  <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:])  <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:])  <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:])  <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:])  <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集形状\"><a href=\"#数据集形状\" class=\"headerlink\" title=\"数据集形状\"></a>数据集形状</h4><blockquote>\n<p>Number of training examples: 209<br>Number of testing examples: 50<br>Each image is of size: (64, 64, 3)<br>train_x_orig shape: (209, 64, 64, 3)<br>train_y shape: (1, 209)<br>test_x_orig shape: (50, 64, 64, 3)<br>test_y shape: (1, 50)</p>\n</blockquote>\n<h4 id=\"展示数据图片\"><a href=\"#展示数据图片\" class=\"headerlink\" title=\"展示数据图片\"></a>展示数据图片</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">index = <span class=\"number\">7</span></span><br><span class=\"line\">plt.imshow(train_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_y[<span class=\"number\">0</span>,index]) + <span class=\"string\">\". It's a \"</span> + classes[train_y[<span class=\"number\">0</span>,index]].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\" picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h4 id=\"图像矩阵向量化\"><a href=\"#图像矩阵向量化\" class=\"headerlink\" title=\"图像矩阵向量化\"></a>图像矩阵向量化</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Reshape the training and test examples </span></span><br><span class=\"line\">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T   <span class=\"comment\"># The \"-1\" makes reshape flatten the remaining dimensions</span></span><br><span class=\"line\">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Standardize data to have feature values between 0 and 1.</span></span><br><span class=\"line\">train_x = train_x_flatten/<span class=\"number\">255.</span></span><br><span class=\"line\">test_x = test_x_flatten/<span class=\"number\">255.</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集最终形状\"><a href=\"#数据集最终形状\" class=\"headerlink\" title=\"数据集最终形状\"></a>数据集最终形状</h4><blockquote>\n<p>train_x’s shape: (12288, 209)<br>test_x’s shape: (12288, 50)</p>\n</blockquote>\n<h3 id=\"网络设计\"><a href=\"#网络设计\" class=\"headerlink\" title=\"网络设计\"></a>网络设计</h3><ol>\n<li>初始化参数 / 定义超参数</li>\n<li>迭代循环:<br> a. 前向传播<br> b. 计算代价函数<br> c. 反向传播<br> d. 更新参数 </li>\n<li>使用训练的参数去预测新的数据标签</li>\n</ol>\n<p>网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><h3 id=\"训练集结果\"><a href=\"#训练集结果\" class=\"headerlink\" title=\"训练集结果\"></a>训练集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layers_dims = [<span class=\"number\">12288</span>, <span class=\"number\">20</span>, <span class=\"number\">7</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>] <span class=\"comment\">#  5-layer model</span></span><br><span class=\"line\">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class=\"number\">2500</span>, print_cost = <span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/images/res1.PNG\" alt=\"\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.9856459330143539</p>\n</blockquote>\n<h3 id=\"测试集结果\"><a href=\"#测试集结果\" class=\"headerlink\" title=\"测试集结果\"></a>测试集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.8</p>\n</blockquote>\n<h3 id=\"数据集外结果\"><a href=\"#数据集外结果\" class=\"headerlink\" title=\"数据集外结果\"></a>数据集外结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy <span class=\"keyword\">import</span> ndimage</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.misc</span><br><span class=\"line\"></span><br><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">0</span>]</span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * <span class=\"number\">3</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\">print(<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)), ].decode(<span class=\"string\">\"utf-8\"</span>) + <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 1.0<br>y = 1.0, your L-layer model predicts a “cat” picture.</p>\n</blockquote>\n<p><img src=\"/images/my_image.jpg\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h2><p>使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。</p>\n<h2 id=\"实验方案\"><a href=\"#实验方案\" class=\"headerlink\" title=\"实验方案\"></a>实验方案</h2><ul>\n<li>使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。</li>\n<li>各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理</li>\n<li>损失函数采用交叉熵，并使用L2正则化</li>\n<li>网络架构<br><img src=\"/images/LlayerNN.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"详细设计\"><a href=\"#详细设计\" class=\"headerlink\" title=\"详细设计\"></a>详细设计</h2><h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><h4 id=\"加载数据\"><a href=\"#加载数据\" class=\"headerlink\" title=\"加载数据\"></a>加载数据</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> h5py</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:])  <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:])  <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:])  <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:])  <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:])  <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集形状\"><a href=\"#数据集形状\" class=\"headerlink\" title=\"数据集形状\"></a>数据集形状</h4><blockquote>\n<p>Number of training examples: 209<br>Number of testing examples: 50<br>Each image is of size: (64, 64, 3)<br>train_x_orig shape: (209, 64, 64, 3)<br>train_y shape: (1, 209)<br>test_x_orig shape: (50, 64, 64, 3)<br>test_y shape: (1, 50)</p>\n</blockquote>\n<h4 id=\"展示数据图片\"><a href=\"#展示数据图片\" class=\"headerlink\" title=\"展示数据图片\"></a>展示数据图片</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">index = <span class=\"number\">7</span></span><br><span class=\"line\">plt.imshow(train_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_y[<span class=\"number\">0</span>,index]) + <span class=\"string\">\". It's a \"</span> + classes[train_y[<span class=\"number\">0</span>,index]].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\" picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h4 id=\"图像矩阵向量化\"><a href=\"#图像矩阵向量化\" class=\"headerlink\" title=\"图像矩阵向量化\"></a>图像矩阵向量化</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Reshape the training and test examples </span></span><br><span class=\"line\">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T   <span class=\"comment\"># The \"-1\" makes reshape flatten the remaining dimensions</span></span><br><span class=\"line\">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Standardize data to have feature values between 0 and 1.</span></span><br><span class=\"line\">train_x = train_x_flatten/<span class=\"number\">255.</span></span><br><span class=\"line\">test_x = test_x_flatten/<span class=\"number\">255.</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集最终形状\"><a href=\"#数据集最终形状\" class=\"headerlink\" title=\"数据集最终形状\"></a>数据集最终形状</h4><blockquote>\n<p>train_x’s shape: (12288, 209)<br>test_x’s shape: (12288, 50)</p>\n</blockquote>\n<h3 id=\"网络设计\"><a href=\"#网络设计\" class=\"headerlink\" title=\"网络设计\"></a>网络设计</h3><ol>\n<li>初始化参数 / 定义超参数</li>\n<li>迭代循环:<br> a. 前向传播<br> b. 计算代价函数<br> c. 反向传播<br> d. 更新参数 </li>\n<li>使用训练的参数去预测新的数据标签</li>\n</ol>\n<p>网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><h3 id=\"训练集结果\"><a href=\"#训练集结果\" class=\"headerlink\" title=\"训练集结果\"></a>训练集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layers_dims = [<span class=\"number\">12288</span>, <span class=\"number\">20</span>, <span class=\"number\">7</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>] <span class=\"comment\">#  5-layer model</span></span><br><span class=\"line\">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class=\"number\">2500</span>, print_cost = <span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/images/res1.PNG\" alt=\"\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.9856459330143539</p>\n</blockquote>\n<h3 id=\"测试集结果\"><a href=\"#测试集结果\" class=\"headerlink\" title=\"测试集结果\"></a>测试集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.8</p>\n</blockquote>\n<h3 id=\"数据集外结果\"><a href=\"#数据集外结果\" class=\"headerlink\" title=\"数据集外结果\"></a>数据集外结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy <span class=\"keyword\">import</span> ndimage</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.misc</span><br><span class=\"line\"></span><br><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">0</span>]</span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * <span class=\"number\">3</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\">print(<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)), ].decode(<span class=\"string\">\"utf-8\"</span>) + <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 1.0<br>y = 1.0, your L-layer model predicts a “cat” picture.</p>\n</blockquote>\n<p><img src=\"/images/my_image.jpg\" alt=\"\"></p>\n"},{"title":"How to set up a blog with hexo on github.io","date":"2018-07-18T10:17:31.000Z","_content":"### Install Git (https://git-scm.com/)\n\n* If you have a git, you can check it by `git -v`\n\n### Install node.js envornment \n\n* Download from [https://nodejs.org/en/](https://nodejs.org/en/)\n* Install as the default(make sure the envorment path is collected)\n* When you finsh, you can check it by `node -v`\n\n###  Make a new repository named \"<your_username>.github.io\"\n\n### Install Hexo\n\n* `npm install hexo-cli -g`\n* `hexo init blog`(that folder you wanted to store your webpage)\n* `cd blog`\n* `npm install`\n* `hexo server`\n\n### Connect hexo with github\n\n* `cd blog`\n* `git config --global user.name \"<your_username>\"`\n* `git config --global user.email \"<your_email>\"`\n\nCheck if you have a ssh keygen. If not you can do as following\n\n* `cd ~/.ssh`\n* generate key: `ssh-kengen -t rsa -C \"<your_email>\"` (choice default setting)\n* add key to ssh-agent: `eval \"$(ssh-agent -s)\"`\n* `ssh-add ~/.ssh/id_rsa`\n\n### Sign in the github, in settings, add a new ssh key, copy the `id_rsa.pub` to key options. check if it's ok by `ssh -T git@github.com`. If you get a hi message, it is ok.\n\n### In your blog folder, edit the _config.yml file like this.\n\n```\n(in the end)\ndeploy:\n    type: git\n    repository: git@github.com:<your_username>/<your_username>.github.io.git\n    branch: master\n```\n\n### Before deploy the blog website, you should install a plug\n\n* `cd blog`\n* `npm install hexo-deployer-git --save`\n\n### Run it online.\n\n* `hexo clean`\n* `hexo g`\n* `hexo d`\n\n","source":"_posts/How-to-set-up-a-blog-with-hexo-on-github-io.md","raw":"---\ntitle: How to set up a blog with hexo on github.io\ndate: 2018-07-18 18:17:31\ntags: hexo\ncategories: web\n---\n### Install Git (https://git-scm.com/)\n\n* If you have a git, you can check it by `git -v`\n\n### Install node.js envornment \n\n* Download from [https://nodejs.org/en/](https://nodejs.org/en/)\n* Install as the default(make sure the envorment path is collected)\n* When you finsh, you can check it by `node -v`\n\n###  Make a new repository named \"<your_username>.github.io\"\n\n### Install Hexo\n\n* `npm install hexo-cli -g`\n* `hexo init blog`(that folder you wanted to store your webpage)\n* `cd blog`\n* `npm install`\n* `hexo server`\n\n### Connect hexo with github\n\n* `cd blog`\n* `git config --global user.name \"<your_username>\"`\n* `git config --global user.email \"<your_email>\"`\n\nCheck if you have a ssh keygen. If not you can do as following\n\n* `cd ~/.ssh`\n* generate key: `ssh-kengen -t rsa -C \"<your_email>\"` (choice default setting)\n* add key to ssh-agent: `eval \"$(ssh-agent -s)\"`\n* `ssh-add ~/.ssh/id_rsa`\n\n### Sign in the github, in settings, add a new ssh key, copy the `id_rsa.pub` to key options. check if it's ok by `ssh -T git@github.com`. If you get a hi message, it is ok.\n\n### In your blog folder, edit the _config.yml file like this.\n\n```\n(in the end)\ndeploy:\n    type: git\n    repository: git@github.com:<your_username>/<your_username>.github.io.git\n    branch: master\n```\n\n### Before deploy the blog website, you should install a plug\n\n* `cd blog`\n* `npm install hexo-deployer-git --save`\n\n### Run it online.\n\n* `hexo clean`\n* `hexo g`\n* `hexo d`\n\n","slug":"How-to-set-up-a-blog-with-hexo-on-github-io","published":1,"updated":"2018-08-07T00:36:25.222Z","_id":"cjkhjrlao000b3bcptfl6xk6j","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"Install-Git-https-git-scm-com\"><a href=\"#Install-Git-https-git-scm-com\" class=\"headerlink\" title=\"Install Git (https://git-scm.com/)\"></a>Install Git (<a href=\"https://git-scm.com/\" target=\"_blank\" rel=\"noopener\">https://git-scm.com/</a>)</h3><ul>\n<li>If you have a git, you can check it by <code>git -v</code></li>\n</ul>\n<h3 id=\"Install-node-js-envornment\"><a href=\"#Install-node-js-envornment\" class=\"headerlink\" title=\"Install node.js envornment\"></a>Install node.js envornment</h3><ul>\n<li>Download from <a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">https://nodejs.org/en/</a></li>\n<li>Install as the default(make sure the envorment path is collected)</li>\n<li>When you finsh, you can check it by <code>node -v</code></li>\n</ul>\n<h3 id=\"Make-a-new-repository-named-“-lt-your-username-gt-github-io”\"><a href=\"#Make-a-new-repository-named-“-lt-your-username-gt-github-io”\" class=\"headerlink\" title=\"Make a new repository named “&lt;your_username&gt;.github.io”\"></a>Make a new repository named “&lt;your_username&gt;.github.io”</h3><h3 id=\"Install-Hexo\"><a href=\"#Install-Hexo\" class=\"headerlink\" title=\"Install Hexo\"></a>Install Hexo</h3><ul>\n<li><code>npm install hexo-cli -g</code></li>\n<li><code>hexo init blog</code>(that folder you wanted to store your webpage)</li>\n<li><code>cd blog</code></li>\n<li><code>npm install</code></li>\n<li><code>hexo server</code></li>\n</ul>\n<h3 id=\"Connect-hexo-with-github\"><a href=\"#Connect-hexo-with-github\" class=\"headerlink\" title=\"Connect hexo with github\"></a>Connect hexo with github</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>git config --global user.name &quot;&lt;your_username&gt;&quot;</code></li>\n<li><code>git config --global user.email &quot;&lt;your_email&gt;&quot;</code></li>\n</ul>\n<p>Check if you have a ssh keygen. If not you can do as following</p>\n<ul>\n<li><code>cd ~/.ssh</code></li>\n<li>generate key: <code>ssh-kengen -t rsa -C &quot;&lt;your_email&gt;&quot;</code> (choice default setting)</li>\n<li>add key to ssh-agent: <code>eval &quot;$(ssh-agent -s)&quot;</code></li>\n<li><code>ssh-add ~/.ssh/id_rsa</code></li>\n</ul>\n<h3 id=\"Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\"><a href=\"#Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\" class=\"headerlink\" title=\"Sign in the github, in settings, add a new ssh key, copy the id_rsa.pub to key options. check if it’s ok by ssh -T git@github.com. If you get a hi message, it is ok.\"></a>Sign in the github, in settings, add a new ssh key, copy the <code>id_rsa.pub</code> to key options. check if it’s ok by <code>ssh -T git@github.com</code>. If you get a hi message, it is ok.</h3><h3 id=\"In-your-blog-folder-edit-the-config-yml-file-like-this\"><a href=\"#In-your-blog-folder-edit-the-config-yml-file-like-this\" class=\"headerlink\" title=\"In your blog folder, edit the _config.yml file like this.\"></a>In your blog folder, edit the _config.yml file like this.</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(in the end)</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">    type: git</span><br><span class=\"line\">    repository: git@github.com:&lt;your_username&gt;/&lt;your_username&gt;.github.io.git</span><br><span class=\"line\">    branch: master</span><br></pre></td></tr></table></figure>\n<h3 id=\"Before-deploy-the-blog-website-you-should-install-a-plug\"><a href=\"#Before-deploy-the-blog-website-you-should-install-a-plug\" class=\"headerlink\" title=\"Before deploy the blog website, you should install a plug\"></a>Before deploy the blog website, you should install a plug</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>npm install hexo-deployer-git --save</code></li>\n</ul>\n<h3 id=\"Run-it-online\"><a href=\"#Run-it-online\" class=\"headerlink\" title=\"Run it online.\"></a>Run it online.</h3><ul>\n<li><code>hexo clean</code></li>\n<li><code>hexo g</code></li>\n<li><code>hexo d</code></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Install-Git-https-git-scm-com\"><a href=\"#Install-Git-https-git-scm-com\" class=\"headerlink\" title=\"Install Git (https://git-scm.com/)\"></a>Install Git (<a href=\"https://git-scm.com/\" target=\"_blank\" rel=\"noopener\">https://git-scm.com/</a>)</h3><ul>\n<li>If you have a git, you can check it by <code>git -v</code></li>\n</ul>\n<h3 id=\"Install-node-js-envornment\"><a href=\"#Install-node-js-envornment\" class=\"headerlink\" title=\"Install node.js envornment\"></a>Install node.js envornment</h3><ul>\n<li>Download from <a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">https://nodejs.org/en/</a></li>\n<li>Install as the default(make sure the envorment path is collected)</li>\n<li>When you finsh, you can check it by <code>node -v</code></li>\n</ul>\n<h3 id=\"Make-a-new-repository-named-“-lt-your-username-gt-github-io”\"><a href=\"#Make-a-new-repository-named-“-lt-your-username-gt-github-io”\" class=\"headerlink\" title=\"Make a new repository named “&lt;your_username&gt;.github.io”\"></a>Make a new repository named “&lt;your_username&gt;.github.io”</h3><h3 id=\"Install-Hexo\"><a href=\"#Install-Hexo\" class=\"headerlink\" title=\"Install Hexo\"></a>Install Hexo</h3><ul>\n<li><code>npm install hexo-cli -g</code></li>\n<li><code>hexo init blog</code>(that folder you wanted to store your webpage)</li>\n<li><code>cd blog</code></li>\n<li><code>npm install</code></li>\n<li><code>hexo server</code></li>\n</ul>\n<h3 id=\"Connect-hexo-with-github\"><a href=\"#Connect-hexo-with-github\" class=\"headerlink\" title=\"Connect hexo with github\"></a>Connect hexo with github</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>git config --global user.name &quot;&lt;your_username&gt;&quot;</code></li>\n<li><code>git config --global user.email &quot;&lt;your_email&gt;&quot;</code></li>\n</ul>\n<p>Check if you have a ssh keygen. If not you can do as following</p>\n<ul>\n<li><code>cd ~/.ssh</code></li>\n<li>generate key: <code>ssh-kengen -t rsa -C &quot;&lt;your_email&gt;&quot;</code> (choice default setting)</li>\n<li>add key to ssh-agent: <code>eval &quot;$(ssh-agent -s)&quot;</code></li>\n<li><code>ssh-add ~/.ssh/id_rsa</code></li>\n</ul>\n<h3 id=\"Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\"><a href=\"#Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\" class=\"headerlink\" title=\"Sign in the github, in settings, add a new ssh key, copy the id_rsa.pub to key options. check if it’s ok by ssh -T git@github.com. If you get a hi message, it is ok.\"></a>Sign in the github, in settings, add a new ssh key, copy the <code>id_rsa.pub</code> to key options. check if it’s ok by <code>ssh -T git@github.com</code>. If you get a hi message, it is ok.</h3><h3 id=\"In-your-blog-folder-edit-the-config-yml-file-like-this\"><a href=\"#In-your-blog-folder-edit-the-config-yml-file-like-this\" class=\"headerlink\" title=\"In your blog folder, edit the _config.yml file like this.\"></a>In your blog folder, edit the _config.yml file like this.</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(in the end)</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">    type: git</span><br><span class=\"line\">    repository: git@github.com:&lt;your_username&gt;/&lt;your_username&gt;.github.io.git</span><br><span class=\"line\">    branch: master</span><br></pre></td></tr></table></figure>\n<h3 id=\"Before-deploy-the-blog-website-you-should-install-a-plug\"><a href=\"#Before-deploy-the-blog-website-you-should-install-a-plug\" class=\"headerlink\" title=\"Before deploy the blog website, you should install a plug\"></a>Before deploy the blog website, you should install a plug</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>npm install hexo-deployer-git --save</code></li>\n</ul>\n<h3 id=\"Run-it-online\"><a href=\"#Run-it-online\" class=\"headerlink\" title=\"Run it online.\"></a>Run it online.</h3><ul>\n<li><code>hexo clean</code></li>\n<li><code>hexo g</code></li>\n<li><code>hexo d</code></li>\n</ul>\n"},{"title":"dropout 正则化","date":"2018-07-20T08:18:24.000Z","mathjax":true,"_content":"## dropout 正则化\n\n**dropout（随机失活）**是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在**计算机视觉（Computer Vision）**领域。\n\n![dropout_regularization](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png)\n\n### 反向随机失活（Inverted dropout）\n\n反向随机失活是实现 dropout 的方法。对第`l`层进行 dropout：\n\n```python\nkeep_prob = 0.8    # 设置神经元保留概率\ndl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob\nal = np.multiply(al, dl)\nal /= keep_prob\n\n# 反向传播过程为\ndal = dal * dl\ndal /= keep_prob\n```\n\n最后一步`al /= keep_prob`是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个`keep_prob`。\n\n**注意**，在**测试阶段不要使用 dropout**，因为那样会使得预测结果变得随机。\n\n### 理解 dropout\n\n对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。\n\n因此，通过传播过程，dropout 将产生和 L2 正则化相同的**收缩权重**的效果。\n\n对于不同的层，设置的`keep_prob`也不同。一般来说，神经元较少的层，会设`keep_prob`为 1.0，而神经元多的层则会设置比较小的`keep_prob`。\n\ndropout 的一大**缺点**是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将`keep_prob`全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。\n\n## 其他正则化方法\n\n* 数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。\n* 早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。\n","source":"_posts/dropout.md","raw":"---\ntitle: dropout 正则化\ndate: 2018-07-20 16:18:24\ntags: dropout\ncategories: 深度学习的实用层面\nmathjax: true\n---\n## dropout 正则化\n\n**dropout（随机失活）**是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在**计算机视觉（Computer Vision）**领域。\n\n![dropout_regularization](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png)\n\n### 反向随机失活（Inverted dropout）\n\n反向随机失活是实现 dropout 的方法。对第`l`层进行 dropout：\n\n```python\nkeep_prob = 0.8    # 设置神经元保留概率\ndl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob\nal = np.multiply(al, dl)\nal /= keep_prob\n\n# 反向传播过程为\ndal = dal * dl\ndal /= keep_prob\n```\n\n最后一步`al /= keep_prob`是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个`keep_prob`。\n\n**注意**，在**测试阶段不要使用 dropout**，因为那样会使得预测结果变得随机。\n\n### 理解 dropout\n\n对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。\n\n因此，通过传播过程，dropout 将产生和 L2 正则化相同的**收缩权重**的效果。\n\n对于不同的层，设置的`keep_prob`也不同。一般来说，神经元较少的层，会设`keep_prob`为 1.0，而神经元多的层则会设置比较小的`keep_prob`。\n\ndropout 的一大**缺点**是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将`keep_prob`全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。\n\n## 其他正则化方法\n\n* 数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。\n* 早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。\n","slug":"dropout","published":1,"updated":"2018-08-07T00:36:25.223Z","_id":"cjkhjrlaq000e3bcpqzriecmo","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"dropout-正则化\"><a href=\"#dropout-正则化\" class=\"headerlink\" title=\"dropout 正则化\"></a>dropout 正则化</h2><p><strong>dropout（随机失活）</strong>是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在<strong>计算机视觉（Computer Vision）</strong>领域。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png\" alt=\"dropout_regularization\"></p>\n<h3 id=\"反向随机失活（Inverted-dropout）\"><a href=\"#反向随机失活（Inverted-dropout）\" class=\"headerlink\" title=\"反向随机失活（Inverted dropout）\"></a>反向随机失活（Inverted dropout）</h3><p>反向随机失活是实现 dropout 的方法。对第<code>l</code>层进行 dropout：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keep_prob = <span class=\"number\">0.8</span>    <span class=\"comment\"># 设置神经元保留概率</span></span><br><span class=\"line\">dl = np.random.rand(al.shape[<span class=\"number\">0</span>], al.shape[<span class=\"number\">1</span>]) &lt; keep_prob</span><br><span class=\"line\">al = np.multiply(al, dl)</span><br><span class=\"line\">al /= keep_prob</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 反向传播过程为</span></span><br><span class=\"line\">dal = dal * dl</span><br><span class=\"line\">dal /= keep_prob</span><br></pre></td></tr></table></figure>\n<p>最后一步<code>al /= keep_prob</code>是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个<code>keep_prob</code>。</p>\n<p><strong>注意</strong>，在<strong>测试阶段不要使用 dropout</strong>，因为那样会使得预测结果变得随机。</p>\n<h3 id=\"理解-dropout\"><a href=\"#理解-dropout\" class=\"headerlink\" title=\"理解 dropout\"></a>理解 dropout</h3><p>对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。</p>\n<p>因此，通过传播过程，dropout 将产生和 L2 正则化相同的<strong>收缩权重</strong>的效果。</p>\n<p>对于不同的层，设置的<code>keep_prob</code>也不同。一般来说，神经元较少的层，会设<code>keep_prob</code>为 1.0，而神经元多的层则会设置比较小的<code>keep_prob</code>。</p>\n<p>dropout 的一大<strong>缺点</strong>是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将<code>keep_prob</code>全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。</p>\n<h2 id=\"其他正则化方法\"><a href=\"#其他正则化方法\" class=\"headerlink\" title=\"其他正则化方法\"></a>其他正则化方法</h2><ul>\n<li>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</li>\n<li>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"dropout-正则化\"><a href=\"#dropout-正则化\" class=\"headerlink\" title=\"dropout 正则化\"></a>dropout 正则化</h2><p><strong>dropout（随机失活）</strong>是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在<strong>计算机视觉（Computer Vision）</strong>领域。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png\" alt=\"dropout_regularization\"></p>\n<h3 id=\"反向随机失活（Inverted-dropout）\"><a href=\"#反向随机失活（Inverted-dropout）\" class=\"headerlink\" title=\"反向随机失活（Inverted dropout）\"></a>反向随机失活（Inverted dropout）</h3><p>反向随机失活是实现 dropout 的方法。对第<code>l</code>层进行 dropout：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keep_prob = <span class=\"number\">0.8</span>    <span class=\"comment\"># 设置神经元保留概率</span></span><br><span class=\"line\">dl = np.random.rand(al.shape[<span class=\"number\">0</span>], al.shape[<span class=\"number\">1</span>]) &lt; keep_prob</span><br><span class=\"line\">al = np.multiply(al, dl)</span><br><span class=\"line\">al /= keep_prob</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 反向传播过程为</span></span><br><span class=\"line\">dal = dal * dl</span><br><span class=\"line\">dal /= keep_prob</span><br></pre></td></tr></table></figure>\n<p>最后一步<code>al /= keep_prob</code>是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个<code>keep_prob</code>。</p>\n<p><strong>注意</strong>，在<strong>测试阶段不要使用 dropout</strong>，因为那样会使得预测结果变得随机。</p>\n<h3 id=\"理解-dropout\"><a href=\"#理解-dropout\" class=\"headerlink\" title=\"理解 dropout\"></a>理解 dropout</h3><p>对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。</p>\n<p>因此，通过传播过程，dropout 将产生和 L2 正则化相同的<strong>收缩权重</strong>的效果。</p>\n<p>对于不同的层，设置的<code>keep_prob</code>也不同。一般来说，神经元较少的层，会设<code>keep_prob</code>为 1.0，而神经元多的层则会设置比较小的<code>keep_prob</code>。</p>\n<p>dropout 的一大<strong>缺点</strong>是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将<code>keep_prob</code>全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。</p>\n<h2 id=\"其他正则化方法\"><a href=\"#其他正则化方法\" class=\"headerlink\" title=\"其他正则化方法\"></a>其他正则化方法</h2><ul>\n<li>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</li>\n<li>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。</li>\n</ul>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncategories: web\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2018-08-07T00:36:25.223Z","updated":"2018-08-07T00:36:25.224Z","_id":"cjkhjrlar000f3bcppm2z2umx","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"},{"title":"python内置小工具","date":"2018-08-04T23:24:47.000Z","_content":"\n## 极简文件下载（Web）服务器\n\n### 作用\n\n快速共享文件\n\n### 实用方法\n\nIn python2：\n\n`python -m SimpleHttpServer`\n\nIn python3:\n\n`python -m http.server`\n\n执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。**若当前目录存在一个名为`index.html`的文件，则默认会显示该文件的内容**\n\n## 使用python解压zip压缩包\n\n`$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources\n`","source":"_posts/python内置小工具.md","raw":"---\ntitle: python内置小工具\ndate: 2018-08-05 07:24:47\ntags: 程序员实用工具\ncategories: 程序员实用工具\n---\n\n## 极简文件下载（Web）服务器\n\n### 作用\n\n快速共享文件\n\n### 实用方法\n\nIn python2：\n\n`python -m SimpleHttpServer`\n\nIn python3:\n\n`python -m http.server`\n\n执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。**若当前目录存在一个名为`index.html`的文件，则默认会显示该文件的内容**\n\n## 使用python解压zip压缩包\n\n`$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources\n`","slug":"python内置小工具","published":1,"updated":"2018-08-07T00:36:25.225Z","_id":"cjkhjrlas000g3bcplvyckq5p","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"极简文件下载（Web）服务器\"><a href=\"#极简文件下载（Web）服务器\" class=\"headerlink\" title=\"极简文件下载（Web）服务器\"></a>极简文件下载（Web）服务器</h2><h3 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h3><p>快速共享文件</p>\n<h3 id=\"实用方法\"><a href=\"#实用方法\" class=\"headerlink\" title=\"实用方法\"></a>实用方法</h3><p>In python2：</p>\n<p><code>python -m SimpleHttpServer</code></p>\n<p>In python3:</p>\n<p><code>python -m http.server</code></p>\n<p>执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。<strong>若当前目录存在一个名为<code>index.html</code>的文件，则默认会显示该文件的内容</strong></p>\n<h2 id=\"使用python解压zip压缩包\"><a href=\"#使用python解压zip压缩包\" class=\"headerlink\" title=\"使用python解压zip压缩包\"></a>使用python解压zip压缩包</h2><p><code>$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources</code></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"极简文件下载（Web）服务器\"><a href=\"#极简文件下载（Web）服务器\" class=\"headerlink\" title=\"极简文件下载（Web）服务器\"></a>极简文件下载（Web）服务器</h2><h3 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h3><p>快速共享文件</p>\n<h3 id=\"实用方法\"><a href=\"#实用方法\" class=\"headerlink\" title=\"实用方法\"></a>实用方法</h3><p>In python2：</p>\n<p><code>python -m SimpleHttpServer</code></p>\n<p>In python3:</p>\n<p><code>python -m http.server</code></p>\n<p>执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。<strong>若当前目录存在一个名为<code>index.html</code>的文件，则默认会显示该文件的内容</strong></p>\n<h2 id=\"使用python解压zip压缩包\"><a href=\"#使用python解压zip压缩包\" class=\"headerlink\" title=\"使用python解压zip压缩包\"></a>使用python解压zip压缩包</h2><p><code>$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources</code></p>\n"},{"title":"人生--路遥","date":"2018-07-19T15:20:58.000Z","_content":"\n### 故事梗概\n\n小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。\n\n### 内容简介\n\n#### 回到土地\n\n主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个\n“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。\n\n#### 离开土地\n\n城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。\n\n#### 回到土地\n\n但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦......）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。\n\n### 创作背景\n\n20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。\n\n20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。\n\n《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。\n\n当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。\n\n早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。\n\n### 人物介绍\n\n#### 高加林\n\n高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。\n\n#### 刘巧珍\n\n巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。\n\n---\n摘自《百度百科词条：人生》","source":"_posts/人生-路遥.md","raw":"---\ntitle: 人生--路遥\ndate: 2018-07-19 23:20:58\ntags: 路遥, 人生\ncategories: 文学\n---\n\n### 故事梗概\n\n小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。\n\n### 内容简介\n\n#### 回到土地\n\n主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个\n“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。\n\n#### 离开土地\n\n城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。\n\n#### 回到土地\n\n但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦......）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。\n\n### 创作背景\n\n20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。\n\n20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。\n\n《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。\n\n当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。\n\n早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。\n\n### 人物介绍\n\n#### 高加林\n\n高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。\n\n#### 刘巧珍\n\n巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。\n\n---\n摘自《百度百科词条：人生》","slug":"人生-路遥","published":1,"updated":"2018-08-07T00:36:25.225Z","_id":"cjkhjrlau000k3bcp9jowogki","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"故事梗概\"><a href=\"#故事梗概\" class=\"headerlink\" title=\"故事梗概\"></a>故事梗概</h3><p>小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。</p>\n<h3 id=\"内容简介\"><a href=\"#内容简介\" class=\"headerlink\" title=\"内容简介\"></a>内容简介</h3><h4 id=\"回到土地\"><a href=\"#回到土地\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个<br>“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。</p>\n<h4 id=\"离开土地\"><a href=\"#离开土地\" class=\"headerlink\" title=\"离开土地\"></a>离开土地</h4><p>城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。</p>\n<h4 id=\"回到土地-1\"><a href=\"#回到土地-1\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦……）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。</p>\n<h3 id=\"创作背景\"><a href=\"#创作背景\" class=\"headerlink\" title=\"创作背景\"></a>创作背景</h3><p>20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。</p>\n<p>20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。</p>\n<p>《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。</p>\n<p>当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。</p>\n<p>早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。</p>\n<h3 id=\"人物介绍\"><a href=\"#人物介绍\" class=\"headerlink\" title=\"人物介绍\"></a>人物介绍</h3><h4 id=\"高加林\"><a href=\"#高加林\" class=\"headerlink\" title=\"高加林\"></a>高加林</h4><p>高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。</p>\n<h4 id=\"刘巧珍\"><a href=\"#刘巧珍\" class=\"headerlink\" title=\"刘巧珍\"></a>刘巧珍</h4><p>巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。</p>\n<hr>\n<p>摘自《百度百科词条：人生》</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"故事梗概\"><a href=\"#故事梗概\" class=\"headerlink\" title=\"故事梗概\"></a>故事梗概</h3><p>小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。</p>\n<h3 id=\"内容简介\"><a href=\"#内容简介\" class=\"headerlink\" title=\"内容简介\"></a>内容简介</h3><h4 id=\"回到土地\"><a href=\"#回到土地\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个<br>“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。</p>\n<h4 id=\"离开土地\"><a href=\"#离开土地\" class=\"headerlink\" title=\"离开土地\"></a>离开土地</h4><p>城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。</p>\n<h4 id=\"回到土地-1\"><a href=\"#回到土地-1\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦……）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。</p>\n<h3 id=\"创作背景\"><a href=\"#创作背景\" class=\"headerlink\" title=\"创作背景\"></a>创作背景</h3><p>20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。</p>\n<p>20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。</p>\n<p>《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。</p>\n<p>当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。</p>\n<p>早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。</p>\n<h3 id=\"人物介绍\"><a href=\"#人物介绍\" class=\"headerlink\" title=\"人物介绍\"></a>人物介绍</h3><h4 id=\"高加林\"><a href=\"#高加林\" class=\"headerlink\" title=\"高加林\"></a>高加林</h4><p>高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。</p>\n<h4 id=\"刘巧珍\"><a href=\"#刘巧珍\" class=\"headerlink\" title=\"刘巧珍\"></a>刘巧珍</h4><p>巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。</p>\n<hr>\n<p>摘自《百度百科词条：人生》</p>\n"},{"title":"初始化参数","date":"2018-07-22T05:22:45.000Z","_content":"\n## Initialization\n\nTraining your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  \n\nA well chosen initialization can:\n- Speed up the convergence of gradient descent\n- Increase the odds of gradient descent converging to a lower training (and generalization) error \n\n## Random initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n## He initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n\n**What you should remember from this artical**:\n- Different initializations lead to different results\n- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n- Don't intialize to values that are too large\n- He initialization works well for networks with ReLU activations. ","source":"_posts/初始化参数.md","raw":"---\ntitle: 初始化参数\ndate: 2018-07-22 13:22:45\ntags: 初始化参数\ncategories: 深度学习的实用层面\n---\n\n## Initialization\n\nTraining your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  \n\nA well chosen initialization can:\n- Speed up the convergence of gradient descent\n- Increase the odds of gradient descent converging to a lower training (and generalization) error \n\n## Random initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n## He initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n\n**What you should remember from this artical**:\n- Different initializations lead to different results\n- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n- Don't intialize to values that are too large\n- He initialization works well for networks with ReLU activations. ","slug":"初始化参数","published":1,"updated":"2018-08-07T00:36:25.226Z","_id":"cjkhjrlfy00123bcp408tow62","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Initialization\"><a href=\"#Initialization\" class=\"headerlink\" title=\"Initialization\"></a>Initialization</h2><p>Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  </p>\n<p>A well chosen initialization can:</p>\n<ul>\n<li>Speed up the convergence of gradient descent</li>\n<li>Increase the odds of gradient descent converging to a lower training (and generalization) error </li>\n</ul>\n<h2 id=\"Random-initialization\"><a href=\"#Random-initialization\" class=\"headerlink\" title=\"Random initialization\"></a>Random initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * <span class=\"number\">0.01</span></span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<h2 id=\"He-initialization\"><a href=\"#He-initialization\" class=\"headerlink\" title=\"He initialization\"></a>He initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * np.sqrt(<span class=\"number\">2</span> / layers_dims[l<span class=\"number\">-1</span>])</span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><strong>What you should remember from this artical</strong>:</p>\n<ul>\n<li>Different initializations lead to different results</li>\n<li>Random initialization is used to break symmetry and make sure different hidden units can learn different things</li>\n<li>Don’t intialize to values that are too large</li>\n<li>He initialization works well for networks with ReLU activations. </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Initialization\"><a href=\"#Initialization\" class=\"headerlink\" title=\"Initialization\"></a>Initialization</h2><p>Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  </p>\n<p>A well chosen initialization can:</p>\n<ul>\n<li>Speed up the convergence of gradient descent</li>\n<li>Increase the odds of gradient descent converging to a lower training (and generalization) error </li>\n</ul>\n<h2 id=\"Random-initialization\"><a href=\"#Random-initialization\" class=\"headerlink\" title=\"Random initialization\"></a>Random initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * <span class=\"number\">0.01</span></span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<h2 id=\"He-initialization\"><a href=\"#He-initialization\" class=\"headerlink\" title=\"He initialization\"></a>He initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * np.sqrt(<span class=\"number\">2</span> / layers_dims[l<span class=\"number\">-1</span>])</span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><strong>What you should remember from this artical</strong>:</p>\n<ul>\n<li>Different initializations lead to different results</li>\n<li>Random initialization is used to break symmetry and make sure different hidden units can learn different things</li>\n<li>Don’t intialize to values that are too large</li>\n<li>He initialization works well for networks with ReLU activations. </li>\n</ul>\n"},{"title":"动态规划","date":"2018-07-21T14:47:53.000Z","mathjax":true,"_content":"## 动态规划（dynamic programming）\n\n与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。\n\n通常用来求解最优化问题。\n\n我们通常按如下4个步骤来设计一个动态规划算法：\n\n* 刻画一个最优解的结构特征\n* 递归地定义最优解的值\n* 计算最优解的值，通常采用自底向上的方法\n* 利用计算出的信息构造一个最优解\n\n## 钢条切割问题\n\n### 问题定义\n\n给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, ..., n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。\n\n### 问题分析\n\n长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为\n\n$$n = i_1 + i_2 + ... + i_k$$\n\n得到的最大收益为\n\n$$r_n = p_{i_1} + p_{i_2} + ... + p_{i_k}$$\n\n当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。\n\n则最优切割收益为\n\n$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, ..., r_{n-1} + r_1)$$\n\n除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。\n\n这样我们得到上述式子的简化版本\n\n$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$\n\n### 代码实现\n\n#### 自顶向下递归实现\n\n```python\ndef cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    if n == 0:\n        return 0\n    q = -1\n    for i in range(1, n+1):\n        q = max(q, p[i] + cut_rod(p, n-i))\n    return q\n```\n\n#### 代码分析\n\n![](/images/动态规划.jpg)\n\n令$T(n)$表示cut_rod的调用次数\n\n$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$\n\n第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$\n\n#### 使用动态规划求解\n\n朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。\n\n动态规划有两种等价的实现。**带备忘的自顶向下**、**自底向上**。这里只给出第二种的代码。\n\n```python\ndef bottom_up_cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    r = range(n + 1)  # to save subproblem's result\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            q = max(q, p[i] + r[j - i])\n            r[j] = q\n    return r[n]\n```\n\n#### 代码分析\n\n自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, ..., n$的子问题。时间复杂度为$\\Theta(n^2)$\n\n#### 扩展代码\n\n前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。\n\n```python\ndef externed_bottom_up_cut_rod(p, n):\n    r = range(n + 1)  # 长度为j的钢条的最大收益值r_j\n    s = range(n + 1)  # 最优解对应的第一条钢条的长度s_j\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            if q < p[i] + r[j - i]:\n                q = p[i] + r[j - i]\n                s[j] = i\n            r[j] = q\n    return r[n]\n\ndef print_cut_rod_solution(p, n):\n    (r, s) = externed_bottom_up_cut_rod(p, n)\n    while n > 0:\n        print(s[n])\n        n = n - s[n]\n```","source":"_posts/动态规划.md","raw":"---\ntitle: 动态规划\ndate: 2018-07-21 22:47:53\ntags: 动态规划\ncategories: 算法导论\nmathjax: true\n---\n## 动态规划（dynamic programming）\n\n与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。\n\n通常用来求解最优化问题。\n\n我们通常按如下4个步骤来设计一个动态规划算法：\n\n* 刻画一个最优解的结构特征\n* 递归地定义最优解的值\n* 计算最优解的值，通常采用自底向上的方法\n* 利用计算出的信息构造一个最优解\n\n## 钢条切割问题\n\n### 问题定义\n\n给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, ..., n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。\n\n### 问题分析\n\n长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为\n\n$$n = i_1 + i_2 + ... + i_k$$\n\n得到的最大收益为\n\n$$r_n = p_{i_1} + p_{i_2} + ... + p_{i_k}$$\n\n当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。\n\n则最优切割收益为\n\n$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, ..., r_{n-1} + r_1)$$\n\n除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。\n\n这样我们得到上述式子的简化版本\n\n$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$\n\n### 代码实现\n\n#### 自顶向下递归实现\n\n```python\ndef cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    if n == 0:\n        return 0\n    q = -1\n    for i in range(1, n+1):\n        q = max(q, p[i] + cut_rod(p, n-i))\n    return q\n```\n\n#### 代码分析\n\n![](/images/动态规划.jpg)\n\n令$T(n)$表示cut_rod的调用次数\n\n$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$\n\n第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$\n\n#### 使用动态规划求解\n\n朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。\n\n动态规划有两种等价的实现。**带备忘的自顶向下**、**自底向上**。这里只给出第二种的代码。\n\n```python\ndef bottom_up_cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    r = range(n + 1)  # to save subproblem's result\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            q = max(q, p[i] + r[j - i])\n            r[j] = q\n    return r[n]\n```\n\n#### 代码分析\n\n自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, ..., n$的子问题。时间复杂度为$\\Theta(n^2)$\n\n#### 扩展代码\n\n前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。\n\n```python\ndef externed_bottom_up_cut_rod(p, n):\n    r = range(n + 1)  # 长度为j的钢条的最大收益值r_j\n    s = range(n + 1)  # 最优解对应的第一条钢条的长度s_j\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            if q < p[i] + r[j - i]:\n                q = p[i] + r[j - i]\n                s[j] = i\n            r[j] = q\n    return r[n]\n\ndef print_cut_rod_solution(p, n):\n    (r, s) = externed_bottom_up_cut_rod(p, n)\n    while n > 0:\n        print(s[n])\n        n = n - s[n]\n```","slug":"动态规划","published":1,"updated":"2018-08-07T00:36:25.226Z","_id":"cjkhjrlg100143bcp4sme643c","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"动态规划（dynamic-programming）\"><a href=\"#动态规划（dynamic-programming）\" class=\"headerlink\" title=\"动态规划（dynamic programming）\"></a>动态规划（dynamic programming）</h2><p>与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。</p>\n<p>通常用来求解最优化问题。</p>\n<p>我们通常按如下4个步骤来设计一个动态规划算法：</p>\n<ul>\n<li>刻画一个最优解的结构特征</li>\n<li>递归地定义最优解的值</li>\n<li>计算最优解的值，通常采用自底向上的方法</li>\n<li>利用计算出的信息构造一个最优解</li>\n</ul>\n<h2 id=\"钢条切割问题\"><a href=\"#钢条切割问题\" class=\"headerlink\" title=\"钢条切割问题\"></a>钢条切割问题</h2><h3 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h3><p>给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, …, n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。</p>\n<h3 id=\"问题分析\"><a href=\"#问题分析\" class=\"headerlink\" title=\"问题分析\"></a>问题分析</h3><p>长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为</p>\n<p>$$n = i_1 + i_2 + … + i_k$$</p>\n<p>得到的最大收益为</p>\n<p>$$r_n = p_{i_1} + p_{i_2} + … + p_{i_k}$$</p>\n<p>当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。</p>\n<p>则最优切割收益为</p>\n<p>$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, …, r_{n-1} + r_1)$$</p>\n<p>除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。</p>\n<p>这样我们得到上述式子的简化版本</p>\n<p>$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$</p>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"自顶向下递归实现\"><a href=\"#自顶向下递归实现\" class=\"headerlink\" title=\"自顶向下递归实现\"></a>自顶向下递归实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    q = <span class=\"number\">-1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        q = max(q, p[i] + cut_rod(p, n-i))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> q</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析\"><a href=\"#代码分析\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p><img src=\"/images/动态规划.jpg\" alt=\"\"></p>\n<p>令$T(n)$表示cut_rod的调用次数</p>\n<p>$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$</p>\n<p>第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$</p>\n<h4 id=\"使用动态规划求解\"><a href=\"#使用动态规划求解\" class=\"headerlink\" title=\"使用动态规划求解\"></a>使用动态规划求解</h4><p>朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。</p>\n<p>动态规划有两种等价的实现。<strong>带备忘的自顶向下</strong>、<strong>自底向上</strong>。这里只给出第二种的代码。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># to save subproblem's result</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            q = max(q, p[i] + r[j - i])</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析-1\"><a href=\"#代码分析-1\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p>自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, …, n$的子问题。时间复杂度为$\\Theta(n^2)$</p>\n<h4 id=\"扩展代码\"><a href=\"#扩展代码\" class=\"headerlink\" title=\"扩展代码\"></a>扩展代码</h4><p>前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">externed_bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 长度为j的钢条的最大收益值r_j</span></span><br><span class=\"line\">    s = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 最优解对应的第一条钢条的长度s_j</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> q &lt; p[i] + r[j - i]:</span><br><span class=\"line\">                q = p[i] + r[j - i]</span><br><span class=\"line\">                s[j] = i</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_cut_rod_solution</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    (r, s) = externed_bottom_up_cut_rod(p, n)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(s[n])</span><br><span class=\"line\">        n = n - s[n]</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"动态规划（dynamic-programming）\"><a href=\"#动态规划（dynamic-programming）\" class=\"headerlink\" title=\"动态规划（dynamic programming）\"></a>动态规划（dynamic programming）</h2><p>与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。</p>\n<p>通常用来求解最优化问题。</p>\n<p>我们通常按如下4个步骤来设计一个动态规划算法：</p>\n<ul>\n<li>刻画一个最优解的结构特征</li>\n<li>递归地定义最优解的值</li>\n<li>计算最优解的值，通常采用自底向上的方法</li>\n<li>利用计算出的信息构造一个最优解</li>\n</ul>\n<h2 id=\"钢条切割问题\"><a href=\"#钢条切割问题\" class=\"headerlink\" title=\"钢条切割问题\"></a>钢条切割问题</h2><h3 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h3><p>给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, …, n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。</p>\n<h3 id=\"问题分析\"><a href=\"#问题分析\" class=\"headerlink\" title=\"问题分析\"></a>问题分析</h3><p>长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为</p>\n<p>$$n = i_1 + i_2 + … + i_k$$</p>\n<p>得到的最大收益为</p>\n<p>$$r_n = p_{i_1} + p_{i_2} + … + p_{i_k}$$</p>\n<p>当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。</p>\n<p>则最优切割收益为</p>\n<p>$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, …, r_{n-1} + r_1)$$</p>\n<p>除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。</p>\n<p>这样我们得到上述式子的简化版本</p>\n<p>$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$</p>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"自顶向下递归实现\"><a href=\"#自顶向下递归实现\" class=\"headerlink\" title=\"自顶向下递归实现\"></a>自顶向下递归实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    q = <span class=\"number\">-1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        q = max(q, p[i] + cut_rod(p, n-i))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> q</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析\"><a href=\"#代码分析\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p><img src=\"/images/动态规划.jpg\" alt=\"\"></p>\n<p>令$T(n)$表示cut_rod的调用次数</p>\n<p>$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$</p>\n<p>第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$</p>\n<h4 id=\"使用动态规划求解\"><a href=\"#使用动态规划求解\" class=\"headerlink\" title=\"使用动态规划求解\"></a>使用动态规划求解</h4><p>朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。</p>\n<p>动态规划有两种等价的实现。<strong>带备忘的自顶向下</strong>、<strong>自底向上</strong>。这里只给出第二种的代码。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># to save subproblem's result</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            q = max(q, p[i] + r[j - i])</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析-1\"><a href=\"#代码分析-1\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p>自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, …, n$的子问题。时间复杂度为$\\Theta(n^2)$</p>\n<h4 id=\"扩展代码\"><a href=\"#扩展代码\" class=\"headerlink\" title=\"扩展代码\"></a>扩展代码</h4><p>前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">externed_bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 长度为j的钢条的最大收益值r_j</span></span><br><span class=\"line\">    s = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 最优解对应的第一条钢条的长度s_j</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> q &lt; p[i] + r[j - i]:</span><br><span class=\"line\">                q = p[i] + r[j - i]</span><br><span class=\"line\">                s[j] = i</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_cut_rod_solution</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    (r, s) = externed_bottom_up_cut_rod(p, n)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(s[n])</span><br><span class=\"line\">        n = n - s[n]</span><br></pre></td></tr></table></figure>"},{"title":"十个策略故事","date":"2018-07-19T16:59:24.000Z","catrgories":"博弈论","_content":"\n### 1. 选数游戏\n\n游戏的参与者：你和一位面试官\n\n游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息\n\n游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。\n\n模拟游戏的程序\n\n```python\nimport random\n\nres = random.randint(1, 100)\n\nfor i in range(5):\n    guess = int(input(\"Epoch {}: \".format(i + 1)))\n    if guess < res:\n        print(\"your guess is lower than the key.\")\n    elif guess > res:\n        print(\"your guess is greater than the key.\")\n    else:\n        print(\"Bingo, you will get {} dollars.\".format(100 - 20 * i))\nprint(\"The key is {}\".format(res))\n```\n\n#### 总结\n\n这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。\n\n","source":"_posts/十个策略故事.md","raw":"---\ntitle: 十个策略故事\ndate: 2018-07-20 00:59:24\ntags: 策略游戏\ncatrgories: 博弈论\n---\n\n### 1. 选数游戏\n\n游戏的参与者：你和一位面试官\n\n游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息\n\n游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。\n\n模拟游戏的程序\n\n```python\nimport random\n\nres = random.randint(1, 100)\n\nfor i in range(5):\n    guess = int(input(\"Epoch {}: \".format(i + 1)))\n    if guess < res:\n        print(\"your guess is lower than the key.\")\n    elif guess > res:\n        print(\"your guess is greater than the key.\")\n    else:\n        print(\"Bingo, you will get {} dollars.\".format(100 - 20 * i))\nprint(\"The key is {}\".format(res))\n```\n\n#### 总结\n\n这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。\n\n","slug":"十个策略故事","published":1,"updated":"2018-08-07T00:36:25.227Z","_id":"cjkhjrlg600163bcpoii0n3n8","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"1-选数游戏\"><a href=\"#1-选数游戏\" class=\"headerlink\" title=\"1. 选数游戏\"></a>1. 选数游戏</h3><p>游戏的参与者：你和一位面试官</p>\n<p>游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息</p>\n<p>游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。</p>\n<p>模拟游戏的程序</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"></span><br><span class=\"line\">res = random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">    guess = int(input(<span class=\"string\">\"Epoch &#123;&#125;: \"</span>.format(i + <span class=\"number\">1</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> guess &lt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is lower than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> guess &gt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is greater than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        print(<span class=\"string\">\"Bingo, you will get &#123;&#125; dollars.\"</span>.format(<span class=\"number\">100</span> - <span class=\"number\">20</span> * i))</span><br><span class=\"line\">print(<span class=\"string\">\"The key is &#123;&#125;\"</span>.format(res))</span><br></pre></td></tr></table></figure>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-选数游戏\"><a href=\"#1-选数游戏\" class=\"headerlink\" title=\"1. 选数游戏\"></a>1. 选数游戏</h3><p>游戏的参与者：你和一位面试官</p>\n<p>游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息</p>\n<p>游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。</p>\n<p>模拟游戏的程序</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"></span><br><span class=\"line\">res = random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">    guess = int(input(<span class=\"string\">\"Epoch &#123;&#125;: \"</span>.format(i + <span class=\"number\">1</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> guess &lt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is lower than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> guess &gt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is greater than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        print(<span class=\"string\">\"Bingo, you will get &#123;&#125; dollars.\"</span>.format(<span class=\"number\">100</span> - <span class=\"number\">20</span> * i))</span><br><span class=\"line\">print(<span class=\"string\">\"The key is &#123;&#125;\"</span>.format(res))</span><br></pre></td></tr></table></figure>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。</p>\n"},{"title":"恋爱领域中普遍存在的贬低倾向","date":"2018-07-19T13:03:37.000Z","_content":"\n## 心理阳痿\n\n### 症状\n\n受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。\n\n### 成因\n\n实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 **情感趋向**和 **肉欲趋向**。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。\n\n### 措施\n\n从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。\n\n----\n摘自弗洛伊德的《爱情心理学》第二章 ","source":"_posts/恋爱领域中普遍存在的贬低倾向.md","raw":"---\ntitle: 恋爱领域中普遍存在的贬低倾向\ndate: 2018-07-19 21:03:37\ntags: 爱情心理学\ncategories: 爱情心理学\n---\n\n## 心理阳痿\n\n### 症状\n\n受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。\n\n### 成因\n\n实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 **情感趋向**和 **肉欲趋向**。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。\n\n### 措施\n\n从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。\n\n----\n摘自弗洛伊德的《爱情心理学》第二章 ","slug":"恋爱领域中普遍存在的贬低倾向","published":1,"updated":"2018-08-07T00:36:25.230Z","_id":"cjkhjrlg900183bcp1jx8siam","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"心理阳痿\"><a href=\"#心理阳痿\" class=\"headerlink\" title=\"心理阳痿\"></a>心理阳痿</h2><h3 id=\"症状\"><a href=\"#症状\" class=\"headerlink\" title=\"症状\"></a>症状</h3><p>受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。</p>\n<h3 id=\"成因\"><a href=\"#成因\" class=\"headerlink\" title=\"成因\"></a>成因</h3><p>实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 <strong>情感趋向</strong>和 <strong>肉欲趋向</strong>。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。</p>\n<h3 id=\"措施\"><a href=\"#措施\" class=\"headerlink\" title=\"措施\"></a>措施</h3><p>从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。</p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第二章 </p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"心理阳痿\"><a href=\"#心理阳痿\" class=\"headerlink\" title=\"心理阳痿\"></a>心理阳痿</h2><h3 id=\"症状\"><a href=\"#症状\" class=\"headerlink\" title=\"症状\"></a>症状</h3><p>受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。</p>\n<h3 id=\"成因\"><a href=\"#成因\" class=\"headerlink\" title=\"成因\"></a>成因</h3><p>实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 <strong>情感趋向</strong>和 <strong>肉欲趋向</strong>。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。</p>\n<h3 id=\"措施\"><a href=\"#措施\" class=\"headerlink\" title=\"措施\"></a>措施</h3><p>从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。</p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第二章 </p>\n"},{"title":"数据划分：训练 / 验证 / 测试集","date":"2018-07-20T06:52:44.000Z","_content":"\n## 数据划分：训练 / 验证 / 测试集\n\n应用深度学习是一个典型的迭代过程。\n\n对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：\n\n* 训练集（train set）：用训练集对算法或模型进行**训练**过程；\n* 验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行**交叉验证**，**选择出最好的模型**；\n* 测试集（test set）：最后利用测试集对模型进行测试，**获取模型运行的无偏估计**（对学习方法进行评估）。\n\n在**小数据量**的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：\n\n* 无验证集的情况：70% / 30%；\n* 有验证集的情况：60% / 20% / 20%；\n\n而在如今的**大数据时代**，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。\n\n验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。\n\n测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。\n\n* 100 万数据量：98% / 1% / 1%；\n* 超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）\n\n### 建议\n\n建议**验证集要和训练集来自于同一个分布**（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。\n\n如果不需要用**无偏估计**来评估模型的性能，则可以不需要测试集。\n\n### 补充：交叉验证（cross validation）\n\n交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。\n\n### 参考资料\n\n[无偏估计_百度百科](https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin)","source":"_posts/数据划分.md","raw":"---\ntitle: 数据划分：训练 / 验证 / 测试集\ndate: 2018-07-20 14:52:44\ntags: 数据划分\ncategories: 深度学习的实用层面\n---\n\n## 数据划分：训练 / 验证 / 测试集\n\n应用深度学习是一个典型的迭代过程。\n\n对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：\n\n* 训练集（train set）：用训练集对算法或模型进行**训练**过程；\n* 验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行**交叉验证**，**选择出最好的模型**；\n* 测试集（test set）：最后利用测试集对模型进行测试，**获取模型运行的无偏估计**（对学习方法进行评估）。\n\n在**小数据量**的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：\n\n* 无验证集的情况：70% / 30%；\n* 有验证集的情况：60% / 20% / 20%；\n\n而在如今的**大数据时代**，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。\n\n验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。\n\n测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。\n\n* 100 万数据量：98% / 1% / 1%；\n* 超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）\n\n### 建议\n\n建议**验证集要和训练集来自于同一个分布**（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。\n\n如果不需要用**无偏估计**来评估模型的性能，则可以不需要测试集。\n\n### 补充：交叉验证（cross validation）\n\n交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。\n\n### 参考资料\n\n[无偏估计_百度百科](https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin)","slug":"数据划分","published":1,"updated":"2018-08-07T00:36:25.231Z","_id":"cjkhjrlga001a3bcp9t71butm","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"数据划分：训练-验证-测试集\"><a href=\"#数据划分：训练-验证-测试集\" class=\"headerlink\" title=\"数据划分：训练 / 验证 / 测试集\"></a>数据划分：训练 / 验证 / 测试集</h2><p>应用深度学习是一个典型的迭代过程。</p>\n<p>对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：</p>\n<ul>\n<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>\n<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>；</li>\n<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>\n</ul>\n<p>在<strong>小数据量</strong>的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>\n<ul>\n<li>无验证集的情况：70% / 30%；</li>\n<li>有验证集的情况：60% / 20% / 20%；</li>\n</ul>\n<p>而在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>\n<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>\n<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>\n<ul>\n<li>100 万数据量：98% / 1% / 1%；</li>\n<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>\n</ul>\n<h3 id=\"建议\"><a href=\"#建议\" class=\"headerlink\" title=\"建议\"></a>建议</h3><p>建议<strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p>\n<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。</p>\n<h3 id=\"补充：交叉验证（cross-validation）\"><a href=\"#补充：交叉验证（cross-validation）\" class=\"headerlink\" title=\"补充：交叉验证（cross validation）\"></a>补充：交叉验证（cross validation）</h3><p>交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><p><a href=\"https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin\" target=\"_blank\" rel=\"noopener\">无偏估计_百度百科</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"数据划分：训练-验证-测试集\"><a href=\"#数据划分：训练-验证-测试集\" class=\"headerlink\" title=\"数据划分：训练 / 验证 / 测试集\"></a>数据划分：训练 / 验证 / 测试集</h2><p>应用深度学习是一个典型的迭代过程。</p>\n<p>对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：</p>\n<ul>\n<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>\n<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>；</li>\n<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>\n</ul>\n<p>在<strong>小数据量</strong>的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>\n<ul>\n<li>无验证集的情况：70% / 30%；</li>\n<li>有验证集的情况：60% / 20% / 20%；</li>\n</ul>\n<p>而在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>\n<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>\n<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>\n<ul>\n<li>100 万数据量：98% / 1% / 1%；</li>\n<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>\n</ul>\n<h3 id=\"建议\"><a href=\"#建议\" class=\"headerlink\" title=\"建议\"></a>建议</h3><p>建议<strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p>\n<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。</p>\n<h3 id=\"补充：交叉验证（cross-validation）\"><a href=\"#补充：交叉验证（cross-validation）\" class=\"headerlink\" title=\"补充：交叉验证（cross validation）\"></a>补充：交叉验证（cross validation）</h3><p>交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><p><a href=\"https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin\" target=\"_blank\" rel=\"noopener\">无偏估计_百度百科</a></p>\n"},{"title":"数据的加载-预处理-可视化","date":"2018-07-21T09:33:49.000Z","_content":"## 图片操作\n\n### 把图片转换为向量\n\n```python\ndef image2vector(image):\n    \"\"\"\n    Argument:\n    image -- a numpy array of shape (length, height, depth)\n    \n    Returns:\n    v -- a vector of shape (length*height*depth, 1)\n    \"\"\"\n    v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2], 1))    \n    return v\n```\n### 读入图片\n\n```python\nmy_image = \"my_image.jpg\" # change this to the name of your image file \nmy_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\n\nplt.imshow(image)\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n```\n\n## 矩阵的正则化\n\n```python\ndef normalize(x):\n    \"\"\"\n    Implement a function that normalizes each row of the matrix x (to have unit length).\n    \n    Argument:\n    x -- A numpy matrix of shape (n, m)\n    \n    Returns:\n    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n    \"\"\"\n    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)  # column: axis=0\n    x = x / x_norm\n    return x\n```\n\n## 猫的数据集\n\n```python\ndef load_dataset():\n    \"\"\"\n    Returns:\n    train_set_x_orig -- shape of (209, 64, 64, 3)\n    train_set_y_orig -- shape of (1, 209)\n    test_set_x_orig -- shape of (50, 64, 64, 3)\n    test_set_y_orig -- shape of (1, 50)\n    \"\"\"\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes\n```\n\n### 可视化\n\n```python\nindex = 23\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\nplt.show()\n```\n\n### 向量化\n\n```python\ntrain_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n```\n\n### 标准化\n\n```python\ntrain_x_set = train_x_set_flatten / 255\ntest_x_set = test_x_set_flatten / 255\n```\n\n## 二维数据的一般操作\n\n### 加载\n\n```python\ndef load_planar_dataset():\n    \"\"\"\n    Returns:\n    X -- a numpy array shaped (2, 400) that contains features (x1, x2)\n    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)\n    \"\"\"\n    np.random.seed(1)\n    m = 400  # number of examples\n    N = int(m / 2)  # number of points per class\n    D = 2  # dimensionality\n    X = np.zeros((m, D))  # data matrix where each row is a single example\n    # labels vector (0 for red, 1 for blue)\n    Y = np.zeros((m, 1), dtype='uint8')\n    a = 4  # maximum ray of the flower\n    for j in range(2):\n        ix = range(N * j, N * (j + 1))\n        t = np.linspace(j * 3.12, (j + 1) * 3.12, N) + \\\n            np.random.randn(N) * 0.2  # theta\n        r = a * np.sin(4 * t) + np.random.randn(N) * 0.2  # radius\n        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        Y[ix] = j\n    X = X.T\n    Y = Y.T\n    return X, Y\n```\n\n### 可视化\n\n```python\nplt.scatter(X[0, :], X[1, :], c=Y.flatten(), s=40, cmp=plt.cm.Spectral)\nplt.show()\n```\n\n### 可视化决策边界\n\n```python\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y.flatten(), cmap=plt.cm.Spectral)\n```\n\n\n## Sklearn中的其他数据集\n\n```python\ndef load_extra_datasets():\n    N = 200\n    noisy_circles = sklearn.datasets.make_circles(\n        n_samples=N, factor=.5, noise=.3)\n    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n    blobs = sklearn.datasets.make_blobs(\n        n_samples=N, random_state=5, n_features=2, centers=6)\n    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(\n        mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n\n    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure\n```","source":"_posts/数据的加载-预处理-可视化.md","raw":"---\ntitle: 数据的加载-预处理-可视化\ndate: 2018-07-21 17:33:49\ntags: 通用数据操作\ncategories: 数据集\n---\n## 图片操作\n\n### 把图片转换为向量\n\n```python\ndef image2vector(image):\n    \"\"\"\n    Argument:\n    image -- a numpy array of shape (length, height, depth)\n    \n    Returns:\n    v -- a vector of shape (length*height*depth, 1)\n    \"\"\"\n    v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2], 1))    \n    return v\n```\n### 读入图片\n\n```python\nmy_image = \"my_image.jpg\" # change this to the name of your image file \nmy_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\n\nplt.imshow(image)\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n```\n\n## 矩阵的正则化\n\n```python\ndef normalize(x):\n    \"\"\"\n    Implement a function that normalizes each row of the matrix x (to have unit length).\n    \n    Argument:\n    x -- A numpy matrix of shape (n, m)\n    \n    Returns:\n    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n    \"\"\"\n    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)  # column: axis=0\n    x = x / x_norm\n    return x\n```\n\n## 猫的数据集\n\n```python\ndef load_dataset():\n    \"\"\"\n    Returns:\n    train_set_x_orig -- shape of (209, 64, 64, 3)\n    train_set_y_orig -- shape of (1, 209)\n    test_set_x_orig -- shape of (50, 64, 64, 3)\n    test_set_y_orig -- shape of (1, 50)\n    \"\"\"\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes\n```\n\n### 可视化\n\n```python\nindex = 23\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\nplt.show()\n```\n\n### 向量化\n\n```python\ntrain_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n```\n\n### 标准化\n\n```python\ntrain_x_set = train_x_set_flatten / 255\ntest_x_set = test_x_set_flatten / 255\n```\n\n## 二维数据的一般操作\n\n### 加载\n\n```python\ndef load_planar_dataset():\n    \"\"\"\n    Returns:\n    X -- a numpy array shaped (2, 400) that contains features (x1, x2)\n    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)\n    \"\"\"\n    np.random.seed(1)\n    m = 400  # number of examples\n    N = int(m / 2)  # number of points per class\n    D = 2  # dimensionality\n    X = np.zeros((m, D))  # data matrix where each row is a single example\n    # labels vector (0 for red, 1 for blue)\n    Y = np.zeros((m, 1), dtype='uint8')\n    a = 4  # maximum ray of the flower\n    for j in range(2):\n        ix = range(N * j, N * (j + 1))\n        t = np.linspace(j * 3.12, (j + 1) * 3.12, N) + \\\n            np.random.randn(N) * 0.2  # theta\n        r = a * np.sin(4 * t) + np.random.randn(N) * 0.2  # radius\n        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        Y[ix] = j\n    X = X.T\n    Y = Y.T\n    return X, Y\n```\n\n### 可视化\n\n```python\nplt.scatter(X[0, :], X[1, :], c=Y.flatten(), s=40, cmp=plt.cm.Spectral)\nplt.show()\n```\n\n### 可视化决策边界\n\n```python\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y.flatten(), cmap=plt.cm.Spectral)\n```\n\n\n## Sklearn中的其他数据集\n\n```python\ndef load_extra_datasets():\n    N = 200\n    noisy_circles = sklearn.datasets.make_circles(\n        n_samples=N, factor=.5, noise=.3)\n    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n    blobs = sklearn.datasets.make_blobs(\n        n_samples=N, random_state=5, n_features=2, centers=6)\n    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(\n        mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n\n    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure\n```","slug":"数据的加载-预处理-可视化","published":1,"updated":"2018-08-07T00:36:25.232Z","_id":"cjkhjrlgb001d3bcpij3w5val","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"图片操作\"><a href=\"#图片操作\" class=\"headerlink\" title=\"图片操作\"></a>图片操作</h2><h3 id=\"把图片转换为向量\"><a href=\"#把图片转换为向量\" class=\"headerlink\" title=\"把图片转换为向量\"></a>把图片转换为向量</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">image2vector</span><span class=\"params\">(image)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    v = image.reshape((image.shape[<span class=\"number\">0</span>] * image.shape[<span class=\"number\">1</span>] * image.shape[<span class=\"number\">2</span>], <span class=\"number\">1</span>))    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<h3 id=\"读入图片\"><a href=\"#读入图片\" class=\"headerlink\" title=\"读入图片\"></a>读入图片</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span> <span class=\"comment\"># change this to the name of your image file </span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">1</span>] <span class=\"comment\"># the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"矩阵的正则化\"><a href=\"#矩阵的正则化\" class=\"headerlink\" title=\"矩阵的正则化\"></a>矩阵的正则化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalize</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    x -- A numpy matrix of shape (n, m)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    x_norm = np.linalg.norm(x, ord=<span class=\"number\">2</span>, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)  <span class=\"comment\"># column: axis=0</span></span><br><span class=\"line\">    x = x / x_norm</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"猫的数据集\"><a href=\"#猫的数据集\" class=\"headerlink\" title=\"猫的数据集\"></a>猫的数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    train_set_x_orig -- shape of (209, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    train_set_y_orig -- shape of (1, 209)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_x_orig -- shape of (50, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_y_orig -- shape of (1, 50)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:]) <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:]) <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:]) <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:]) <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:]) <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化\"><a href=\"#可视化\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index = <span class=\"number\">23</span></span><br><span class=\"line\">plt.imshow(train_set_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_set_y[:, index]) + <span class=\"string\">\", it's a '\"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"' picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\">test_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br></pre></td></tr></table></figure>\n<h3 id=\"标准化\"><a href=\"#标准化\" class=\"headerlink\" title=\"标准化\"></a>标准化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set = train_x_set_flatten / <span class=\"number\">255</span></span><br><span class=\"line\">test_x_set = test_x_set_flatten / <span class=\"number\">255</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"二维数据的一般操作\"><a href=\"#二维数据的一般操作\" class=\"headerlink\" title=\"二维数据的一般操作\"></a>二维数据的一般操作</h2><h3 id=\"加载\"><a href=\"#加载\" class=\"headerlink\" title=\"加载\"></a>加载</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_planar_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    X -- a numpy array shaped (2, 400) that contains features (x1, x2)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">1</span>)</span><br><span class=\"line\">    m = <span class=\"number\">400</span>  <span class=\"comment\"># number of examples</span></span><br><span class=\"line\">    N = int(m / <span class=\"number\">2</span>)  <span class=\"comment\"># number of points per class</span></span><br><span class=\"line\">    D = <span class=\"number\">2</span>  <span class=\"comment\"># dimensionality</span></span><br><span class=\"line\">    X = np.zeros((m, D))  <span class=\"comment\"># data matrix where each row is a single example</span></span><br><span class=\"line\">    <span class=\"comment\"># labels vector (0 for red, 1 for blue)</span></span><br><span class=\"line\">    Y = np.zeros((m, <span class=\"number\">1</span>), dtype=<span class=\"string\">'uint8'</span>)</span><br><span class=\"line\">    a = <span class=\"number\">4</span>  <span class=\"comment\"># maximum ray of the flower</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>):</span><br><span class=\"line\">        ix = range(N * j, N * (j + <span class=\"number\">1</span>))</span><br><span class=\"line\">        t = np.linspace(j * <span class=\"number\">3.12</span>, (j + <span class=\"number\">1</span>) * <span class=\"number\">3.12</span>, N) + \\</span><br><span class=\"line\">            np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># theta</span></span><br><span class=\"line\">        r = a * np.sin(<span class=\"number\">4</span> * t) + np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># radius</span></span><br><span class=\"line\">        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]</span><br><span class=\"line\">        Y[ix] = j</span><br><span class=\"line\">    X = X.T</span><br><span class=\"line\">    Y = Y.T</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X, Y</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化-1\"><a href=\"#可视化-1\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=Y.flatten(), s=<span class=\"number\">40</span>, cmp=plt.cm.Spectral)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化决策边界\"><a href=\"#可视化决策边界\" class=\"headerlink\" title=\"可视化决策边界\"></a>可视化决策边界</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_boundary</span><span class=\"params\">(model, X, y)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Set min and max values and give it some padding</span></span><br><span class=\"line\">    x_min, x_max = X[<span class=\"number\">0</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">0</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    y_min, y_max = X[<span class=\"number\">1</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">1</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    h = <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"comment\"># Generate a grid of points with distance h between them</span></span><br><span class=\"line\">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class=\"line\">                         np.arange(y_min, y_max, h))</span><br><span class=\"line\">    <span class=\"comment\"># Predict the function value for the whole grid</span></span><br><span class=\"line\">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">    Z = Z.reshape(xx.shape)</span><br><span class=\"line\">    <span class=\"comment\"># Plot the contour and training examples</span></span><br><span class=\"line\">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'x2'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'x1'</span>)</span><br><span class=\"line\">    plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=y.flatten(), cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Sklearn中的其他数据集\"><a href=\"#Sklearn中的其他数据集\" class=\"headerlink\" title=\"Sklearn中的其他数据集\"></a>Sklearn中的其他数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_extra_datasets</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    N = <span class=\"number\">200</span></span><br><span class=\"line\">    noisy_circles = sklearn.datasets.make_circles(</span><br><span class=\"line\">        n_samples=N, factor=<span class=\"number\">.5</span>, noise=<span class=\"number\">.3</span>)</span><br><span class=\"line\">    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=<span class=\"number\">.2</span>)</span><br><span class=\"line\">    blobs = sklearn.datasets.make_blobs(</span><br><span class=\"line\">        n_samples=N, random_state=<span class=\"number\">5</span>, n_features=<span class=\"number\">2</span>, centers=<span class=\"number\">6</span>)</span><br><span class=\"line\">    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(</span><br><span class=\"line\">        mean=<span class=\"keyword\">None</span>, cov=<span class=\"number\">0.5</span>, n_samples=N, n_features=<span class=\"number\">2</span>, n_classes=<span class=\"number\">2</span>, shuffle=<span class=\"keyword\">True</span>, random_state=<span class=\"keyword\">None</span>)</span><br><span class=\"line\">    no_structure = np.random.rand(N, <span class=\"number\">2</span>), np.random.rand(N, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"图片操作\"><a href=\"#图片操作\" class=\"headerlink\" title=\"图片操作\"></a>图片操作</h2><h3 id=\"把图片转换为向量\"><a href=\"#把图片转换为向量\" class=\"headerlink\" title=\"把图片转换为向量\"></a>把图片转换为向量</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">image2vector</span><span class=\"params\">(image)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    v = image.reshape((image.shape[<span class=\"number\">0</span>] * image.shape[<span class=\"number\">1</span>] * image.shape[<span class=\"number\">2</span>], <span class=\"number\">1</span>))    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<h3 id=\"读入图片\"><a href=\"#读入图片\" class=\"headerlink\" title=\"读入图片\"></a>读入图片</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span> <span class=\"comment\"># change this to the name of your image file </span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">1</span>] <span class=\"comment\"># the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"矩阵的正则化\"><a href=\"#矩阵的正则化\" class=\"headerlink\" title=\"矩阵的正则化\"></a>矩阵的正则化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalize</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    x -- A numpy matrix of shape (n, m)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    x_norm = np.linalg.norm(x, ord=<span class=\"number\">2</span>, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)  <span class=\"comment\"># column: axis=0</span></span><br><span class=\"line\">    x = x / x_norm</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"猫的数据集\"><a href=\"#猫的数据集\" class=\"headerlink\" title=\"猫的数据集\"></a>猫的数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    train_set_x_orig -- shape of (209, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    train_set_y_orig -- shape of (1, 209)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_x_orig -- shape of (50, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_y_orig -- shape of (1, 50)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:]) <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:]) <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:]) <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:]) <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:]) <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化\"><a href=\"#可视化\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index = <span class=\"number\">23</span></span><br><span class=\"line\">plt.imshow(train_set_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_set_y[:, index]) + <span class=\"string\">\", it's a '\"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"' picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\">test_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br></pre></td></tr></table></figure>\n<h3 id=\"标准化\"><a href=\"#标准化\" class=\"headerlink\" title=\"标准化\"></a>标准化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set = train_x_set_flatten / <span class=\"number\">255</span></span><br><span class=\"line\">test_x_set = test_x_set_flatten / <span class=\"number\">255</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"二维数据的一般操作\"><a href=\"#二维数据的一般操作\" class=\"headerlink\" title=\"二维数据的一般操作\"></a>二维数据的一般操作</h2><h3 id=\"加载\"><a href=\"#加载\" class=\"headerlink\" title=\"加载\"></a>加载</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_planar_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    X -- a numpy array shaped (2, 400) that contains features (x1, x2)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">1</span>)</span><br><span class=\"line\">    m = <span class=\"number\">400</span>  <span class=\"comment\"># number of examples</span></span><br><span class=\"line\">    N = int(m / <span class=\"number\">2</span>)  <span class=\"comment\"># number of points per class</span></span><br><span class=\"line\">    D = <span class=\"number\">2</span>  <span class=\"comment\"># dimensionality</span></span><br><span class=\"line\">    X = np.zeros((m, D))  <span class=\"comment\"># data matrix where each row is a single example</span></span><br><span class=\"line\">    <span class=\"comment\"># labels vector (0 for red, 1 for blue)</span></span><br><span class=\"line\">    Y = np.zeros((m, <span class=\"number\">1</span>), dtype=<span class=\"string\">'uint8'</span>)</span><br><span class=\"line\">    a = <span class=\"number\">4</span>  <span class=\"comment\"># maximum ray of the flower</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>):</span><br><span class=\"line\">        ix = range(N * j, N * (j + <span class=\"number\">1</span>))</span><br><span class=\"line\">        t = np.linspace(j * <span class=\"number\">3.12</span>, (j + <span class=\"number\">1</span>) * <span class=\"number\">3.12</span>, N) + \\</span><br><span class=\"line\">            np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># theta</span></span><br><span class=\"line\">        r = a * np.sin(<span class=\"number\">4</span> * t) + np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># radius</span></span><br><span class=\"line\">        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]</span><br><span class=\"line\">        Y[ix] = j</span><br><span class=\"line\">    X = X.T</span><br><span class=\"line\">    Y = Y.T</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X, Y</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化-1\"><a href=\"#可视化-1\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=Y.flatten(), s=<span class=\"number\">40</span>, cmp=plt.cm.Spectral)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化决策边界\"><a href=\"#可视化决策边界\" class=\"headerlink\" title=\"可视化决策边界\"></a>可视化决策边界</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_boundary</span><span class=\"params\">(model, X, y)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Set min and max values and give it some padding</span></span><br><span class=\"line\">    x_min, x_max = X[<span class=\"number\">0</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">0</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    y_min, y_max = X[<span class=\"number\">1</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">1</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    h = <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"comment\"># Generate a grid of points with distance h between them</span></span><br><span class=\"line\">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class=\"line\">                         np.arange(y_min, y_max, h))</span><br><span class=\"line\">    <span class=\"comment\"># Predict the function value for the whole grid</span></span><br><span class=\"line\">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">    Z = Z.reshape(xx.shape)</span><br><span class=\"line\">    <span class=\"comment\"># Plot the contour and training examples</span></span><br><span class=\"line\">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'x2'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'x1'</span>)</span><br><span class=\"line\">    plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=y.flatten(), cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Sklearn中的其他数据集\"><a href=\"#Sklearn中的其他数据集\" class=\"headerlink\" title=\"Sklearn中的其他数据集\"></a>Sklearn中的其他数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_extra_datasets</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    N = <span class=\"number\">200</span></span><br><span class=\"line\">    noisy_circles = sklearn.datasets.make_circles(</span><br><span class=\"line\">        n_samples=N, factor=<span class=\"number\">.5</span>, noise=<span class=\"number\">.3</span>)</span><br><span class=\"line\">    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=<span class=\"number\">.2</span>)</span><br><span class=\"line\">    blobs = sklearn.datasets.make_blobs(</span><br><span class=\"line\">        n_samples=N, random_state=<span class=\"number\">5</span>, n_features=<span class=\"number\">2</span>, centers=<span class=\"number\">6</span>)</span><br><span class=\"line\">    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(</span><br><span class=\"line\">        mean=<span class=\"keyword\">None</span>, cov=<span class=\"number\">0.5</span>, n_samples=N, n_features=<span class=\"number\">2</span>, n_classes=<span class=\"number\">2</span>, shuffle=<span class=\"keyword\">True</span>, random_state=<span class=\"keyword\">None</span>)</span><br><span class=\"line\">    no_structure = np.random.rand(N, <span class=\"number\">2</span>), np.random.rand(N, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure</span><br></pre></td></tr></table></figure>"},{"title":"标准化输入","date":"2018-07-20T08:27:20.000Z","mathjax":true,"_content":"## 标准化输入\n\n使用标准化处理输入 X 能够有效加速收敛。\n\n### 标准化公式\n\n$$x = \\frac{x - \\mu}{\\sigma}$$\n\n其中，\n\n$$\\mu = \\frac{1}{m}\\sum^m\\_{i=1}x^{(i)}$$\n\n$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m\\_{i=1}{x^{(i)}}^2}$$\n\n（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）\n\n### 使用标准化的原因\n\n![why_normalize](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png)\n\n有图可知，使用标准化前后，成本函数的形状有较大差别。\n\n在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。\n","source":"_posts/标准化输入.md","raw":"---\ntitle: 标准化输入\ndate: 2018-07-20 16:27:20\ntags: 标准化输入\ncategories: 深度学习的实用层面\nmathjax: true\n---\n## 标准化输入\n\n使用标准化处理输入 X 能够有效加速收敛。\n\n### 标准化公式\n\n$$x = \\frac{x - \\mu}{\\sigma}$$\n\n其中，\n\n$$\\mu = \\frac{1}{m}\\sum^m\\_{i=1}x^{(i)}$$\n\n$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m\\_{i=1}{x^{(i)}}^2}$$\n\n（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）\n\n### 使用标准化的原因\n\n![why_normalize](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png)\n\n有图可知，使用标准化前后，成本函数的形状有较大差别。\n\n在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。\n","slug":"标准化输入","published":1,"updated":"2018-08-07T00:36:25.233Z","_id":"cjkhjrlgc001f3bcp8ajkkkch","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"标准化输入\"><a href=\"#标准化输入\" class=\"headerlink\" title=\"标准化输入\"></a>标准化输入</h2><p>使用标准化处理输入 X 能够有效加速收敛。</p>\n<h3 id=\"标准化公式\"><a href=\"#标准化公式\" class=\"headerlink\" title=\"标准化公式\"></a>标准化公式</h3><p>$$x = \\frac{x - \\mu}{\\sigma}$$</p>\n<p>其中，</p>\n<p>$$\\mu = \\frac{1}{m}\\sum^m_{i=1}x^{(i)}$$</p>\n<p>$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m_{i=1}{x^{(i)}}^2}$$</p>\n<p>（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）</p>\n<h3 id=\"使用标准化的原因\"><a href=\"#使用标准化的原因\" class=\"headerlink\" title=\"使用标准化的原因\"></a>使用标准化的原因</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png\" alt=\"why_normalize\"></p>\n<p>有图可知，使用标准化前后，成本函数的形状有较大差别。</p>\n<p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"标准化输入\"><a href=\"#标准化输入\" class=\"headerlink\" title=\"标准化输入\"></a>标准化输入</h2><p>使用标准化处理输入 X 能够有效加速收敛。</p>\n<h3 id=\"标准化公式\"><a href=\"#标准化公式\" class=\"headerlink\" title=\"标准化公式\"></a>标准化公式</h3><p>$$x = \\frac{x - \\mu}{\\sigma}$$</p>\n<p>其中，</p>\n<p>$$\\mu = \\frac{1}{m}\\sum^m_{i=1}x^{(i)}$$</p>\n<p>$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m_{i=1}{x^{(i)}}^2}$$</p>\n<p>（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）</p>\n<h3 id=\"使用标准化的原因\"><a href=\"#使用标准化的原因\" class=\"headerlink\" title=\"使用标准化的原因\"></a>使用标准化的原因</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png\" alt=\"why_normalize\"></p>\n<p>有图可知，使用标准化前后，成本函数的形状有较大差别。</p>\n<p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p>\n"},{"title":"梯度检验","date":"2018-07-20T08:31:29.000Z","mathjax":true,"_content":"## 梯度检验（Gradient checking）\n\n### 梯度的数值逼近\n\n使用双边误差的方法去逼近导数，精度要高于单边误差。\n\n* 单边误差：\n\n![one-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$\n\n误差：$O(\\varepsilon)$\n\n* 双边误差求导（即导数的定义）：\n\n![two-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$\n\n误差：$O(\\varepsilon^2)$\n\n当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。\n\n### 梯度检验的实施\n\n#### 连接参数\n\n将 $W^{[1]}$，$b^{[1]}$，...，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，\n\n$$J(W^{[1]}, b^{[1]}, ..., W^{[L]}，b^{[L]}) = J(\\theta)$$\n\n同时，对 $dW^{[1]}$，$db^{[1]}$，...，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。\n\n![dictionary_to_vector](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png)\n\n现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。\n\n#### 进行梯度检验\n\n求得一个梯度逼近值\n\n$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i+\\varepsilon, ...) - J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i-\\varepsilon, ...)}{2\\varepsilon}$$\n\n应该\n\n$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$\n\n因此，我们用梯度检验值\n\n$$\\frac{||d\\theta\\_{approx} - d\\theta||\\_2}{||d\\theta\\_{approx}||\\_2+||d\\theta||\\_2}$$\n\n检验反向传播的实施是否正确。其中，\n\n$${||x||}\\_2 = \\sum^N\\_{i=1}{|x_i|}^2$$\n\n表示向量 x 的 2-范数（也称“欧几里德范数”）。\n\n如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。\n\n### 在神经网络实施梯度检验的实用技巧和注意事项\n\n1. 不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；\n2. 如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；\n3. 当成本函数包含正则项时，也需要带上正则项进行检验；\n4. 梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；","source":"_posts/梯度检验.md","raw":"---\ntitle: 梯度检验\ndate: 2018-07-20 16:31:29\ntags: 梯度检验\ncategories: 深度学习的实用层面\nmathjax: true\n---\n## 梯度检验（Gradient checking）\n\n### 梯度的数值逼近\n\n使用双边误差的方法去逼近导数，精度要高于单边误差。\n\n* 单边误差：\n\n![one-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$\n\n误差：$O(\\varepsilon)$\n\n* 双边误差求导（即导数的定义）：\n\n![two-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$\n\n误差：$O(\\varepsilon^2)$\n\n当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。\n\n### 梯度检验的实施\n\n#### 连接参数\n\n将 $W^{[1]}$，$b^{[1]}$，...，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，\n\n$$J(W^{[1]}, b^{[1]}, ..., W^{[L]}，b^{[L]}) = J(\\theta)$$\n\n同时，对 $dW^{[1]}$，$db^{[1]}$，...，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。\n\n![dictionary_to_vector](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png)\n\n现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。\n\n#### 进行梯度检验\n\n求得一个梯度逼近值\n\n$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i+\\varepsilon, ...) - J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i-\\varepsilon, ...)}{2\\varepsilon}$$\n\n应该\n\n$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$\n\n因此，我们用梯度检验值\n\n$$\\frac{||d\\theta\\_{approx} - d\\theta||\\_2}{||d\\theta\\_{approx}||\\_2+||d\\theta||\\_2}$$\n\n检验反向传播的实施是否正确。其中，\n\n$${||x||}\\_2 = \\sum^N\\_{i=1}{|x_i|}^2$$\n\n表示向量 x 的 2-范数（也称“欧几里德范数”）。\n\n如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。\n\n### 在神经网络实施梯度检验的实用技巧和注意事项\n\n1. 不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；\n2. 如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；\n3. 当成本函数包含正则项时，也需要带上正则项进行检验；\n4. 梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；","slug":"梯度检验","published":1,"updated":"2018-08-07T00:36:25.234Z","_id":"cjkhjrlge001h3bcpzu3ta8k7","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"梯度检验（Gradient-checking）\"><a href=\"#梯度检验（Gradient-checking）\" class=\"headerlink\" title=\"梯度检验（Gradient checking）\"></a>梯度检验（Gradient checking）</h2><h3 id=\"梯度的数值逼近\"><a href=\"#梯度的数值逼近\" class=\"headerlink\" title=\"梯度的数值逼近\"></a>梯度的数值逼近</h3><p>使用双边误差的方法去逼近导数，精度要高于单边误差。</p>\n<ul>\n<li>单边误差：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png\" alt=\"one-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon)$</p>\n<ul>\n<li>双边误差求导（即导数的定义）：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png\" alt=\"two-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon^2)$</p>\n<p>当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。</p>\n<h3 id=\"梯度检验的实施\"><a href=\"#梯度检验的实施\" class=\"headerlink\" title=\"梯度检验的实施\"></a>梯度检验的实施</h3><h4 id=\"连接参数\"><a href=\"#连接参数\" class=\"headerlink\" title=\"连接参数\"></a>连接参数</h4><p>将 $W^{[1]}$，$b^{[1]}$，…，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，</p>\n<p>$$J(W^{[1]}, b^{[1]}, …, W^{[L]}，b^{[L]}) = J(\\theta)$$</p>\n<p>同时，对 $dW^{[1]}$，$db^{[1]}$，…，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png\" alt=\"dictionary_to_vector\"></p>\n<p>现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。</p>\n<h4 id=\"进行梯度检验\"><a href=\"#进行梯度检验\" class=\"headerlink\" title=\"进行梯度检验\"></a>进行梯度检验</h4><p>求得一个梯度逼近值</p>\n<p>$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta_1, \\theta_2, …, \\theta_i+\\varepsilon, …) - J(\\theta_1, \\theta_2, …, \\theta_i-\\varepsilon, …)}{2\\varepsilon}$$</p>\n<p>应该</p>\n<p>$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$</p>\n<p>因此，我们用梯度检验值</p>\n<p>$$\\frac{||d\\theta_{approx} - d\\theta||_2}{||d\\theta_{approx}||_2+||d\\theta||_2}$$</p>\n<p>检验反向传播的实施是否正确。其中，</p>\n<p>$${||x||}_2 = \\sum^N_{i=1}{|x_i|}^2$$</p>\n<p>表示向量 x 的 2-范数（也称“欧几里德范数”）。</p>\n<p>如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。</p>\n<h3 id=\"在神经网络实施梯度检验的实用技巧和注意事项\"><a href=\"#在神经网络实施梯度检验的实用技巧和注意事项\" class=\"headerlink\" title=\"在神经网络实施梯度检验的实用技巧和注意事项\"></a>在神经网络实施梯度检验的实用技巧和注意事项</h3><ol>\n<li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；</li>\n<li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li>\n<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>\n<li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"梯度检验（Gradient-checking）\"><a href=\"#梯度检验（Gradient-checking）\" class=\"headerlink\" title=\"梯度检验（Gradient checking）\"></a>梯度检验（Gradient checking）</h2><h3 id=\"梯度的数值逼近\"><a href=\"#梯度的数值逼近\" class=\"headerlink\" title=\"梯度的数值逼近\"></a>梯度的数值逼近</h3><p>使用双边误差的方法去逼近导数，精度要高于单边误差。</p>\n<ul>\n<li>单边误差：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png\" alt=\"one-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon)$</p>\n<ul>\n<li>双边误差求导（即导数的定义）：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png\" alt=\"two-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon^2)$</p>\n<p>当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。</p>\n<h3 id=\"梯度检验的实施\"><a href=\"#梯度检验的实施\" class=\"headerlink\" title=\"梯度检验的实施\"></a>梯度检验的实施</h3><h4 id=\"连接参数\"><a href=\"#连接参数\" class=\"headerlink\" title=\"连接参数\"></a>连接参数</h4><p>将 $W^{[1]}$，$b^{[1]}$，…，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，</p>\n<p>$$J(W^{[1]}, b^{[1]}, …, W^{[L]}，b^{[L]}) = J(\\theta)$$</p>\n<p>同时，对 $dW^{[1]}$，$db^{[1]}$，…，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png\" alt=\"dictionary_to_vector\"></p>\n<p>现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。</p>\n<h4 id=\"进行梯度检验\"><a href=\"#进行梯度检验\" class=\"headerlink\" title=\"进行梯度检验\"></a>进行梯度检验</h4><p>求得一个梯度逼近值</p>\n<p>$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta_1, \\theta_2, …, \\theta_i+\\varepsilon, …) - J(\\theta_1, \\theta_2, …, \\theta_i-\\varepsilon, …)}{2\\varepsilon}$$</p>\n<p>应该</p>\n<p>$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$</p>\n<p>因此，我们用梯度检验值</p>\n<p>$$\\frac{||d\\theta_{approx} - d\\theta||_2}{||d\\theta_{approx}||_2+||d\\theta||_2}$$</p>\n<p>检验反向传播的实施是否正确。其中，</p>\n<p>$${||x||}_2 = \\sum^N_{i=1}{|x_i|}^2$$</p>\n<p>表示向量 x 的 2-范数（也称“欧几里德范数”）。</p>\n<p>如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。</p>\n<h3 id=\"在神经网络实施梯度检验的实用技巧和注意事项\"><a href=\"#在神经网络实施梯度检验的实用技巧和注意事项\" class=\"headerlink\" title=\"在神经网络实施梯度检验的实用技巧和注意事项\"></a>在神经网络实施梯度检验的实用技巧和注意事项</h3><ol>\n<li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；</li>\n<li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li>\n<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>\n<li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li>\n</ol>\n"},{"title":"梯度消失和梯度爆炸","date":"2018-07-20T08:25:16.000Z","categroies":"深度学习的实用层面","mathjax":true,"_content":"\n## 梯度消失和梯度爆炸\n\n在梯度函数上出现的以指数级递增或者递减的情况分别称为**梯度爆炸**或者**梯度消失**。\n\n假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：\n\n$$\\hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X$$\n\n* 对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；\n* 对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。\n\n对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。\n\n### 利用初始化缓解梯度消失和爆炸\n\n根据\n\n$$z={w}_1{x}\\_1+{w}\\_2{x}\\_2 + ... + {w}\\_n{x}\\_n + b$$\n\n可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。\n\n为了得到较小的 wi，设置`Var(wi)=1/n`，这里称为 **Xavier initialization**。\n\n```python\nWL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1/n)\n```\n\n其中 n 是输入的神经元个数，即`WL.shape[1]`。\n\n这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。\n\n同理，也有 **He Initialization**。它和  Xavier initialization 唯一的区别是`Var(wi)=2/n`，适用于 **ReLU** 作为激活函数时。\n\n当激活函数使用 ReLU 时，`Var(wi)=2/n`；当激活函数使用 tanh 时，`Var(wi)=1/n`。","source":"_posts/梯度消失和梯度爆炸.md","raw":"---\ntitle: 梯度消失和梯度爆炸\ndate: 2018-07-20 16:25:16\ntags: 梯度消失和梯度爆炸\ncategroies: 深度学习的实用层面\nmathjax: true\n---\n\n## 梯度消失和梯度爆炸\n\n在梯度函数上出现的以指数级递增或者递减的情况分别称为**梯度爆炸**或者**梯度消失**。\n\n假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：\n\n$$\\hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X$$\n\n* 对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；\n* 对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。\n\n对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。\n\n### 利用初始化缓解梯度消失和爆炸\n\n根据\n\n$$z={w}_1{x}\\_1+{w}\\_2{x}\\_2 + ... + {w}\\_n{x}\\_n + b$$\n\n可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。\n\n为了得到较小的 wi，设置`Var(wi)=1/n`，这里称为 **Xavier initialization**。\n\n```python\nWL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1/n)\n```\n\n其中 n 是输入的神经元个数，即`WL.shape[1]`。\n\n这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。\n\n同理，也有 **He Initialization**。它和  Xavier initialization 唯一的区别是`Var(wi)=2/n`，适用于 **ReLU** 作为激活函数时。\n\n当激活函数使用 ReLU 时，`Var(wi)=2/n`；当激活函数使用 tanh 时，`Var(wi)=1/n`。","slug":"梯度消失和梯度爆炸","published":1,"updated":"2018-08-07T00:36:25.234Z","_id":"cjkhjrlgf001l3bcph0lt2itj","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"梯度消失和梯度爆炸\"><a href=\"#梯度消失和梯度爆炸\" class=\"headerlink\" title=\"梯度消失和梯度爆炸\"></a>梯度消失和梯度爆炸</h2><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸</strong>或者<strong>梯度消失</strong>。</p>\n<p>假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：</p>\n<p>$$\\hat{y} = W^{[L]}W^{[L-1]}…W^{[2]}W^{[1]}X$$</p>\n<ul>\n<li>对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；</li>\n<li>对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。</li>\n</ul>\n<p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>\n<h3 id=\"利用初始化缓解梯度消失和爆炸\"><a href=\"#利用初始化缓解梯度消失和爆炸\" class=\"headerlink\" title=\"利用初始化缓解梯度消失和爆炸\"></a>利用初始化缓解梯度消失和爆炸</h3><p>根据</p>\n<p>$$z={w}_1{x}_1+{w}_2{x}_2 + … + {w}_n{x}_n + b$$</p>\n<p>可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。</p>\n<p>为了得到较小的 wi，设置<code>Var(wi)=1/n</code>，这里称为 <strong>Xavier initialization</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WL = np.random.randn(WL.shape[<span class=\"number\">0</span>], WL.shape[<span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">1</span>/n)</span><br></pre></td></tr></table></figure>\n<p>其中 n 是输入的神经元个数，即<code>WL.shape[1]</code>。</p>\n<p>这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p>\n<p>同理，也有 <strong>He Initialization</strong>。它和  Xavier initialization 唯一的区别是<code>Var(wi)=2/n</code>，适用于 <strong>ReLU</strong> 作为激活函数时。</p>\n<p>当激活函数使用 ReLU 时，<code>Var(wi)=2/n</code>；当激活函数使用 tanh 时，<code>Var(wi)=1/n</code>。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"梯度消失和梯度爆炸\"><a href=\"#梯度消失和梯度爆炸\" class=\"headerlink\" title=\"梯度消失和梯度爆炸\"></a>梯度消失和梯度爆炸</h2><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸</strong>或者<strong>梯度消失</strong>。</p>\n<p>假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：</p>\n<p>$$\\hat{y} = W^{[L]}W^{[L-1]}…W^{[2]}W^{[1]}X$$</p>\n<ul>\n<li>对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；</li>\n<li>对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。</li>\n</ul>\n<p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>\n<h3 id=\"利用初始化缓解梯度消失和爆炸\"><a href=\"#利用初始化缓解梯度消失和爆炸\" class=\"headerlink\" title=\"利用初始化缓解梯度消失和爆炸\"></a>利用初始化缓解梯度消失和爆炸</h3><p>根据</p>\n<p>$$z={w}_1{x}_1+{w}_2{x}_2 + … + {w}_n{x}_n + b$$</p>\n<p>可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。</p>\n<p>为了得到较小的 wi，设置<code>Var(wi)=1/n</code>，这里称为 <strong>Xavier initialization</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WL = np.random.randn(WL.shape[<span class=\"number\">0</span>], WL.shape[<span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">1</span>/n)</span><br></pre></td></tr></table></figure>\n<p>其中 n 是输入的神经元个数，即<code>WL.shape[1]</code>。</p>\n<p>这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p>\n<p>同理，也有 <strong>He Initialization</strong>。它和  Xavier initialization 唯一的区别是<code>Var(wi)=2/n</code>，适用于 <strong>ReLU</strong> 作为激活函数时。</p>\n<p>当激活函数使用 ReLU 时，<code>Var(wi)=2/n</code>；当激活函数使用 tanh 时，<code>Var(wi)=1/n</code>。</p>\n"},{"title":"模型估计：偏差 / 方差","date":"2018-07-20T07:56:17.000Z","_content":"## 模型估计：偏差 / 方差\n\n**“偏差-方差分解”（bias-variance decomposition）**是解释学习算法泛化性能的一种重要工具。\n\n泛化误差可分解为偏差、方差与噪声之和：\n\n* **偏差**：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了**学习算法本身的拟合能力**；\n* **方差**：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动所造成的影响**；\n* **噪声**：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了**学习问题本身的难度**。\n\n偏差-方差分解说明，**泛化性能**是由**学习算法的能力**、**数据的充分性**以及**学习任务本身的难度**所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。\n\n<!-- 以上摘自周志华《机器学习》 -->\n\n在**欠拟合（underfitting）**的情况下，出现**高偏差（high bias）**的情况，即不能很好地对数据进行分类。\n\n当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现**过拟合（overfitting）**的情况，在验证集上出现**高方差（high variance）**的现象。\n\n当训练出一个模型以后，如果：\n\n* 训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；\n* 训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；\n* 训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；\n* 训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。\n\n偏差和方差的权衡问题对于模型来说十分重要。\n\n最优误差通常也称为“贝叶斯误差”。\n\n### 应对方法\n\n存在高偏差：\n\n* 扩大网络规模，如添加隐藏层或隐藏单元数目；\n* 寻找合适的网络架构，使用更大的 NN 结构；\n* 花费更长时间训练。\n\n存在高方差：\n\n* 获取更多的数据；\n* 正则化（regularization）；\n* 寻找更合适的网络结构。\n\n不断尝试，直到找到低偏差、低方差的框架。\n\n在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。","source":"_posts/模型估计.md","raw":"---\ntitle: 模型估计：偏差 / 方差\ndate: 2018-07-20 15:56:17\ntags: 模型估计\ncategories: 深度学习的实用层面\n---\n## 模型估计：偏差 / 方差\n\n**“偏差-方差分解”（bias-variance decomposition）**是解释学习算法泛化性能的一种重要工具。\n\n泛化误差可分解为偏差、方差与噪声之和：\n\n* **偏差**：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了**学习算法本身的拟合能力**；\n* **方差**：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动所造成的影响**；\n* **噪声**：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了**学习问题本身的难度**。\n\n偏差-方差分解说明，**泛化性能**是由**学习算法的能力**、**数据的充分性**以及**学习任务本身的难度**所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。\n\n<!-- 以上摘自周志华《机器学习》 -->\n\n在**欠拟合（underfitting）**的情况下，出现**高偏差（high bias）**的情况，即不能很好地对数据进行分类。\n\n当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现**过拟合（overfitting）**的情况，在验证集上出现**高方差（high variance）**的现象。\n\n当训练出一个模型以后，如果：\n\n* 训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；\n* 训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；\n* 训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；\n* 训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。\n\n偏差和方差的权衡问题对于模型来说十分重要。\n\n最优误差通常也称为“贝叶斯误差”。\n\n### 应对方法\n\n存在高偏差：\n\n* 扩大网络规模，如添加隐藏层或隐藏单元数目；\n* 寻找合适的网络架构，使用更大的 NN 结构；\n* 花费更长时间训练。\n\n存在高方差：\n\n* 获取更多的数据；\n* 正则化（regularization）；\n* 寻找更合适的网络结构。\n\n不断尝试，直到找到低偏差、低方差的框架。\n\n在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。","slug":"模型估计","published":1,"updated":"2018-08-07T00:36:25.235Z","_id":"cjkhjrlgh001o3bcp1gcot466","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"模型估计：偏差-方差\"><a href=\"#模型估计：偏差-方差\" class=\"headerlink\" title=\"模型估计：偏差 / 方差\"></a>模型估计：偏差 / 方差</h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>\n<p>泛化误差可分解为偏差、方差与噪声之和：</p>\n<ul>\n<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>\n<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>\n<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>\n</ul>\n<p>偏差-方差分解说明，<strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>以及<strong>学习任务本身的难度</strong>所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。</p>\n<!-- 以上摘自周志华《机器学习》 -->\n<p>在<strong>欠拟合（underfitting）</strong>的情况下，出现<strong>高偏差（high bias）</strong>的情况，即不能很好地对数据进行分类。</p>\n<p>当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现<strong>过拟合（overfitting）</strong>的情况，在验证集上出现<strong>高方差（high variance）</strong>的现象。</p>\n<p>当训练出一个模型以后，如果：</p>\n<ul>\n<li>训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；</li>\n<li>训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；</li>\n<li>训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；</li>\n<li>训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。</li>\n</ul>\n<p>偏差和方差的权衡问题对于模型来说十分重要。</p>\n<p>最优误差通常也称为“贝叶斯误差”。</p>\n<h3 id=\"应对方法\"><a href=\"#应对方法\" class=\"headerlink\" title=\"应对方法\"></a>应对方法</h3><p>存在高偏差：</p>\n<ul>\n<li>扩大网络规模，如添加隐藏层或隐藏单元数目；</li>\n<li>寻找合适的网络架构，使用更大的 NN 结构；</li>\n<li>花费更长时间训练。</li>\n</ul>\n<p>存在高方差：</p>\n<ul>\n<li>获取更多的数据；</li>\n<li>正则化（regularization）；</li>\n<li>寻找更合适的网络结构。</li>\n</ul>\n<p>不断尝试，直到找到低偏差、低方差的框架。</p>\n<p>在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"模型估计：偏差-方差\"><a href=\"#模型估计：偏差-方差\" class=\"headerlink\" title=\"模型估计：偏差 / 方差\"></a>模型估计：偏差 / 方差</h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>\n<p>泛化误差可分解为偏差、方差与噪声之和：</p>\n<ul>\n<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>\n<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>\n<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>\n</ul>\n<p>偏差-方差分解说明，<strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>以及<strong>学习任务本身的难度</strong>所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。</p>\n<!-- 以上摘自周志华《机器学习》 -->\n<p>在<strong>欠拟合（underfitting）</strong>的情况下，出现<strong>高偏差（high bias）</strong>的情况，即不能很好地对数据进行分类。</p>\n<p>当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现<strong>过拟合（overfitting）</strong>的情况，在验证集上出现<strong>高方差（high variance）</strong>的现象。</p>\n<p>当训练出一个模型以后，如果：</p>\n<ul>\n<li>训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；</li>\n<li>训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；</li>\n<li>训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；</li>\n<li>训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。</li>\n</ul>\n<p>偏差和方差的权衡问题对于模型来说十分重要。</p>\n<p>最优误差通常也称为“贝叶斯误差”。</p>\n<h3 id=\"应对方法\"><a href=\"#应对方法\" class=\"headerlink\" title=\"应对方法\"></a>应对方法</h3><p>存在高偏差：</p>\n<ul>\n<li>扩大网络规模，如添加隐藏层或隐藏单元数目；</li>\n<li>寻找合适的网络架构，使用更大的 NN 结构；</li>\n<li>花费更长时间训练。</li>\n</ul>\n<p>存在高方差：</p>\n<ul>\n<li>获取更多的数据；</li>\n<li>正则化（regularization）；</li>\n<li>寻找更合适的网络结构。</li>\n</ul>\n<p>不断尝试，直到找到低偏差、低方差的框架。</p>\n<p>在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。</p>\n"},{"title":"正则化（regularization）","date":"2018-07-20T07:58:06.000Z","mathjax":true,"_content":"## 正则化（regularization）\n\n**正则化**是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。\n\n### Logistic 回归中的正则化\n\n对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：\n\n$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2\\_2$$\n\n* L2 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}^2\\_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}w^2\\_j = \\frac{\\lambda}{2m}w^Tw$$\n\n* L1 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}\\_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}{|w\\_j|}$$\n\n其中，λ 为**正则化因子**，是**超参数**。\n\n由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。\n\n**注意**，`lambda`在 Python 中属于保留字，所以在编程的时候，用`lambd`代替这里的正则化因子。\n\n### 神经网络中的正则化\n\n对于神经网络，加入正则化的成本函数：\n\n$$J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$\n\n因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此\n\n$${||w^{[l]}||}^2\\_F = \\sum^{n^{[l-1]}}\\_{i=1}\\sum^{n^{[l]}}\\_{j=1}(w^{[l]}\\_{ij})^2$$\n\n```python\nL2_regularization_cost = 1./m * lambd/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\ncost = cross_entropy_cost + L2_regularization_cost\n```\n\n该矩阵范数被称为**弗罗贝尼乌斯范数（Frobenius Norm）**，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。\n\n#### 权重衰减（Weight decay）\n\n**在加入正则化项后，梯度变为**（反向传播要按这个计算）：\n\n$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$\n\n```python\ndW = 1./m * np.dot(dZ, A_prev.T) + lambd / m * W\n```\n\n代入梯度更新公式：\n\n$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$\n\n可得：\n\n$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$\n\n$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n其中，因为 $1 - \\frac{\\alpha\\lambda}{m}<1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为**权重衰减（Weight Decay）**。\n\n### 正则化可以减小过拟合的原因\n\n#### 直观解释\n\n正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，**直观上**相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。\n\n#### 数学解释\n\n假设神经元中使用的激活函数为`g(z) = tanh(z)`（sigmoid 同理）。\n\n![regularization_prevent_overfitting](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png)\n\n在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，`tanh(z)`函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。\n\n#### 其他解释\n\n在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。\n","source":"_posts/正则化.md","raw":"---\ntitle: 正则化（regularization）\ndate: 2018-07-20 15:58:06\ntags: 正则化\ncategories: 深度学习的实用层面\nmathjax: true\n---\n## 正则化（regularization）\n\n**正则化**是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。\n\n### Logistic 回归中的正则化\n\n对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：\n\n$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2\\_2$$\n\n* L2 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}^2\\_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}w^2\\_j = \\frac{\\lambda}{2m}w^Tw$$\n\n* L1 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}\\_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}{|w\\_j|}$$\n\n其中，λ 为**正则化因子**，是**超参数**。\n\n由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。\n\n**注意**，`lambda`在 Python 中属于保留字，所以在编程的时候，用`lambd`代替这里的正则化因子。\n\n### 神经网络中的正则化\n\n对于神经网络，加入正则化的成本函数：\n\n$$J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$\n\n因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此\n\n$${||w^{[l]}||}^2\\_F = \\sum^{n^{[l-1]}}\\_{i=1}\\sum^{n^{[l]}}\\_{j=1}(w^{[l]}\\_{ij})^2$$\n\n```python\nL2_regularization_cost = 1./m * lambd/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\ncost = cross_entropy_cost + L2_regularization_cost\n```\n\n该矩阵范数被称为**弗罗贝尼乌斯范数（Frobenius Norm）**，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。\n\n#### 权重衰减（Weight decay）\n\n**在加入正则化项后，梯度变为**（反向传播要按这个计算）：\n\n$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$\n\n```python\ndW = 1./m * np.dot(dZ, A_prev.T) + lambd / m * W\n```\n\n代入梯度更新公式：\n\n$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$\n\n可得：\n\n$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$\n\n$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n其中，因为 $1 - \\frac{\\alpha\\lambda}{m}<1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为**权重衰减（Weight Decay）**。\n\n### 正则化可以减小过拟合的原因\n\n#### 直观解释\n\n正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，**直观上**相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。\n\n#### 数学解释\n\n假设神经元中使用的激活函数为`g(z) = tanh(z)`（sigmoid 同理）。\n\n![regularization_prevent_overfitting](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png)\n\n在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，`tanh(z)`函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。\n\n#### 其他解释\n\n在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。\n","slug":"正则化","published":1,"updated":"2018-08-07T00:36:25.236Z","_id":"cjkhjrlgi001r3bcp9a8lay5y","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"正则化（regularization）\"><a href=\"#正则化（regularization）\" class=\"headerlink\" title=\"正则化（regularization）\"></a>正则化（regularization）</h2><p><strong>正则化</strong>是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。</p>\n<h3 id=\"Logistic-回归中的正则化\"><a href=\"#Logistic-回归中的正则化\" class=\"headerlink\" title=\"Logistic 回归中的正则化\"></a>Logistic 回归中的正则化</h3><p>对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：</p>\n<p>$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2_2$$</p>\n<ul>\n<li>L2 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}^2_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}w^2_j = \\frac{\\lambda}{2m}w^Tw$$</p>\n<ul>\n<li>L1 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}{|w_j|}$$</p>\n<p>其中，λ 为<strong>正则化因子</strong>，是<strong>超参数</strong>。</p>\n<p>由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。</p>\n<p><strong>注意</strong>，<code>lambda</code>在 Python 中属于保留字，所以在编程的时候，用<code>lambd</code>代替这里的正则化因子。</p>\n<h3 id=\"神经网络中的正则化\"><a href=\"#神经网络中的正则化\" class=\"headerlink\" title=\"神经网络中的正则化\"></a>神经网络中的正则化</h3><p>对于神经网络，加入正则化的成本函数：</p>\n<p>$$J(w^{[1]}, b^{[1]}, …, w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$</p>\n<p>因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此</p>\n<p>$${||w^{[l]}||}^2_F = \\sum^{n^{[l-1]}}_{i=1}\\sum^{n^{[l]}}_{j=1}(w^{[l]}_{ij})^2$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">L2_regularization_cost = <span class=\"number\">1.</span>/m * lambd/<span class=\"number\">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))</span><br><span class=\"line\">cost = cross_entropy_cost + L2_regularization_cost</span><br></pre></td></tr></table></figure>\n<p>该矩阵范数被称为<strong>弗罗贝尼乌斯范数（Frobenius Norm）</strong>，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。</p>\n<h4 id=\"权重衰减（Weight-decay）\"><a href=\"#权重衰减（Weight-decay）\" class=\"headerlink\" title=\"权重衰减（Weight decay）\"></a>权重衰减（Weight decay）</h4><p><strong>在加入正则化项后，梯度变为</strong>（反向传播要按这个计算）：</p>\n<p>$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dW = <span class=\"number\">1.</span>/m * np.dot(dZ, A_prev.T) + lambd / m * W</span><br></pre></td></tr></table></figure>\n<p>代入梯度更新公式：</p>\n<p>$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$</p>\n<p>可得：</p>\n<p>$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$</p>\n<p>$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>其中，因为 $1 - \\frac{\\alpha\\lambda}{m}&lt;1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为<strong>权重衰减（Weight Decay）</strong>。</p>\n<h3 id=\"正则化可以减小过拟合的原因\"><a href=\"#正则化可以减小过拟合的原因\" class=\"headerlink\" title=\"正则化可以减小过拟合的原因\"></a>正则化可以减小过拟合的原因</h3><h4 id=\"直观解释\"><a href=\"#直观解释\" class=\"headerlink\" title=\"直观解释\"></a>直观解释</h4><p>正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，<strong>直观上</strong>相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。</p>\n<h4 id=\"数学解释\"><a href=\"#数学解释\" class=\"headerlink\" title=\"数学解释\"></a>数学解释</h4><p>假设神经元中使用的激活函数为<code>g(z) = tanh(z)</code>（sigmoid 同理）。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png\" alt=\"regularization_prevent_overfitting\"></p>\n<p>在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，<code>tanh(z)</code>函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。</p>\n<h4 id=\"其他解释\"><a href=\"#其他解释\" class=\"headerlink\" title=\"其他解释\"></a>其他解释</h4><p>在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"正则化（regularization）\"><a href=\"#正则化（regularization）\" class=\"headerlink\" title=\"正则化（regularization）\"></a>正则化（regularization）</h2><p><strong>正则化</strong>是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。</p>\n<h3 id=\"Logistic-回归中的正则化\"><a href=\"#Logistic-回归中的正则化\" class=\"headerlink\" title=\"Logistic 回归中的正则化\"></a>Logistic 回归中的正则化</h3><p>对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：</p>\n<p>$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2_2$$</p>\n<ul>\n<li>L2 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}^2_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}w^2_j = \\frac{\\lambda}{2m}w^Tw$$</p>\n<ul>\n<li>L1 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}{|w_j|}$$</p>\n<p>其中，λ 为<strong>正则化因子</strong>，是<strong>超参数</strong>。</p>\n<p>由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。</p>\n<p><strong>注意</strong>，<code>lambda</code>在 Python 中属于保留字，所以在编程的时候，用<code>lambd</code>代替这里的正则化因子。</p>\n<h3 id=\"神经网络中的正则化\"><a href=\"#神经网络中的正则化\" class=\"headerlink\" title=\"神经网络中的正则化\"></a>神经网络中的正则化</h3><p>对于神经网络，加入正则化的成本函数：</p>\n<p>$$J(w^{[1]}, b^{[1]}, …, w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$</p>\n<p>因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此</p>\n<p>$${||w^{[l]}||}^2_F = \\sum^{n^{[l-1]}}_{i=1}\\sum^{n^{[l]}}_{j=1}(w^{[l]}_{ij})^2$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">L2_regularization_cost = <span class=\"number\">1.</span>/m * lambd/<span class=\"number\">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))</span><br><span class=\"line\">cost = cross_entropy_cost + L2_regularization_cost</span><br></pre></td></tr></table></figure>\n<p>该矩阵范数被称为<strong>弗罗贝尼乌斯范数（Frobenius Norm）</strong>，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。</p>\n<h4 id=\"权重衰减（Weight-decay）\"><a href=\"#权重衰减（Weight-decay）\" class=\"headerlink\" title=\"权重衰减（Weight decay）\"></a>权重衰减（Weight decay）</h4><p><strong>在加入正则化项后，梯度变为</strong>（反向传播要按这个计算）：</p>\n<p>$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dW = <span class=\"number\">1.</span>/m * np.dot(dZ, A_prev.T) + lambd / m * W</span><br></pre></td></tr></table></figure>\n<p>代入梯度更新公式：</p>\n<p>$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$</p>\n<p>可得：</p>\n<p>$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$</p>\n<p>$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>其中，因为 $1 - \\frac{\\alpha\\lambda}{m}&lt;1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为<strong>权重衰减（Weight Decay）</strong>。</p>\n<h3 id=\"正则化可以减小过拟合的原因\"><a href=\"#正则化可以减小过拟合的原因\" class=\"headerlink\" title=\"正则化可以减小过拟合的原因\"></a>正则化可以减小过拟合的原因</h3><h4 id=\"直观解释\"><a href=\"#直观解释\" class=\"headerlink\" title=\"直观解释\"></a>直观解释</h4><p>正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，<strong>直观上</strong>相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。</p>\n<h4 id=\"数学解释\"><a href=\"#数学解释\" class=\"headerlink\" title=\"数学解释\"></a>数学解释</h4><p>假设神经元中使用的激活函数为<code>g(z) = tanh(z)</code>（sigmoid 同理）。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png\" alt=\"regularization_prevent_overfitting\"></p>\n<p>在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，<code>tanh(z)</code>函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。</p>\n<h4 id=\"其他解释\"><a href=\"#其他解释\" class=\"headerlink\" title=\"其他解释\"></a>其他解释</h4><p>在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。</p>\n"},{"title":"github使用手册","date":"2018-08-05T02:45:20.000Z","_content":"## git clone\n\n### clone地址https和SSH的区别\n\n前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。\n\nhttps url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。\n\n### 在github上添加ssh key的方法\n\n1. \t首先需要检查你电脑是否已经有 SSH key \n\n`cd ~/.ssh/ | ls` 检查是否有文件id_rsa.pub, 若存在则跳过第二步\n\n2. 创建一个ssh key\n\n`ssh-keygen -t rsa -C \"your_email@example.com\"` 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub\n\n3. 添加ssh key到github\n\n拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。\n\n4. 测试ssh key\n\n`ssh -T git@github.com`\n\n### clone指定分支\n\n`git clone -b <分支名> <address.git>`\n\n## 添加新的分支\n\n1. 先将仓库克隆到本地\n2. `git branch`查看分支。`git branch <分支名>` 新建分支\n3. `git checkout <分支名>` 切换到新分支\n4. `git push -u origin <分支名>` 同步分支到github\n","source":"_posts/github使用手册.md","raw":"---\ntitle: github使用手册\ndate: 2018-08-05 10:45:20\ntags: git\ncategories: 程序员实用工具\n---\n## git clone\n\n### clone地址https和SSH的区别\n\n前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。\n\nhttps url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。\n\n### 在github上添加ssh key的方法\n\n1. \t首先需要检查你电脑是否已经有 SSH key \n\n`cd ~/.ssh/ | ls` 检查是否有文件id_rsa.pub, 若存在则跳过第二步\n\n2. 创建一个ssh key\n\n`ssh-keygen -t rsa -C \"your_email@example.com\"` 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub\n\n3. 添加ssh key到github\n\n拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。\n\n4. 测试ssh key\n\n`ssh -T git@github.com`\n\n### clone指定分支\n\n`git clone -b <分支名> <address.git>`\n\n## 添加新的分支\n\n1. 先将仓库克隆到本地\n2. `git branch`查看分支。`git branch <分支名>` 新建分支\n3. `git checkout <分支名>` 切换到新分支\n4. `git push -u origin <分支名>` 同步分支到github\n","slug":"github使用手册","published":1,"updated":"2018-08-07T00:36:25.223Z","_id":"cjkhjrlgj001u3bcpnrecmmr1","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"git-clone\"><a href=\"#git-clone\" class=\"headerlink\" title=\"git clone\"></a>git clone</h2><h3 id=\"clone地址https和SSH的区别\"><a href=\"#clone地址https和SSH的区别\" class=\"headerlink\" title=\"clone地址https和SSH的区别\"></a>clone地址https和SSH的区别</h3><p>前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。</p>\n<p>https url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。</p>\n<h3 id=\"在github上添加ssh-key的方法\"><a href=\"#在github上添加ssh-key的方法\" class=\"headerlink\" title=\"在github上添加ssh key的方法\"></a>在github上添加ssh key的方法</h3><ol>\n<li>首先需要检查你电脑是否已经有 SSH key </li>\n</ol>\n<p><code>cd ~/.ssh/ | ls</code> 检查是否有文件id_rsa.pub, 若存在则跳过第二步</p>\n<ol start=\"2\">\n<li>创建一个ssh key</li>\n</ol>\n<p><code>ssh-keygen -t rsa -C &quot;your_email@example.com&quot;</code> 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub</p>\n<ol start=\"3\">\n<li>添加ssh key到github</li>\n</ol>\n<p>拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。</p>\n<ol start=\"4\">\n<li>测试ssh key</li>\n</ol>\n<p><code>ssh -T git@github.com</code></p>\n<h3 id=\"clone指定分支\"><a href=\"#clone指定分支\" class=\"headerlink\" title=\"clone指定分支\"></a>clone指定分支</h3><p><code>git clone -b &lt;分支名&gt; &lt;address.git&gt;</code></p>\n<h2 id=\"添加新的分支\"><a href=\"#添加新的分支\" class=\"headerlink\" title=\"添加新的分支\"></a>添加新的分支</h2><ol>\n<li>先将仓库克隆到本地</li>\n<li><code>git branch</code>查看分支。<code>git branch &lt;分支名&gt;</code> 新建分支</li>\n<li><code>git checkout &lt;分支名&gt;</code> 切换到新分支</li>\n<li><code>git push -u origin &lt;分支名&gt;</code> 同步分支到github</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"git-clone\"><a href=\"#git-clone\" class=\"headerlink\" title=\"git clone\"></a>git clone</h2><h3 id=\"clone地址https和SSH的区别\"><a href=\"#clone地址https和SSH的区别\" class=\"headerlink\" title=\"clone地址https和SSH的区别\"></a>clone地址https和SSH的区别</h3><p>前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。</p>\n<p>https url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。</p>\n<h3 id=\"在github上添加ssh-key的方法\"><a href=\"#在github上添加ssh-key的方法\" class=\"headerlink\" title=\"在github上添加ssh key的方法\"></a>在github上添加ssh key的方法</h3><ol>\n<li>首先需要检查你电脑是否已经有 SSH key </li>\n</ol>\n<p><code>cd ~/.ssh/ | ls</code> 检查是否有文件id_rsa.pub, 若存在则跳过第二步</p>\n<ol start=\"2\">\n<li>创建一个ssh key</li>\n</ol>\n<p><code>ssh-keygen -t rsa -C &quot;your_email@example.com&quot;</code> 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub</p>\n<ol start=\"3\">\n<li>添加ssh key到github</li>\n</ol>\n<p>拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。</p>\n<ol start=\"4\">\n<li>测试ssh key</li>\n</ol>\n<p><code>ssh -T git@github.com</code></p>\n<h3 id=\"clone指定分支\"><a href=\"#clone指定分支\" class=\"headerlink\" title=\"clone指定分支\"></a>clone指定分支</h3><p><code>git clone -b &lt;分支名&gt; &lt;address.git&gt;</code></p>\n<h2 id=\"添加新的分支\"><a href=\"#添加新的分支\" class=\"headerlink\" title=\"添加新的分支\"></a>添加新的分支</h2><ol>\n<li>先将仓库克隆到本地</li>\n<li><code>git branch</code>查看分支。<code>git branch &lt;分支名&gt;</code> 新建分支</li>\n<li><code>git checkout &lt;分支名&gt;</code> 切换到新分支</li>\n<li><code>git push -u origin &lt;分支名&gt;</code> 同步分支到github</li>\n</ol>\n"},{"title":"男人的对象选择中的一种特殊类型","date":"2018-07-19T10:15:03.000Z","_content":"\n首先我将描述一种对象选择类型--选择的主体是男人--特点是设定一系列的“恋爱的必要条件”。\n\n1. 在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 **“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人--如未婚少女或心无所系的已婚妇女--他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。** 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。\n2. 第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，**名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。** 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。\n\n现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。**它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。**\n\n---\n\n摘自弗洛伊德的《爱情心理学》第一章 \n\n\n","source":"_posts/男人的对象选择中的一种特殊类型.md","raw":"---\ntitle: 男人的对象选择中的一种特殊类型\ndate: 2018-07-19 18:15:03\ntags: 爱情心理学\ncategories: 爱情心理学\n---\n\n首先我将描述一种对象选择类型--选择的主体是男人--特点是设定一系列的“恋爱的必要条件”。\n\n1. 在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 **“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人--如未婚少女或心无所系的已婚妇女--他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。** 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。\n2. 第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，**名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。** 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。\n\n现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。**它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。**\n\n---\n\n摘自弗洛伊德的《爱情心理学》第一章 \n\n\n","slug":"男人的对象选择中的一种特殊类型","published":1,"updated":"2018-08-07T00:36:25.238Z","_id":"cjkhjrlgk001x3bcpen67q7lv","comments":1,"layout":"post","photos":[],"link":"","content":"<p>首先我将描述一种对象选择类型–选择的主体是男人–特点是设定一系列的“恋爱的必要条件”。</p>\n<ol>\n<li>在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 <strong>“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人–如未婚少女或心无所系的已婚妇女–他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。</strong> 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。</li>\n<li>第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，<strong>名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。</strong> 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。</li>\n</ol>\n<p>现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。<strong>它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。</strong></p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第一章 </p>\n","site":{"data":{}},"excerpt":"","more":"<p>首先我将描述一种对象选择类型–选择的主体是男人–特点是设定一系列的“恋爱的必要条件”。</p>\n<ol>\n<li>在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 <strong>“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人–如未婚少女或心无所系的已婚妇女–他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。</strong> 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。</li>\n<li>第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，<strong>名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。</strong> 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。</li>\n</ol>\n<p>现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。<strong>它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。</strong></p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第一章 </p>\n"},{"title":"短诗三首","date":"2018-07-19T02:25:47.000Z","_content":"\n### 确认过眼神\n\n```\n它，睁开朦胧的双眼\n看见天空\n穿着蓝色的裙子\n在微风中跳舞\n红了脸\n害羞地躲到云朵中\n\n它，热烈地燃烧\n想带给天空温暖\n却抵挡不了黑夜的到来\n黑夜一无所有\n却能给天空安慰\n\n它，裹在云朵中哭泣\n眼泪在泥土上溅起水花\n弄脏了天空的裙子\n又停止哭泣\n给天空画上了美丽的彩虹\n```\n\n### 1900\n```\n冬天来了\n你迫不及待地等待夏天\n夏天来了\n你还活在死寂的冬天\n```\n\n### 远方的山\n```\n有些情\n像远方的山\n不知所起，亦不知所终\n隔着呼啸的海浪\n任心被激打\n激打出沉默的浪花\n何不做那海燕\n到山那边歇歇脚\n```","source":"_posts/短诗三首.md","raw":"---\ntitle: 短诗三首\ndate: 2018-07-19 10:25:47\ntags: 现代诗\ncategories: 现代诗\n---\n\n### 确认过眼神\n\n```\n它，睁开朦胧的双眼\n看见天空\n穿着蓝色的裙子\n在微风中跳舞\n红了脸\n害羞地躲到云朵中\n\n它，热烈地燃烧\n想带给天空温暖\n却抵挡不了黑夜的到来\n黑夜一无所有\n却能给天空安慰\n\n它，裹在云朵中哭泣\n眼泪在泥土上溅起水花\n弄脏了天空的裙子\n又停止哭泣\n给天空画上了美丽的彩虹\n```\n\n### 1900\n```\n冬天来了\n你迫不及待地等待夏天\n夏天来了\n你还活在死寂的冬天\n```\n\n### 远方的山\n```\n有些情\n像远方的山\n不知所起，亦不知所终\n隔着呼啸的海浪\n任心被激打\n激打出沉默的浪花\n何不做那海燕\n到山那边歇歇脚\n```","slug":"短诗三首","published":1,"updated":"2018-08-07T00:36:25.238Z","_id":"cjkhjrlgl001z3bcpvcg84t3s","comments":1,"layout":"post","photos":[],"link":"","content":"<h3 id=\"确认过眼神\"><a href=\"#确认过眼神\" class=\"headerlink\" title=\"确认过眼神\"></a>确认过眼神</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">它，睁开朦胧的双眼</span><br><span class=\"line\">看见天空</span><br><span class=\"line\">穿着蓝色的裙子</span><br><span class=\"line\">在微风中跳舞</span><br><span class=\"line\">红了脸</span><br><span class=\"line\">害羞地躲到云朵中</span><br><span class=\"line\"></span><br><span class=\"line\">它，热烈地燃烧</span><br><span class=\"line\">想带给天空温暖</span><br><span class=\"line\">却抵挡不了黑夜的到来</span><br><span class=\"line\">黑夜一无所有</span><br><span class=\"line\">却能给天空安慰</span><br><span class=\"line\"></span><br><span class=\"line\">它，裹在云朵中哭泣</span><br><span class=\"line\">眼泪在泥土上溅起水花</span><br><span class=\"line\">弄脏了天空的裙子</span><br><span class=\"line\">又停止哭泣</span><br><span class=\"line\">给天空画上了美丽的彩虹</span><br></pre></td></tr></table></figure>\n<h3 id=\"1900\"><a href=\"#1900\" class=\"headerlink\" title=\"1900\"></a>1900</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">冬天来了</span><br><span class=\"line\">你迫不及待地等待夏天</span><br><span class=\"line\">夏天来了</span><br><span class=\"line\">你还活在死寂的冬天</span><br></pre></td></tr></table></figure>\n<h3 id=\"远方的山\"><a href=\"#远方的山\" class=\"headerlink\" title=\"远方的山\"></a>远方的山</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">有些情</span><br><span class=\"line\">像远方的山</span><br><span class=\"line\">不知所起，亦不知所终</span><br><span class=\"line\">隔着呼啸的海浪</span><br><span class=\"line\">任心被激打</span><br><span class=\"line\">激打出沉默的浪花</span><br><span class=\"line\">何不做那海燕</span><br><span class=\"line\">到山那边歇歇脚</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"确认过眼神\"><a href=\"#确认过眼神\" class=\"headerlink\" title=\"确认过眼神\"></a>确认过眼神</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">它，睁开朦胧的双眼</span><br><span class=\"line\">看见天空</span><br><span class=\"line\">穿着蓝色的裙子</span><br><span class=\"line\">在微风中跳舞</span><br><span class=\"line\">红了脸</span><br><span class=\"line\">害羞地躲到云朵中</span><br><span class=\"line\"></span><br><span class=\"line\">它，热烈地燃烧</span><br><span class=\"line\">想带给天空温暖</span><br><span class=\"line\">却抵挡不了黑夜的到来</span><br><span class=\"line\">黑夜一无所有</span><br><span class=\"line\">却能给天空安慰</span><br><span class=\"line\"></span><br><span class=\"line\">它，裹在云朵中哭泣</span><br><span class=\"line\">眼泪在泥土上溅起水花</span><br><span class=\"line\">弄脏了天空的裙子</span><br><span class=\"line\">又停止哭泣</span><br><span class=\"line\">给天空画上了美丽的彩虹</span><br></pre></td></tr></table></figure>\n<h3 id=\"1900\"><a href=\"#1900\" class=\"headerlink\" title=\"1900\"></a>1900</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">冬天来了</span><br><span class=\"line\">你迫不及待地等待夏天</span><br><span class=\"line\">夏天来了</span><br><span class=\"line\">你还活在死寂的冬天</span><br></pre></td></tr></table></figure>\n<h3 id=\"远方的山\"><a href=\"#远方的山\" class=\"headerlink\" title=\"远方的山\"></a>远方的山</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">有些情</span><br><span class=\"line\">像远方的山</span><br><span class=\"line\">不知所起，亦不知所终</span><br><span class=\"line\">隔着呼啸的海浪</span><br><span class=\"line\">任心被激打</span><br><span class=\"line\">激打出沉默的浪花</span><br><span class=\"line\">何不做那海燕</span><br><span class=\"line\">到山那边歇歇脚</span><br></pre></td></tr></table></figure>"},{"title":"布雷默曼极限","date":"2018-08-05T13:38:25.000Z","mathjax":true,"_content":"\n**以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度**由爱因斯坦的质能效应和海森堡测不准原理得到。\n\n$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$\n\n在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。\n\n\n","source":"_posts/布雷默曼极限.md","raw":"---\ntitle: 布雷默曼极限\ndate: 2018-08-05 21:38:25\ntags: 计算限制\ncategories: 计算机科学\nmathjax: true\n---\n\n**以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度**由爱因斯坦的质能效应和海森堡测不准原理得到。\n\n$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$\n\n在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。\n\n\n","slug":"布雷默曼极限","published":1,"updated":"2018-08-07T00:36:25.228Z","_id":"cjkhjrlgm00233bcpao1as9lb","comments":1,"layout":"post","photos":[],"link":"","content":"<p><strong>以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度</strong>由爱因斯坦的质能效应和海森堡测不准原理得到。</p>\n<p>$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$</p>\n<p>在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度</strong>由爱因斯坦的质能效应和海森堡测不准原理得到。</p>\n<p>$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$</p>\n<p>在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。</p>\n"},{"title":"贞洁禁忌","date":"2018-07-21T07:31:50.000Z","_content":"## 现象\n\n**处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。**史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。\n\n## 解释\n\n### 对血的恐惧\n\n贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。\n\n### 对新事物的恐惧\n\n正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。\n\n### 贞洁禁忌是整个性生活的一部分\n\n不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。\n\n只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果--卸载紧张、引发身体疲软--或许是男人惧怕女人的原型。\n\n## 结论\n\n在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。\n\n十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。","source":"_posts/贞洁禁忌.md","raw":"---\ntitle: 贞洁禁忌\ndate: 2018-07-21 15:31:50\ntags: 爱情心理学\ncategories: 爱情心理学\n---\n## 现象\n\n**处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。**史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。\n\n## 解释\n\n### 对血的恐惧\n\n贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。\n\n### 对新事物的恐惧\n\n正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。\n\n### 贞洁禁忌是整个性生活的一部分\n\n不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。\n\n只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果--卸载紧张、引发身体疲软--或许是男人惧怕女人的原型。\n\n## 结论\n\n在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。\n\n十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。","slug":"贞洁禁忌","published":1,"updated":"2018-08-07T00:36:25.240Z","_id":"cjkhjrlgo00273bcpg5xblft8","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"现象\"><a href=\"#现象\" class=\"headerlink\" title=\"现象\"></a>现象</h2><p><strong>处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。</strong>史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。</p>\n<h2 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h2><h3 id=\"对血的恐惧\"><a href=\"#对血的恐惧\" class=\"headerlink\" title=\"对血的恐惧\"></a>对血的恐惧</h3><p>贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。</p>\n<h3 id=\"对新事物的恐惧\"><a href=\"#对新事物的恐惧\" class=\"headerlink\" title=\"对新事物的恐惧\"></a>对新事物的恐惧</h3><p>正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。</p>\n<h3 id=\"贞洁禁忌是整个性生活的一部分\"><a href=\"#贞洁禁忌是整个性生活的一部分\" class=\"headerlink\" title=\"贞洁禁忌是整个性生活的一部分\"></a>贞洁禁忌是整个性生活的一部分</h3><p>不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。</p>\n<p>只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果–卸载紧张、引发身体疲软–或许是男人惧怕女人的原型。</p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。</p>\n<p>十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"现象\"><a href=\"#现象\" class=\"headerlink\" title=\"现象\"></a>现象</h2><p><strong>处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。</strong>史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。</p>\n<h2 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h2><h3 id=\"对血的恐惧\"><a href=\"#对血的恐惧\" class=\"headerlink\" title=\"对血的恐惧\"></a>对血的恐惧</h3><p>贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。</p>\n<h3 id=\"对新事物的恐惧\"><a href=\"#对新事物的恐惧\" class=\"headerlink\" title=\"对新事物的恐惧\"></a>对新事物的恐惧</h3><p>正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。</p>\n<h3 id=\"贞洁禁忌是整个性生活的一部分\"><a href=\"#贞洁禁忌是整个性生活的一部分\" class=\"headerlink\" title=\"贞洁禁忌是整个性生活的一部分\"></a>贞洁禁忌是整个性生活的一部分</h3><p>不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。</p>\n<p>只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果–卸载紧张、引发身体疲软–或许是男人惧怕女人的原型。</p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。</p>\n<p>十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。</p>\n"},{"title":"深度学习中的优化算法","date":"2018-08-04T05:26:01.000Z","mathjax":true,"_content":"深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。\n\n## batch 梯度下降法\n\n**batch 梯度下降法**（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。\n\n对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。\n\n但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 **mini-batch**。\n\n## Mini-Batch 梯度下降法\n\n**Mini-Batch 梯度下降法**（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。\n\n使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。\n\nbatch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：\n\n![training-with-mini-batch-gradient-descent](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png)\n\n### batch 的不同大小（size）带来的影响\n\n* mini-batch 的大小为 1，即是**随机梯度下降法（stochastic gradient descent）**，每个样本都是独立的 mini-batch；\n* mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；\n\n![choosing-mini-batch-size](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png)\n\n* batch 梯度下降法：\n    * 对所有 m 个训练样本执行一次梯度下降，**每一次迭代时间较长，训练过程慢**； \n    * 相对噪声低一些，幅度也大一些；\n    * 成本函数总是向减小的方向下降。\n\n* 随机梯度下降法：\n    * 对每一个训练样本执行一次梯度下降，训练速度快，但**丢失了向量化带来的计算加速**；\n    * 有很多噪声，减小学习率可以适当；\n    * 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。\n\n因此，选择一个`1 < size < m`的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。\n\n### mini-batch 大小的选择\n\n* 如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；\n* 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、...、$2^9$；\n* mini-batch 的大小要符合 CPU/GPU 内存。\n\nmini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。\n\n### 获得 mini-batch 的步骤\n\n1. 将数据集打乱；\n2. 按照既定的大小分割数据集；\n\n其中打乱数据集的代码：\n\n```py\nm = X.shape[1] \npermutation = list(np.random.permutation(m))\nshuffled_X = X[:, permutation]\nshuffled_Y = Y[:, permutation].reshape((1,m))\n```\n\n`np.random.permutation`与`np.random.shuffle`有两处不同：\n\n1. 如果传给`permutation`一个矩阵，它会返回一个洗牌后的矩阵副本；而`shuffle`只是对一个矩阵进行洗牌，没有返回值。\n2. 如果传入一个整数，它会返回一个洗牌后的`arange`。\n\n### 符号表示\n\n* 使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；\n* 使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；\n* 现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。\n\n## 指数平均加权\n\n**指数加权平均（Exponentially Weight Average）**是一种常用的序列数据处理方式，计算公式为：\n\n$$\nS\\_t = \n\\begin{cases} \nY\\_1, &t = 1 \\\\\\\\ \n\\beta S\\_{t-1} + (1-\\beta)Y_t, &t > 1 \n\\end{cases}\n$$\n\n其中 $Y\\_t$ 为 t 下的实际值，$S\\_t$ 为 t 下加权平均后的值，β 为权重值。\n\n指数加权平均数在统计学中被称为“指数加权移动平均值”。\n\n![Exponentially-weight-average](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png)\n\n给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。\n\n当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。**β 越大相当于求取平均利用的天数越多**，曲线自然就会越平滑而且越滞后。\n\n### 理解指数平均加权\n\n当 β 为 0.9 时，\n\n$$v\\_{100} = 0.9v\\_{99} + 0.1 \\theta\\_{100}$$\n\n$$v\\_{99} = 0.9v\\_{98} + 0.1 \\theta\\_{99}$$\n\n$$v\\_{98} = 0.9v\\_{97} + 0.1 \\theta\\_{98}$$\n$$...$$\n\n展开：\n\n$$v\\_{100} = 0.1 \\theta\\_{100} + 0.1 \\* 0.9 \\theta\\_{99} + 0.1 \\* {(0.9)}^2 \\theta\\_{98} + ...$$\n\n其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作**偏差修正（Bias Correction）**。\n\n根据函数极限的一条定理：\n\n$$\\lim\\_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$\n\n当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。\n\n因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。\n\n$$v\\_t = \\beta v\\_{t-1} + (1 - \\beta)\\theta_t$$\n\n考虑到代码，只需要不断更新 v 即可：\n\n$$v := \\beta v + (1 - \\beta)\\theta_t$$\n<!--此处应有公式的实现代码-->\n\n指数平均加权并**不是最精准**的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。\n\n指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此**效率极高，且节省成本**。\n\n### 指数平均加权的偏差修正\n\n我们通常有\n\n$$v\\_0 = 0$$\n$$v\\_1 = 0.98v\\_0 + 0.02\\theta\\_1$$\n\n因此，$v\\_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。\n\n因此，我们修改公式为\n\n$$v\\_t = \\frac{\\beta v\\_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$\n\n随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。\n\n## 动量梯度下降法\n\n**动量梯度下降（Gradient Descent with Momentum）**是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：\n\nfor l = 1, .. , L：\n\n$$v\\_{dW^{[l]}} = \\beta v\\_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$\n$$v\\_{db^{[l]}} = \\beta v\\_{db^{[l]}} + (1 - \\beta) db^{[l]}$$\n$$W^{[l]} := W^{[l]} - \\alpha v\\_{dW^{[l]}}$$\n$$b^{[l]} := b^{[l]} - \\alpha v\\_{db^{[l]}}$$\n\n其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。\n\n![Gradient-Descent-with-Momentum](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png)\n\n进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。\n\n而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。\n\n当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。\n\n另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。\n\n### 动量梯度下降法的形象解释\n\n将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v\\_{dw}$、$v\\_{db}$ 相当于速度。\n\n小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。\n\n## RMSProp 算法\n\n**RMSProp（Root Mean Square Prop，均方根支）**算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：\n\n$$s\\_{dw} = \\beta s\\_{dw} + (1 - \\beta)(dw)^2$$\n$$s\\_{db} = \\beta s\\_{db} + (1 - \\beta)(db)^2$$\n$$w := w - \\alpha \\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n$$b := b - \\alpha \\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。\n\n当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s\\_{dw}$、$s\\_{db}$也会较大，最终使得\n\n$$\\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n\n和\n\n$$\\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。\n\n![RMSProp](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png)\n\nRMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。\n\n注意，β 也是一个超参数。\n\n## Adam 优化算法\n\n**Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）**基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：\n\n首先进行初始化：\n\n$$v\\_{dW} = 0, s\\_{dW} = 0, v\\_{db} = 0, s\\_{db} = 0$$\n\n用每一个 mini-batch 计算 dW、db，第 t 次迭代时：\n\n$$v\\_{dW} = \\beta\\_1 v\\_{dW} + (1 - \\beta\\_1) dW$$\n$$v\\_{db} = \\beta\\_1 v\\_{db} + (1 - \\beta\\_1) db$$\n$$s\\_{dW} = \\beta\\_2 s\\_{dW} + (1 - \\beta\\_2) (dW)^2$$\n$$s\\_{db} = \\beta\\_2 s\\_{db} + (1 - \\beta\\_2) (db)^2$$\n\n一般使用 Adam 算法时需要计算偏差修正：\n\n$$v^{corrected}\\_{dW} = \\frac{v\\_{dW}}{1-{\\beta\\_1}^t}$$\n$$v^{corrected}\\_{db} = \\frac{v\\_{db}}{1-{\\beta\\_1}^t}$$\n$$s^{corrected}\\_{dW} = \\frac{s\\_{dW}}{1-{\\beta\\_2}^t}$$\n$$s^{corrected}\\_{db} = \\frac{s\\_{db}}{1-{\\beta\\_2}^t}$$\n\n所以，更新 W、b 时有：\n\n$$W := W - \\alpha \\frac{v^{corrected}\\_{dW}}{\\sqrt{s^{corrected}\\_{dW} + \\epsilon}}$$\n\n$$b := b - \\alpha \\frac{v^{corrected}\\_{db}}{\\sqrt{s^{corrected}\\_{db}} + \\epsilon}$$\n\n（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）\n\n### 超参数的选择\n\nAdam 优化算法有很多的超参数，其中\n\n* 学习率 α：需要尝试一系列的值，来寻找比较合适的；\n* β1：常用的缺省值为 0.9；\n* β2：Adam 算法的作者建议为 0.999；\n* ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；\n\nβ1、β2、ϵ 通常不需要调试。\n\n## 学习率衰减\n\n如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。\n\n而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。\n\n最常用的学习率衰减方法：\n\n$$\\alpha = \\frac{1}{1 + decay\\\\\\_rate \\* epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n其中，`decay_rate`为衰减率（超参数），`epoch_num`为将所有的训练样本完整过一遍的次数。\n\n* 指数衰减：\n\n$$\\alpha = 0.95^{epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n* 其他：\n\n$$\\alpha = \\frac{k}{\\sqrt{epoch\\\\\\_num}} \\* \\alpha\\_0$$\n\n* 离散下降\n\n对于较小的模型，也有人会在训练时根据进度手动调小学习率。\n\n## 局部最优问题\n\n![saddle](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png)\n\n**鞍点（saddle）**是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。\n\n结论：\n\n* 在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；\n* 鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。\n","source":"_posts/深度学习中的优化算法.md","raw":"---\ntitle: 深度学习中的优化算法\ndate: 2018-08-04 13:26:01\ntags: 优化算法\ncategories: 深度学习\nmathjax: true\n---\n深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。\n\n## batch 梯度下降法\n\n**batch 梯度下降法**（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。\n\n对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。\n\n但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 **mini-batch**。\n\n## Mini-Batch 梯度下降法\n\n**Mini-Batch 梯度下降法**（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。\n\n使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。\n\nbatch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：\n\n![training-with-mini-batch-gradient-descent](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png)\n\n### batch 的不同大小（size）带来的影响\n\n* mini-batch 的大小为 1，即是**随机梯度下降法（stochastic gradient descent）**，每个样本都是独立的 mini-batch；\n* mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；\n\n![choosing-mini-batch-size](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png)\n\n* batch 梯度下降法：\n    * 对所有 m 个训练样本执行一次梯度下降，**每一次迭代时间较长，训练过程慢**； \n    * 相对噪声低一些，幅度也大一些；\n    * 成本函数总是向减小的方向下降。\n\n* 随机梯度下降法：\n    * 对每一个训练样本执行一次梯度下降，训练速度快，但**丢失了向量化带来的计算加速**；\n    * 有很多噪声，减小学习率可以适当；\n    * 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。\n\n因此，选择一个`1 < size < m`的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。\n\n### mini-batch 大小的选择\n\n* 如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；\n* 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、...、$2^9$；\n* mini-batch 的大小要符合 CPU/GPU 内存。\n\nmini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。\n\n### 获得 mini-batch 的步骤\n\n1. 将数据集打乱；\n2. 按照既定的大小分割数据集；\n\n其中打乱数据集的代码：\n\n```py\nm = X.shape[1] \npermutation = list(np.random.permutation(m))\nshuffled_X = X[:, permutation]\nshuffled_Y = Y[:, permutation].reshape((1,m))\n```\n\n`np.random.permutation`与`np.random.shuffle`有两处不同：\n\n1. 如果传给`permutation`一个矩阵，它会返回一个洗牌后的矩阵副本；而`shuffle`只是对一个矩阵进行洗牌，没有返回值。\n2. 如果传入一个整数，它会返回一个洗牌后的`arange`。\n\n### 符号表示\n\n* 使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；\n* 使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；\n* 现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。\n\n## 指数平均加权\n\n**指数加权平均（Exponentially Weight Average）**是一种常用的序列数据处理方式，计算公式为：\n\n$$\nS\\_t = \n\\begin{cases} \nY\\_1, &t = 1 \\\\\\\\ \n\\beta S\\_{t-1} + (1-\\beta)Y_t, &t > 1 \n\\end{cases}\n$$\n\n其中 $Y\\_t$ 为 t 下的实际值，$S\\_t$ 为 t 下加权平均后的值，β 为权重值。\n\n指数加权平均数在统计学中被称为“指数加权移动平均值”。\n\n![Exponentially-weight-average](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png)\n\n给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。\n\n当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。**β 越大相当于求取平均利用的天数越多**，曲线自然就会越平滑而且越滞后。\n\n### 理解指数平均加权\n\n当 β 为 0.9 时，\n\n$$v\\_{100} = 0.9v\\_{99} + 0.1 \\theta\\_{100}$$\n\n$$v\\_{99} = 0.9v\\_{98} + 0.1 \\theta\\_{99}$$\n\n$$v\\_{98} = 0.9v\\_{97} + 0.1 \\theta\\_{98}$$\n$$...$$\n\n展开：\n\n$$v\\_{100} = 0.1 \\theta\\_{100} + 0.1 \\* 0.9 \\theta\\_{99} + 0.1 \\* {(0.9)}^2 \\theta\\_{98} + ...$$\n\n其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作**偏差修正（Bias Correction）**。\n\n根据函数极限的一条定理：\n\n$$\\lim\\_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$\n\n当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。\n\n因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。\n\n$$v\\_t = \\beta v\\_{t-1} + (1 - \\beta)\\theta_t$$\n\n考虑到代码，只需要不断更新 v 即可：\n\n$$v := \\beta v + (1 - \\beta)\\theta_t$$\n<!--此处应有公式的实现代码-->\n\n指数平均加权并**不是最精准**的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。\n\n指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此**效率极高，且节省成本**。\n\n### 指数平均加权的偏差修正\n\n我们通常有\n\n$$v\\_0 = 0$$\n$$v\\_1 = 0.98v\\_0 + 0.02\\theta\\_1$$\n\n因此，$v\\_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。\n\n因此，我们修改公式为\n\n$$v\\_t = \\frac{\\beta v\\_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$\n\n随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。\n\n## 动量梯度下降法\n\n**动量梯度下降（Gradient Descent with Momentum）**是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：\n\nfor l = 1, .. , L：\n\n$$v\\_{dW^{[l]}} = \\beta v\\_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$\n$$v\\_{db^{[l]}} = \\beta v\\_{db^{[l]}} + (1 - \\beta) db^{[l]}$$\n$$W^{[l]} := W^{[l]} - \\alpha v\\_{dW^{[l]}}$$\n$$b^{[l]} := b^{[l]} - \\alpha v\\_{db^{[l]}}$$\n\n其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。\n\n![Gradient-Descent-with-Momentum](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png)\n\n进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。\n\n而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。\n\n当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。\n\n另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。\n\n### 动量梯度下降法的形象解释\n\n将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v\\_{dw}$、$v\\_{db}$ 相当于速度。\n\n小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。\n\n## RMSProp 算法\n\n**RMSProp（Root Mean Square Prop，均方根支）**算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：\n\n$$s\\_{dw} = \\beta s\\_{dw} + (1 - \\beta)(dw)^2$$\n$$s\\_{db} = \\beta s\\_{db} + (1 - \\beta)(db)^2$$\n$$w := w - \\alpha \\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n$$b := b - \\alpha \\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。\n\n当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s\\_{dw}$、$s\\_{db}$也会较大，最终使得\n\n$$\\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n\n和\n\n$$\\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。\n\n![RMSProp](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png)\n\nRMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。\n\n注意，β 也是一个超参数。\n\n## Adam 优化算法\n\n**Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）**基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：\n\n首先进行初始化：\n\n$$v\\_{dW} = 0, s\\_{dW} = 0, v\\_{db} = 0, s\\_{db} = 0$$\n\n用每一个 mini-batch 计算 dW、db，第 t 次迭代时：\n\n$$v\\_{dW} = \\beta\\_1 v\\_{dW} + (1 - \\beta\\_1) dW$$\n$$v\\_{db} = \\beta\\_1 v\\_{db} + (1 - \\beta\\_1) db$$\n$$s\\_{dW} = \\beta\\_2 s\\_{dW} + (1 - \\beta\\_2) (dW)^2$$\n$$s\\_{db} = \\beta\\_2 s\\_{db} + (1 - \\beta\\_2) (db)^2$$\n\n一般使用 Adam 算法时需要计算偏差修正：\n\n$$v^{corrected}\\_{dW} = \\frac{v\\_{dW}}{1-{\\beta\\_1}^t}$$\n$$v^{corrected}\\_{db} = \\frac{v\\_{db}}{1-{\\beta\\_1}^t}$$\n$$s^{corrected}\\_{dW} = \\frac{s\\_{dW}}{1-{\\beta\\_2}^t}$$\n$$s^{corrected}\\_{db} = \\frac{s\\_{db}}{1-{\\beta\\_2}^t}$$\n\n所以，更新 W、b 时有：\n\n$$W := W - \\alpha \\frac{v^{corrected}\\_{dW}}{\\sqrt{s^{corrected}\\_{dW} + \\epsilon}}$$\n\n$$b := b - \\alpha \\frac{v^{corrected}\\_{db}}{\\sqrt{s^{corrected}\\_{db}} + \\epsilon}$$\n\n（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）\n\n### 超参数的选择\n\nAdam 优化算法有很多的超参数，其中\n\n* 学习率 α：需要尝试一系列的值，来寻找比较合适的；\n* β1：常用的缺省值为 0.9；\n* β2：Adam 算法的作者建议为 0.999；\n* ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；\n\nβ1、β2、ϵ 通常不需要调试。\n\n## 学习率衰减\n\n如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。\n\n而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。\n\n最常用的学习率衰减方法：\n\n$$\\alpha = \\frac{1}{1 + decay\\\\\\_rate \\* epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n其中，`decay_rate`为衰减率（超参数），`epoch_num`为将所有的训练样本完整过一遍的次数。\n\n* 指数衰减：\n\n$$\\alpha = 0.95^{epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n* 其他：\n\n$$\\alpha = \\frac{k}{\\sqrt{epoch\\\\\\_num}} \\* \\alpha\\_0$$\n\n* 离散下降\n\n对于较小的模型，也有人会在训练时根据进度手动调小学习率。\n\n## 局部最优问题\n\n![saddle](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png)\n\n**鞍点（saddle）**是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。\n\n结论：\n\n* 在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；\n* 鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。\n","slug":"深度学习中的优化算法","published":1,"updated":"2018-08-07T00:36:25.237Z","_id":"cjkhjrlgr002a3bcp3p0310pj","comments":1,"layout":"post","photos":[],"link":"","content":"<p>深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。</p>\n<h2 id=\"batch-梯度下降法\"><a href=\"#batch-梯度下降法\" class=\"headerlink\" title=\"batch 梯度下降法\"></a>batch 梯度下降法</h2><p><strong>batch 梯度下降法</strong>（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。</p>\n<p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。</p>\n<p>但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 <strong>mini-batch</strong>。</p>\n<h2 id=\"Mini-Batch-梯度下降法\"><a href=\"#Mini-Batch-梯度下降法\" class=\"headerlink\" title=\"Mini-Batch 梯度下降法\"></a>Mini-Batch 梯度下降法</h2><p><strong>Mini-Batch 梯度下降法</strong>（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。</p>\n<p>使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。</p>\n<p>batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png\" alt=\"training-with-mini-batch-gradient-descent\"></p>\n<h3 id=\"batch-的不同大小（size）带来的影响\"><a href=\"#batch-的不同大小（size）带来的影响\" class=\"headerlink\" title=\"batch 的不同大小（size）带来的影响\"></a>batch 的不同大小（size）带来的影响</h3><ul>\n<li>mini-batch 的大小为 1，即是<strong>随机梯度下降法（stochastic gradient descent）</strong>，每个样本都是独立的 mini-batch；</li>\n<li>mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png\" alt=\"choosing-mini-batch-size\"></p>\n<ul>\n<li><p>batch 梯度下降法：</p>\n<ul>\n<li>对所有 m 个训练样本执行一次梯度下降，<strong>每一次迭代时间较长，训练过程慢</strong>； </li>\n<li>相对噪声低一些，幅度也大一些；</li>\n<li>成本函数总是向减小的方向下降。</li>\n</ul>\n</li>\n<li><p>随机梯度下降法：</p>\n<ul>\n<li>对每一个训练样本执行一次梯度下降，训练速度快，但<strong>丢失了向量化带来的计算加速</strong>；</li>\n<li>有很多噪声，减小学习率可以适当；</li>\n<li>成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。</li>\n</ul>\n</li>\n</ul>\n<p>因此，选择一个<code>1 &lt; size &lt; m</code>的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。</p>\n<h3 id=\"mini-batch-大小的选择\"><a href=\"#mini-batch-大小的选择\" class=\"headerlink\" title=\"mini-batch 大小的选择\"></a>mini-batch 大小的选择</h3><ul>\n<li>如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；</li>\n<li>如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、…、$2^9$；</li>\n<li>mini-batch 的大小要符合 CPU/GPU 内存。</li>\n</ul>\n<p>mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。</p>\n<h3 id=\"获得-mini-batch-的步骤\"><a href=\"#获得-mini-batch-的步骤\" class=\"headerlink\" title=\"获得 mini-batch 的步骤\"></a>获得 mini-batch 的步骤</h3><ol>\n<li>将数据集打乱；</li>\n<li>按照既定的大小分割数据集；</li>\n</ol>\n<p>其中打乱数据集的代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m = X.shape[<span class=\"number\">1</span>] </span><br><span class=\"line\">permutation = list(np.random.permutation(m))</span><br><span class=\"line\">shuffled_X = X[:, permutation]</span><br><span class=\"line\">shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br></pre></td></tr></table></figure>\n<p><code>np.random.permutation</code>与<code>np.random.shuffle</code>有两处不同：</p>\n<ol>\n<li>如果传给<code>permutation</code>一个矩阵，它会返回一个洗牌后的矩阵副本；而<code>shuffle</code>只是对一个矩阵进行洗牌，没有返回值。</li>\n<li>如果传入一个整数，它会返回一个洗牌后的<code>arange</code>。</li>\n</ol>\n<h3 id=\"符号表示\"><a href=\"#符号表示\" class=\"headerlink\" title=\"符号表示\"></a>符号表示</h3><ul>\n<li>使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；</li>\n<li>使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；</li>\n<li>现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。</li>\n</ul>\n<h2 id=\"指数平均加权\"><a href=\"#指数平均加权\" class=\"headerlink\" title=\"指数平均加权\"></a>指数平均加权</h2><p><strong>指数加权平均（Exponentially Weight Average）</strong>是一种常用的序列数据处理方式，计算公式为：</p>\n<p>$$<br>S_t =<br>\\begin{cases}<br>Y_1, &amp;t = 1 \\\\<br>\\beta S_{t-1} + (1-\\beta)Y_t, &amp;t &gt; 1<br>\\end{cases}<br>$$</p>\n<p>其中 $Y_t$ 为 t 下的实际值，$S_t$ 为 t 下加权平均后的值，β 为权重值。</p>\n<p>指数加权平均数在统计学中被称为“指数加权移动平均值”。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png\" alt=\"Exponentially-weight-average\"></p>\n<p>给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。</p>\n<p>当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。<strong>β 越大相当于求取平均利用的天数越多</strong>，曲线自然就会越平滑而且越滞后。</p>\n<h3 id=\"理解指数平均加权\"><a href=\"#理解指数平均加权\" class=\"headerlink\" title=\"理解指数平均加权\"></a>理解指数平均加权</h3><p>当 β 为 0.9 时，</p>\n<p>$$v_{100} = 0.9v_{99} + 0.1 \\theta_{100}$$</p>\n<p>$$v_{99} = 0.9v_{98} + 0.1 \\theta_{99}$$</p>\n<p>$$v_{98} = 0.9v_{97} + 0.1 \\theta_{98}$$<br>$$…$$</p>\n<p>展开：</p>\n<p>$$v_{100} = 0.1 \\theta_{100} + 0.1 * 0.9 \\theta_{99} + 0.1 * {(0.9)}^2 \\theta_{98} + …$$</p>\n<p>其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作<strong>偏差修正（Bias Correction）</strong>。</p>\n<p>根据函数极限的一条定理：</p>\n<p>$$\\lim_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$</p>\n<p>当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。</p>\n<p>因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。</p>\n<p>$$v_t = \\beta v_{t-1} + (1 - \\beta)\\theta_t$$</p>\n<p>考虑到代码，只需要不断更新 v 即可：</p>\n<p>$$v := \\beta v + (1 - \\beta)\\theta_t$$<br><!--此处应有公式的实现代码--></p>\n<p>指数平均加权并<strong>不是最精准</strong>的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。</p>\n<p>指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此<strong>效率极高，且节省成本</strong>。</p>\n<h3 id=\"指数平均加权的偏差修正\"><a href=\"#指数平均加权的偏差修正\" class=\"headerlink\" title=\"指数平均加权的偏差修正\"></a>指数平均加权的偏差修正</h3><p>我们通常有</p>\n<p>$$v_0 = 0$$<br>$$v_1 = 0.98v_0 + 0.02\\theta_1$$</p>\n<p>因此，$v_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。</p>\n<p>因此，我们修改公式为</p>\n<p>$$v_t = \\frac{\\beta v_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$</p>\n<p>随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。</p>\n<h2 id=\"动量梯度下降法\"><a href=\"#动量梯度下降法\" class=\"headerlink\" title=\"动量梯度下降法\"></a>动量梯度下降法</h2><p><strong>动量梯度下降（Gradient Descent with Momentum）</strong>是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：</p>\n<p>for l = 1, .. , L：</p>\n<p>$$v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$<br>$$v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]}$$<br>$$W^{[l]} := W^{[l]} - \\alpha v_{dW^{[l]}}$$<br>$$b^{[l]} := b^{[l]} - \\alpha v_{db^{[l]}}$$</p>\n<p>其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png\" alt=\"Gradient-Descent-with-Momentum\"></p>\n<p>进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。</p>\n<p>而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。</p>\n<p>当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。</p>\n<p>另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。</p>\n<h3 id=\"动量梯度下降法的形象解释\"><a href=\"#动量梯度下降法的形象解释\" class=\"headerlink\" title=\"动量梯度下降法的形象解释\"></a>动量梯度下降法的形象解释</h3><p>将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v_{dw}$、$v_{db}$ 相当于速度。</p>\n<p>小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。</p>\n<h2 id=\"RMSProp-算法\"><a href=\"#RMSProp-算法\" class=\"headerlink\" title=\"RMSProp 算法\"></a>RMSProp 算法</h2><p><strong>RMSProp（Root Mean Square Prop，均方根支）</strong>算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：</p>\n<p>$$s_{dw} = \\beta s_{dw} + (1 - \\beta)(dw)^2$$<br>$$s_{db} = \\beta s_{db} + (1 - \\beta)(db)^2$$<br>$$w := w - \\alpha \\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$<br>$$b := b - \\alpha \\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。</p>\n<p>当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s_{dw}$、$s_{db}$也会较大，最终使得</p>\n<p>$$\\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$</p>\n<p>和</p>\n<p>$$\\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png\" alt=\"RMSProp\"></p>\n<p>RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。</p>\n<p>注意，β 也是一个超参数。</p>\n<h2 id=\"Adam-优化算法\"><a href=\"#Adam-优化算法\" class=\"headerlink\" title=\"Adam 优化算法\"></a>Adam 优化算法</h2><p><strong>Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）</strong>基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：</p>\n<p>首先进行初始化：</p>\n<p>$$v_{dW} = 0, s_{dW} = 0, v_{db} = 0, s_{db} = 0$$</p>\n<p>用每一个 mini-batch 计算 dW、db，第 t 次迭代时：</p>\n<p>$$v_{dW} = \\beta_1 v_{dW} + (1 - \\beta_1) dW$$<br>$$v_{db} = \\beta_1 v_{db} + (1 - \\beta_1) db$$<br>$$s_{dW} = \\beta_2 s_{dW} + (1 - \\beta_2) (dW)^2$$<br>$$s_{db} = \\beta_2 s_{db} + (1 - \\beta_2) (db)^2$$</p>\n<p>一般使用 Adam 算法时需要计算偏差修正：</p>\n<p>$$v^{corrected}_{dW} = \\frac{v_{dW}}{1-{\\beta_1}^t}$$<br>$$v^{corrected}_{db} = \\frac{v_{db}}{1-{\\beta_1}^t}$$<br>$$s^{corrected}_{dW} = \\frac{s_{dW}}{1-{\\beta_2}^t}$$<br>$$s^{corrected}_{db} = \\frac{s_{db}}{1-{\\beta_2}^t}$$</p>\n<p>所以，更新 W、b 时有：</p>\n<p>$$W := W - \\alpha \\frac{v^{corrected}_{dW}}{\\sqrt{s^{corrected}_{dW} + \\epsilon}}$$</p>\n<p>$$b := b - \\alpha \\frac{v^{corrected}_{db}}{\\sqrt{s^{corrected}_{db}} + \\epsilon}$$</p>\n<p>（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）</p>\n<h3 id=\"超参数的选择\"><a href=\"#超参数的选择\" class=\"headerlink\" title=\"超参数的选择\"></a>超参数的选择</h3><p>Adam 优化算法有很多的超参数，其中</p>\n<ul>\n<li>学习率 α：需要尝试一系列的值，来寻找比较合适的；</li>\n<li>β1：常用的缺省值为 0.9；</li>\n<li>β2：Adam 算法的作者建议为 0.999；</li>\n<li>ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；</li>\n</ul>\n<p>β1、β2、ϵ 通常不需要调试。</p>\n<h2 id=\"学习率衰减\"><a href=\"#学习率衰减\" class=\"headerlink\" title=\"学习率衰减\"></a>学习率衰减</h2><p>如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。</p>\n<p>而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。</p>\n<p>最常用的学习率衰减方法：</p>\n<p>$$\\alpha = \\frac{1}{1 + decay\\_rate * epoch\\_num} * \\alpha_0$$</p>\n<p>其中，<code>decay_rate</code>为衰减率（超参数），<code>epoch_num</code>为将所有的训练样本完整过一遍的次数。</p>\n<ul>\n<li>指数衰减：</li>\n</ul>\n<p>$$\\alpha = 0.95^{epoch\\_num} * \\alpha_0$$</p>\n<ul>\n<li>其他：</li>\n</ul>\n<p>$$\\alpha = \\frac{k}{\\sqrt{epoch\\_num}} * \\alpha_0$$</p>\n<ul>\n<li>离散下降</li>\n</ul>\n<p>对于较小的模型，也有人会在训练时根据进度手动调小学习率。</p>\n<h2 id=\"局部最优问题\"><a href=\"#局部最优问题\" class=\"headerlink\" title=\"局部最优问题\"></a>局部最优问题</h2><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png\" alt=\"saddle\"></p>\n<p><strong>鞍点（saddle）</strong>是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。</p>\n<p>结论：</p>\n<ul>\n<li>在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；</li>\n<li>鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。</p>\n<h2 id=\"batch-梯度下降法\"><a href=\"#batch-梯度下降法\" class=\"headerlink\" title=\"batch 梯度下降法\"></a>batch 梯度下降法</h2><p><strong>batch 梯度下降法</strong>（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。</p>\n<p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。</p>\n<p>但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 <strong>mini-batch</strong>。</p>\n<h2 id=\"Mini-Batch-梯度下降法\"><a href=\"#Mini-Batch-梯度下降法\" class=\"headerlink\" title=\"Mini-Batch 梯度下降法\"></a>Mini-Batch 梯度下降法</h2><p><strong>Mini-Batch 梯度下降法</strong>（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。</p>\n<p>使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。</p>\n<p>batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png\" alt=\"training-with-mini-batch-gradient-descent\"></p>\n<h3 id=\"batch-的不同大小（size）带来的影响\"><a href=\"#batch-的不同大小（size）带来的影响\" class=\"headerlink\" title=\"batch 的不同大小（size）带来的影响\"></a>batch 的不同大小（size）带来的影响</h3><ul>\n<li>mini-batch 的大小为 1，即是<strong>随机梯度下降法（stochastic gradient descent）</strong>，每个样本都是独立的 mini-batch；</li>\n<li>mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png\" alt=\"choosing-mini-batch-size\"></p>\n<ul>\n<li><p>batch 梯度下降法：</p>\n<ul>\n<li>对所有 m 个训练样本执行一次梯度下降，<strong>每一次迭代时间较长，训练过程慢</strong>； </li>\n<li>相对噪声低一些，幅度也大一些；</li>\n<li>成本函数总是向减小的方向下降。</li>\n</ul>\n</li>\n<li><p>随机梯度下降法：</p>\n<ul>\n<li>对每一个训练样本执行一次梯度下降，训练速度快，但<strong>丢失了向量化带来的计算加速</strong>；</li>\n<li>有很多噪声，减小学习率可以适当；</li>\n<li>成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。</li>\n</ul>\n</li>\n</ul>\n<p>因此，选择一个<code>1 &lt; size &lt; m</code>的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。</p>\n<h3 id=\"mini-batch-大小的选择\"><a href=\"#mini-batch-大小的选择\" class=\"headerlink\" title=\"mini-batch 大小的选择\"></a>mini-batch 大小的选择</h3><ul>\n<li>如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；</li>\n<li>如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、…、$2^9$；</li>\n<li>mini-batch 的大小要符合 CPU/GPU 内存。</li>\n</ul>\n<p>mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。</p>\n<h3 id=\"获得-mini-batch-的步骤\"><a href=\"#获得-mini-batch-的步骤\" class=\"headerlink\" title=\"获得 mini-batch 的步骤\"></a>获得 mini-batch 的步骤</h3><ol>\n<li>将数据集打乱；</li>\n<li>按照既定的大小分割数据集；</li>\n</ol>\n<p>其中打乱数据集的代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m = X.shape[<span class=\"number\">1</span>] </span><br><span class=\"line\">permutation = list(np.random.permutation(m))</span><br><span class=\"line\">shuffled_X = X[:, permutation]</span><br><span class=\"line\">shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br></pre></td></tr></table></figure>\n<p><code>np.random.permutation</code>与<code>np.random.shuffle</code>有两处不同：</p>\n<ol>\n<li>如果传给<code>permutation</code>一个矩阵，它会返回一个洗牌后的矩阵副本；而<code>shuffle</code>只是对一个矩阵进行洗牌，没有返回值。</li>\n<li>如果传入一个整数，它会返回一个洗牌后的<code>arange</code>。</li>\n</ol>\n<h3 id=\"符号表示\"><a href=\"#符号表示\" class=\"headerlink\" title=\"符号表示\"></a>符号表示</h3><ul>\n<li>使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；</li>\n<li>使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；</li>\n<li>现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。</li>\n</ul>\n<h2 id=\"指数平均加权\"><a href=\"#指数平均加权\" class=\"headerlink\" title=\"指数平均加权\"></a>指数平均加权</h2><p><strong>指数加权平均（Exponentially Weight Average）</strong>是一种常用的序列数据处理方式，计算公式为：</p>\n<p>$$<br>S_t =<br>\\begin{cases}<br>Y_1, &amp;t = 1 \\\\<br>\\beta S_{t-1} + (1-\\beta)Y_t, &amp;t &gt; 1<br>\\end{cases}<br>$$</p>\n<p>其中 $Y_t$ 为 t 下的实际值，$S_t$ 为 t 下加权平均后的值，β 为权重值。</p>\n<p>指数加权平均数在统计学中被称为“指数加权移动平均值”。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png\" alt=\"Exponentially-weight-average\"></p>\n<p>给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。</p>\n<p>当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。<strong>β 越大相当于求取平均利用的天数越多</strong>，曲线自然就会越平滑而且越滞后。</p>\n<h3 id=\"理解指数平均加权\"><a href=\"#理解指数平均加权\" class=\"headerlink\" title=\"理解指数平均加权\"></a>理解指数平均加权</h3><p>当 β 为 0.9 时，</p>\n<p>$$v_{100} = 0.9v_{99} + 0.1 \\theta_{100}$$</p>\n<p>$$v_{99} = 0.9v_{98} + 0.1 \\theta_{99}$$</p>\n<p>$$v_{98} = 0.9v_{97} + 0.1 \\theta_{98}$$<br>$$…$$</p>\n<p>展开：</p>\n<p>$$v_{100} = 0.1 \\theta_{100} + 0.1 * 0.9 \\theta_{99} + 0.1 * {(0.9)}^2 \\theta_{98} + …$$</p>\n<p>其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作<strong>偏差修正（Bias Correction）</strong>。</p>\n<p>根据函数极限的一条定理：</p>\n<p>$$\\lim_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$</p>\n<p>当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。</p>\n<p>因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。</p>\n<p>$$v_t = \\beta v_{t-1} + (1 - \\beta)\\theta_t$$</p>\n<p>考虑到代码，只需要不断更新 v 即可：</p>\n<p>$$v := \\beta v + (1 - \\beta)\\theta_t$$<br><!--此处应有公式的实现代码--></p>\n<p>指数平均加权并<strong>不是最精准</strong>的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。</p>\n<p>指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此<strong>效率极高，且节省成本</strong>。</p>\n<h3 id=\"指数平均加权的偏差修正\"><a href=\"#指数平均加权的偏差修正\" class=\"headerlink\" title=\"指数平均加权的偏差修正\"></a>指数平均加权的偏差修正</h3><p>我们通常有</p>\n<p>$$v_0 = 0$$<br>$$v_1 = 0.98v_0 + 0.02\\theta_1$$</p>\n<p>因此，$v_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。</p>\n<p>因此，我们修改公式为</p>\n<p>$$v_t = \\frac{\\beta v_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$</p>\n<p>随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。</p>\n<h2 id=\"动量梯度下降法\"><a href=\"#动量梯度下降法\" class=\"headerlink\" title=\"动量梯度下降法\"></a>动量梯度下降法</h2><p><strong>动量梯度下降（Gradient Descent with Momentum）</strong>是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：</p>\n<p>for l = 1, .. , L：</p>\n<p>$$v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$<br>$$v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]}$$<br>$$W^{[l]} := W^{[l]} - \\alpha v_{dW^{[l]}}$$<br>$$b^{[l]} := b^{[l]} - \\alpha v_{db^{[l]}}$$</p>\n<p>其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png\" alt=\"Gradient-Descent-with-Momentum\"></p>\n<p>进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。</p>\n<p>而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。</p>\n<p>当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。</p>\n<p>另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。</p>\n<h3 id=\"动量梯度下降法的形象解释\"><a href=\"#动量梯度下降法的形象解释\" class=\"headerlink\" title=\"动量梯度下降法的形象解释\"></a>动量梯度下降法的形象解释</h3><p>将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v_{dw}$、$v_{db}$ 相当于速度。</p>\n<p>小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。</p>\n<h2 id=\"RMSProp-算法\"><a href=\"#RMSProp-算法\" class=\"headerlink\" title=\"RMSProp 算法\"></a>RMSProp 算法</h2><p><strong>RMSProp（Root Mean Square Prop，均方根支）</strong>算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：</p>\n<p>$$s_{dw} = \\beta s_{dw} + (1 - \\beta)(dw)^2$$<br>$$s_{db} = \\beta s_{db} + (1 - \\beta)(db)^2$$<br>$$w := w - \\alpha \\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$<br>$$b := b - \\alpha \\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。</p>\n<p>当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s_{dw}$、$s_{db}$也会较大，最终使得</p>\n<p>$$\\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$</p>\n<p>和</p>\n<p>$$\\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png\" alt=\"RMSProp\"></p>\n<p>RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。</p>\n<p>注意，β 也是一个超参数。</p>\n<h2 id=\"Adam-优化算法\"><a href=\"#Adam-优化算法\" class=\"headerlink\" title=\"Adam 优化算法\"></a>Adam 优化算法</h2><p><strong>Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）</strong>基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：</p>\n<p>首先进行初始化：</p>\n<p>$$v_{dW} = 0, s_{dW} = 0, v_{db} = 0, s_{db} = 0$$</p>\n<p>用每一个 mini-batch 计算 dW、db，第 t 次迭代时：</p>\n<p>$$v_{dW} = \\beta_1 v_{dW} + (1 - \\beta_1) dW$$<br>$$v_{db} = \\beta_1 v_{db} + (1 - \\beta_1) db$$<br>$$s_{dW} = \\beta_2 s_{dW} + (1 - \\beta_2) (dW)^2$$<br>$$s_{db} = \\beta_2 s_{db} + (1 - \\beta_2) (db)^2$$</p>\n<p>一般使用 Adam 算法时需要计算偏差修正：</p>\n<p>$$v^{corrected}_{dW} = \\frac{v_{dW}}{1-{\\beta_1}^t}$$<br>$$v^{corrected}_{db} = \\frac{v_{db}}{1-{\\beta_1}^t}$$<br>$$s^{corrected}_{dW} = \\frac{s_{dW}}{1-{\\beta_2}^t}$$<br>$$s^{corrected}_{db} = \\frac{s_{db}}{1-{\\beta_2}^t}$$</p>\n<p>所以，更新 W、b 时有：</p>\n<p>$$W := W - \\alpha \\frac{v^{corrected}_{dW}}{\\sqrt{s^{corrected}_{dW} + \\epsilon}}$$</p>\n<p>$$b := b - \\alpha \\frac{v^{corrected}_{db}}{\\sqrt{s^{corrected}_{db}} + \\epsilon}$$</p>\n<p>（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）</p>\n<h3 id=\"超参数的选择\"><a href=\"#超参数的选择\" class=\"headerlink\" title=\"超参数的选择\"></a>超参数的选择</h3><p>Adam 优化算法有很多的超参数，其中</p>\n<ul>\n<li>学习率 α：需要尝试一系列的值，来寻找比较合适的；</li>\n<li>β1：常用的缺省值为 0.9；</li>\n<li>β2：Adam 算法的作者建议为 0.999；</li>\n<li>ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；</li>\n</ul>\n<p>β1、β2、ϵ 通常不需要调试。</p>\n<h2 id=\"学习率衰减\"><a href=\"#学习率衰减\" class=\"headerlink\" title=\"学习率衰减\"></a>学习率衰减</h2><p>如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。</p>\n<p>而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。</p>\n<p>最常用的学习率衰减方法：</p>\n<p>$$\\alpha = \\frac{1}{1 + decay\\_rate * epoch\\_num} * \\alpha_0$$</p>\n<p>其中，<code>decay_rate</code>为衰减率（超参数），<code>epoch_num</code>为将所有的训练样本完整过一遍的次数。</p>\n<ul>\n<li>指数衰减：</li>\n</ul>\n<p>$$\\alpha = 0.95^{epoch\\_num} * \\alpha_0$$</p>\n<ul>\n<li>其他：</li>\n</ul>\n<p>$$\\alpha = \\frac{k}{\\sqrt{epoch\\_num}} * \\alpha_0$$</p>\n<ul>\n<li>离散下降</li>\n</ul>\n<p>对于较小的模型，也有人会在训练时根据进度手动调小学习率。</p>\n<h2 id=\"局部最优问题\"><a href=\"#局部最优问题\" class=\"headerlink\" title=\"局部最优问题\"></a>局部最优问题</h2><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png\" alt=\"saddle\"></p>\n<p><strong>鞍点（saddle）</strong>是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。</p>\n<p>结论：</p>\n<ul>\n<li>在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；</li>\n<li>鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。</li>\n</ul>\n"},{"title":"神经网络中的通用函数代码","date":"2018-07-21T08:58:55.000Z","_content":"\n## 激活函数\n\n### Sigmoid\n\n```python\ndef sigmoid(Z):\n    \"\"\"\n    Implements the sigmoid activation in numpy\n\n    Arguments:\n    Z -- numpy array of any shape\n\n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    A = 1 / (1 + np.exp(-Z))\n    cache = Z\n    return A, cache\n\ndef sigmoid_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single SIGMOID unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    s = 1 / (1 + np.exp(-Z))\n    dZ = dA * s * (1 - s)\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n### Relu\n\n```python\ndef relu(Z):\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    A = np.maximum(0, Z)\n    assert(A.shape == Z.shape)\n    cache = Z\n    return A, cache\n\ndef relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n    # When z <= 0, you should set dz to 0 as well.\n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n## 代价函数\n\n### 交叉熵\n\n```python\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    m = Y.shape[1]\n    # Compute loss from aL and y.\n    cost = -1 / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    return cost\n```\n\n### L1 and L2 Loss\n\n```python\ndef L1(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L1 loss function defined above\n    \"\"\"\n    loss = np.sum(abs(yhat - y))\n    return loss\n\ndef L2(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L2 loss function defined above\n    \"\"\"\n    loss = np.sum((y - yhat) ** 2)\n    return loss\n```\n\n## 线性函数\n\n```python\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter\n    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    Z = np.dot(W, A) + b\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    return Z, cache\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = 1 / m * np.dot(dZ, A_prev.T)\n    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    return dA_prev, dW, db\n```\n\n## 线性到激活层\n\n```python\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value\n    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n    return A, cache\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n\n    Arguments:\n    dA -- post-activation gradient for current layer l\n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    return dA_prev, dW, db\n```\n\n## L层前馈网络\n\n```python\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n\n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n\n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")    # 注意这里是 A\n    caches.append(cache)\n    assert(AL.shape == (1, X.shape[1]))\n    return AL, caches\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n\n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n\n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ...\n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ...\n    \"\"\"\n    grads = {}\n    L = len(caches)  # the number of layers\n    # m = AL.shape[1]\n    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL\n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    # Lth layer (SIGMOID -> LINEAR) gradients.\n    current_cache = caches[L - 1]\n    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n    for l in reversed(range(L - 1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], caches[l], 'relu')\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\n```\n\n## 参数初始化\n\n```python\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n\n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l - 1])  # He initialization\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n    return parameters\n```\n\n## 参数更新（梯度下降）\n\n```python\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n\n    Arguments:\n    parameters -- python dictionary containing your parameters\n    grads -- python dictionary containing your gradients, output of L_model_backward\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters\n                  parameters[\"W\" + str(l)] = ...\n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    L = len(parameters) // 2  # number of layers in the neural network\n    # Update rule for each parameter. Use a for loop.\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n    return parameters\n```\n\n## 训练模型\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 预测\n\n```python\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n\n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n\n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    m = X.shape[1]\n    p = np.zeros((1, m))\n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0, i] > 0.5:\n            p[0, i] = 1\n        else:\n            p[0, i] = 0\n    print(\"Accuracy: \" + str(np.sum((p == y) / m)))\n    return p\n```","source":"_posts/神经网络中的通用函数代码.md","raw":"---\ntitle: 神经网络中的通用函数代码\ndate: 2018-07-21 16:58:55\ntags: [神经网络, 通用函数代码]\ncategories: 深度学习\n---\n\n## 激活函数\n\n### Sigmoid\n\n```python\ndef sigmoid(Z):\n    \"\"\"\n    Implements the sigmoid activation in numpy\n\n    Arguments:\n    Z -- numpy array of any shape\n\n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    A = 1 / (1 + np.exp(-Z))\n    cache = Z\n    return A, cache\n\ndef sigmoid_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single SIGMOID unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    s = 1 / (1 + np.exp(-Z))\n    dZ = dA * s * (1 - s)\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n### Relu\n\n```python\ndef relu(Z):\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    A = np.maximum(0, Z)\n    assert(A.shape == Z.shape)\n    cache = Z\n    return A, cache\n\ndef relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n    # When z <= 0, you should set dz to 0 as well.\n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n## 代价函数\n\n### 交叉熵\n\n```python\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    m = Y.shape[1]\n    # Compute loss from aL and y.\n    cost = -1 / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    return cost\n```\n\n### L1 and L2 Loss\n\n```python\ndef L1(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L1 loss function defined above\n    \"\"\"\n    loss = np.sum(abs(yhat - y))\n    return loss\n\ndef L2(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n    \n    Returns:\n    loss -- the value of the L2 loss function defined above\n    \"\"\"\n    loss = np.sum((y - yhat) ** 2)\n    return loss\n```\n\n## 线性函数\n\n```python\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter\n    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    Z = np.dot(W, A) + b\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    return Z, cache\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = 1 / m * np.dot(dZ, A_prev.T)\n    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    return dA_prev, dW, db\n```\n\n## 线性到激活层\n\n```python\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value\n    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n    return A, cache\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n\n    Arguments:\n    dA -- post-activation gradient for current layer l\n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    return dA_prev, dW, db\n```\n\n## L层前馈网络\n\n```python\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n\n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n\n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")    # 注意这里是 A\n    caches.append(cache)\n    assert(AL.shape == (1, X.shape[1]))\n    return AL, caches\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n\n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n\n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ...\n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ...\n    \"\"\"\n    grads = {}\n    L = len(caches)  # the number of layers\n    # m = AL.shape[1]\n    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL\n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    # Lth layer (SIGMOID -> LINEAR) gradients.\n    current_cache = caches[L - 1]\n    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n    for l in reversed(range(L - 1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], caches[l], 'relu')\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\n```\n\n## 参数初始化\n\n```python\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n\n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l - 1])  # He initialization\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n    return parameters\n```\n\n## 参数更新（梯度下降）\n\n```python\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n\n    Arguments:\n    parameters -- python dictionary containing your parameters\n    grads -- python dictionary containing your gradients, output of L_model_backward\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters\n                  parameters[\"W\" + str(l)] = ...\n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    L = len(parameters) // 2  # number of layers in the neural network\n    # Update rule for each parameter. Use a for loop.\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n    return parameters\n```\n\n## 训练模型\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 预测\n\n```python\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n\n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n\n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    m = X.shape[1]\n    p = np.zeros((1, m))\n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0, i] > 0.5:\n            p[0, i] = 1\n        else:\n            p[0, i] = 0\n    print(\"Accuracy: \" + str(np.sum((p == y) / m)))\n    return p\n```","slug":"神经网络中的通用函数代码","published":1,"updated":"2018-08-07T00:36:25.239Z","_id":"cjkhjrlgt002d3bcpbz1nh009","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><h3 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements the sigmoid activation in numpy</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- numpy array of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    s = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    dZ = dA * s * (<span class=\"number\">1</span> - s)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h3 id=\"Relu\"><a href=\"#Relu\" class=\"headerlink\" title=\"Relu\"></a>Relu</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the RELU function.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- Output of the linear layer, of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = np.maximum(<span class=\"number\">0</span>, Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(A.shape == Z.shape)</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single RELU unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    dZ = np.array(dA, copy=<span class=\"keyword\">True</span>)  <span class=\"comment\"># just converting dz to a correct object.</span></span><br><span class=\"line\">    <span class=\"comment\"># When z &lt;= 0, you should set dz to 0 as well.</span></span><br><span class=\"line\">    dZ[Z &lt;= <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h2 id=\"代价函数\"><a href=\"#代价函数\" class=\"headerlink\" title=\"代价函数\"></a>代价函数</h2><h3 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a>交叉熵</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_cost</span><span class=\"params\">(AL, Y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the cost function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    cost -- cross-entropy cost</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = Y.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Compute loss from aL and y.</span></span><br><span class=\"line\">    cost = <span class=\"number\">-1</span> / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(<span class=\"number\">1</span> - Y, np.log(<span class=\"number\">1</span> - AL).T))</span><br><span class=\"line\">    cost = np.squeeze(cost)      <span class=\"comment\"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(cost.shape == ())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cost</span><br></pre></td></tr></table></figure>\n<h3 id=\"L1-and-L2-Loss\"><a href=\"#L1-and-L2-Loss\" class=\"headerlink\" title=\"L1 and L2 Loss\"></a>L1 and L2 Loss</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L1</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L1 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum(abs(yhat - y))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L2</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L2 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum((y - yhat) ** <span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性函数\"><a href=\"#线性函数\" class=\"headerlink\" title=\"线性函数\"></a>线性函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_forward</span><span class=\"params\">(A, W, b)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear part of a layer's forward propagation.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- the input of the activation function, also called pre-activation parameter</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = np.dot(W, A) + b</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(Z.shape == (W.shape[<span class=\"number\">0</span>], A.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (A, W, b)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Z, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_backward</span><span class=\"params\">(dZ, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A_prev, W, b = cache</span><br><span class=\"line\">    m = A_prev.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    dW = <span class=\"number\">1</span> / m * np.dot(dZ, A_prev.T)</span><br><span class=\"line\">    db = <span class=\"number\">1</span> / m * np.sum(dZ, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    dA_prev = np.dot(W.T, dZ)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dW.shape == W.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (db.shape == b.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性到激活层\"><a href=\"#线性到激活层\" class=\"headerlink\" title=\"线性到激活层\"></a>线性到激活层</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_forward</span><span class=\"params\">(A_prev, W, b, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- the output of the activation function, also called the post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";</span></span><br><span class=\"line\"><span class=\"string\">             stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = sigmoid(Z)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = relu(Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (A.shape == (W.shape[<span class=\"number\">0</span>], A_prev.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (linear_cache, activation_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_backward</span><span class=\"params\">(dA, cache, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient for current layer l</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    linear_cache, activation_cache = cache</span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        dZ = relu_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"L层前馈网络\"><a href=\"#L层前馈网络\" class=\"headerlink\" title=\"L层前馈网络\"></a>L层前馈网络</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_forward</span><span class=\"params\">(X, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- output of initialize_parameters_deep()</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- last post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    caches = []</span><br><span class=\"line\">    A = X</span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        A_prev = A</span><br><span class=\"line\">        A, cache = linear_activation_forward(A_prev, parameters[<span class=\"string\">'W'</span> + str(l)], parameters[<span class=\"string\">'b'</span> + str(l)], <span class=\"string\">\"relu\"</span>)</span><br><span class=\"line\">        caches.append(cache)</span><br><span class=\"line\">    <span class=\"comment\"># Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    AL, cache = linear_activation_forward(A, parameters[<span class=\"string\">'W'</span> + str(L)], parameters[<span class=\"string\">'b'</span> + str(L)], <span class=\"string\">\"sigmoid\"</span>)    <span class=\"comment\"># 注意这里是 A</span></span><br><span class=\"line\">    caches.append(cache)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(AL.shape == (<span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> AL, caches</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_backward</span><span class=\"params\">(AL, Y, caches)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    grads -- A dictionary with the gradients</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dA\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    grads = &#123;&#125;</span><br><span class=\"line\">    L = len(caches)  <span class=\"comment\"># the number of layers</span></span><br><span class=\"line\">    <span class=\"comment\"># m = AL.shape[1]</span></span><br><span class=\"line\">    Y = Y.reshape(AL.shape)  <span class=\"comment\"># after this line, Y is the same shape as AL</span></span><br><span class=\"line\">    <span class=\"comment\"># Initializing the backpropagation</span></span><br><span class=\"line\">    dAL = - (np.divide(Y, AL) - np.divide(<span class=\"number\">1</span> - Y, <span class=\"number\">1</span> - AL))</span><br><span class=\"line\">    <span class=\"comment\"># Lth layer (SIGMOID -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">    current_cache = caches[L - <span class=\"number\">1</span>]</span><br><span class=\"line\">    grads[<span class=\"string\">\"dA\"</span> + str(L)], grads[<span class=\"string\">\"dW\"</span> + str(L)], grads[<span class=\"string\">\"db\"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class=\"string\">'sigmoid'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> reversed(range(L - <span class=\"number\">1</span>)):</span><br><span class=\"line\">        <span class=\"comment\"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">        current_cache = caches[l]</span><br><span class=\"line\">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">2</span>)], caches[l], <span class=\"string\">'relu'</span>)</span><br><span class=\"line\">        grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">1</span>)] = dA_prev_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)] = dW_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)] = db_temp</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grads</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数初始化\"><a href=\"#参数初始化\" class=\"headerlink\" title=\"参数初始化\"></a>参数初始化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_parameters_deep</span><span class=\"params\">(layer_dims)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":</span></span><br><span class=\"line\"><span class=\"string\">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class=\"line\"><span class=\"string\">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">3</span>)</span><br><span class=\"line\">    parameters = &#123;&#125;</span><br><span class=\"line\">    L = len(layer_dims)            <span class=\"comment\"># number of layers in the network</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">2</span> / layer_dims[l - <span class=\"number\">1</span>])  <span class=\"comment\"># He initialization</span></span><br><span class=\"line\">        parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'b'</span> + str(l)].shape == (layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数更新（梯度下降）\"><a href=\"#参数更新（梯度下降）\" class=\"headerlink\" title=\"参数更新（梯度下降）\"></a>参数更新（梯度下降）</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters</span><span class=\"params\">(parameters, grads, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using gradient descent</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"W\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"b\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Update rule for each parameter. Use a for loop.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"预测\"><a href=\"#预测\" class=\"headerlink\" title=\"预测\"></a>预测</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(X, y, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    This function is used to predict the results of a  L-layer neural network.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data set of examples you would like to label</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters of the trained model</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    p -- predictions for the given dataset X</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    p = np.zeros((<span class=\"number\">1</span>, m))</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    probas, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># convert probas to 0/1 predictions</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, probas.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> probas[<span class=\"number\">0</span>, i] &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">0</span></span><br><span class=\"line\">    print(<span class=\"string\">\"Accuracy: \"</span> + str(np.sum((p == y) / m)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><h3 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements the sigmoid activation in numpy</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- numpy array of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    s = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    dZ = dA * s * (<span class=\"number\">1</span> - s)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h3 id=\"Relu\"><a href=\"#Relu\" class=\"headerlink\" title=\"Relu\"></a>Relu</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the RELU function.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- Output of the linear layer, of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = np.maximum(<span class=\"number\">0</span>, Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(A.shape == Z.shape)</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single RELU unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    dZ = np.array(dA, copy=<span class=\"keyword\">True</span>)  <span class=\"comment\"># just converting dz to a correct object.</span></span><br><span class=\"line\">    <span class=\"comment\"># When z &lt;= 0, you should set dz to 0 as well.</span></span><br><span class=\"line\">    dZ[Z &lt;= <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h2 id=\"代价函数\"><a href=\"#代价函数\" class=\"headerlink\" title=\"代价函数\"></a>代价函数</h2><h3 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a>交叉熵</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_cost</span><span class=\"params\">(AL, Y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the cost function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    cost -- cross-entropy cost</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = Y.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Compute loss from aL and y.</span></span><br><span class=\"line\">    cost = <span class=\"number\">-1</span> / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(<span class=\"number\">1</span> - Y, np.log(<span class=\"number\">1</span> - AL).T))</span><br><span class=\"line\">    cost = np.squeeze(cost)      <span class=\"comment\"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(cost.shape == ())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cost</span><br></pre></td></tr></table></figure>\n<h3 id=\"L1-and-L2-Loss\"><a href=\"#L1-and-L2-Loss\" class=\"headerlink\" title=\"L1 and L2 Loss\"></a>L1 and L2 Loss</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L1</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L1 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum(abs(yhat - y))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L2</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L2 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum((y - yhat) ** <span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性函数\"><a href=\"#线性函数\" class=\"headerlink\" title=\"线性函数\"></a>线性函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_forward</span><span class=\"params\">(A, W, b)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear part of a layer's forward propagation.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- the input of the activation function, also called pre-activation parameter</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = np.dot(W, A) + b</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(Z.shape == (W.shape[<span class=\"number\">0</span>], A.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (A, W, b)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Z, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_backward</span><span class=\"params\">(dZ, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A_prev, W, b = cache</span><br><span class=\"line\">    m = A_prev.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    dW = <span class=\"number\">1</span> / m * np.dot(dZ, A_prev.T)</span><br><span class=\"line\">    db = <span class=\"number\">1</span> / m * np.sum(dZ, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    dA_prev = np.dot(W.T, dZ)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dW.shape == W.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (db.shape == b.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性到激活层\"><a href=\"#线性到激活层\" class=\"headerlink\" title=\"线性到激活层\"></a>线性到激活层</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_forward</span><span class=\"params\">(A_prev, W, b, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- the output of the activation function, also called the post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";</span></span><br><span class=\"line\"><span class=\"string\">             stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = sigmoid(Z)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = relu(Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (A.shape == (W.shape[<span class=\"number\">0</span>], A_prev.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (linear_cache, activation_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_backward</span><span class=\"params\">(dA, cache, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient for current layer l</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    linear_cache, activation_cache = cache</span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        dZ = relu_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"L层前馈网络\"><a href=\"#L层前馈网络\" class=\"headerlink\" title=\"L层前馈网络\"></a>L层前馈网络</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_forward</span><span class=\"params\">(X, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- output of initialize_parameters_deep()</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- last post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    caches = []</span><br><span class=\"line\">    A = X</span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        A_prev = A</span><br><span class=\"line\">        A, cache = linear_activation_forward(A_prev, parameters[<span class=\"string\">'W'</span> + str(l)], parameters[<span class=\"string\">'b'</span> + str(l)], <span class=\"string\">\"relu\"</span>)</span><br><span class=\"line\">        caches.append(cache)</span><br><span class=\"line\">    <span class=\"comment\"># Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    AL, cache = linear_activation_forward(A, parameters[<span class=\"string\">'W'</span> + str(L)], parameters[<span class=\"string\">'b'</span> + str(L)], <span class=\"string\">\"sigmoid\"</span>)    <span class=\"comment\"># 注意这里是 A</span></span><br><span class=\"line\">    caches.append(cache)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(AL.shape == (<span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> AL, caches</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_backward</span><span class=\"params\">(AL, Y, caches)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    grads -- A dictionary with the gradients</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dA\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    grads = &#123;&#125;</span><br><span class=\"line\">    L = len(caches)  <span class=\"comment\"># the number of layers</span></span><br><span class=\"line\">    <span class=\"comment\"># m = AL.shape[1]</span></span><br><span class=\"line\">    Y = Y.reshape(AL.shape)  <span class=\"comment\"># after this line, Y is the same shape as AL</span></span><br><span class=\"line\">    <span class=\"comment\"># Initializing the backpropagation</span></span><br><span class=\"line\">    dAL = - (np.divide(Y, AL) - np.divide(<span class=\"number\">1</span> - Y, <span class=\"number\">1</span> - AL))</span><br><span class=\"line\">    <span class=\"comment\"># Lth layer (SIGMOID -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">    current_cache = caches[L - <span class=\"number\">1</span>]</span><br><span class=\"line\">    grads[<span class=\"string\">\"dA\"</span> + str(L)], grads[<span class=\"string\">\"dW\"</span> + str(L)], grads[<span class=\"string\">\"db\"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class=\"string\">'sigmoid'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> reversed(range(L - <span class=\"number\">1</span>)):</span><br><span class=\"line\">        <span class=\"comment\"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">        current_cache = caches[l]</span><br><span class=\"line\">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">2</span>)], caches[l], <span class=\"string\">'relu'</span>)</span><br><span class=\"line\">        grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">1</span>)] = dA_prev_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)] = dW_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)] = db_temp</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grads</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数初始化\"><a href=\"#参数初始化\" class=\"headerlink\" title=\"参数初始化\"></a>参数初始化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_parameters_deep</span><span class=\"params\">(layer_dims)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":</span></span><br><span class=\"line\"><span class=\"string\">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class=\"line\"><span class=\"string\">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">3</span>)</span><br><span class=\"line\">    parameters = &#123;&#125;</span><br><span class=\"line\">    L = len(layer_dims)            <span class=\"comment\"># number of layers in the network</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">2</span> / layer_dims[l - <span class=\"number\">1</span>])  <span class=\"comment\"># He initialization</span></span><br><span class=\"line\">        parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'b'</span> + str(l)].shape == (layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数更新（梯度下降）\"><a href=\"#参数更新（梯度下降）\" class=\"headerlink\" title=\"参数更新（梯度下降）\"></a>参数更新（梯度下降）</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters</span><span class=\"params\">(parameters, grads, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using gradient descent</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"W\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"b\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Update rule for each parameter. Use a for loop.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"预测\"><a href=\"#预测\" class=\"headerlink\" title=\"预测\"></a>预测</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(X, y, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    This function is used to predict the results of a  L-layer neural network.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data set of examples you would like to label</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters of the trained model</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    p -- predictions for the given dataset X</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    p = np.zeros((<span class=\"number\">1</span>, m))</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    probas, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># convert probas to 0/1 predictions</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, probas.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> probas[<span class=\"number\">0</span>, i] &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">0</span></span><br><span class=\"line\">    print(<span class=\"string\">\"Accuracy: \"</span> + str(np.sum((p == y) / m)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p</span><br></pre></td></tr></table></figure>"},{"title":"Gradient Descent Famliy","date":"2018-08-07T00:59:26.000Z","mathjax":true,"_content":"## (Batch) Gradient Descent\n\n``` python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost.\n    cost = compute_cost(a, Y)\n    # Backward propagation.\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters.\n    parameters = update_parameters(parameters, grads)     \n```\n\n## Stochastic Gradient Descent\n\n```python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost = compute_cost(a, Y[:,j])\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads)\n```\n\n## Mini-Batch Gradient descent\n\n- **Shuffle**:\n\n<img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\">\n\n- **Partition**:\n\n<img src=\"/images/partition.png\" style=\"width:550px;height:300px;\">\n\nNote that the last mini-batch might end up smaller than `mini_batch_size=64`. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is `math.floor(s)` in Python). If the total number of examples is not a multiple of `mini_batch_size=64` then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). \n\n```python\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    return mini_batches\n```\n\n## Momentum\n\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n\nMomentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. \n\n<img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\">\n<caption><center> <u><font color='purple'>**Figure 3**</u><font color='purple'>: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color='black'> </center>\n\n```python\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}    \n    # Initialize velocity\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape[0], parameters['W' + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape[0], parameters['b' + str(l+1)].shape[1]))\n        \n    return v\n```\n\n$$\\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}$$\n\n$$\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n\\end{cases}\\tag{4}$$\n\nwhere L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary.  Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that's a \"one\" on the superscript). So you will need to shift `l` to `l+1` when coding.\n\n```python\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks    \n    # Momentum update for each parameter\n    for l in range(L):        \n        # compute velocities\n        v[\"dW\" + str(l+1)] = beta * v['dW' + str(l+1)] + (1 - beta) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta * v['db' + str(l+1)] + (1 - beta) * grads['db' + str(l+1)]\n        # update parameters\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n    return parameters, v\n```\n\n\n**How do you choose $\\beta$?**\n\n- The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. \n- Common values for $\\beta$ range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. \n- Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. \n\n## Adam\n\nAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n\n**How does Adam work?**\n1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n\nThe update rule is, for $l = 1, ..., L$: \n\n<img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\">\n\nwhere:\n- t counts the number of steps taken of Adam \n- L is the number of layers\n- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n- $\\alpha$ is the learning rate\n- $\\varepsilon$ is a very small number to avoid dividing by zero\n\nAs usual, we will store all parameters in the `parameters` dictionary\n\n```python\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n    return v, s\n```\n\n```python\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients.\n        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)]\n        # Compute bias-corrected first moment estimate.\n        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t)\n        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t)\n        # Moving average of the squared gradients.\n        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads['dW' + str(l+1)] ** 2)\n        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads['db' + str(l+1)] ** 2)\n        # Compute bias-corrected second raw moment estimate.\n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n        # Update parameters. \n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n    return parameters, v, s\n```\n\n## Model with different optimization algorithms\n\n```python\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)  \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n    return parameters\n\ntrain_X, train_Y = load_dataset()\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n```\n## Summary\n\n![](/images/sgd.png)\n![](/images/minibatch.png)\n\n- **The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.**\n- **You have to tune a learning rate hyperparameter $\\alpha$.**\n- **With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).**\n- **Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.**\n\n- **Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.**\n\n- **Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster.**\n\n**Some advantages of Adam include:**\n- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n- Usually works well even with little tuning of hyperparameters (except $\\alpha$)","source":"_posts/Gradient-Descent-Famliy.md","raw":"---\ntitle: Gradient Descent Famliy\ndate: 2018-08-07 08:59:26\ntags: 优化算法\ncategories: 深度学习\nmathjax: true\n---\n## (Batch) Gradient Descent\n\n``` python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost.\n    cost = compute_cost(a, Y)\n    # Backward propagation.\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters.\n    parameters = update_parameters(parameters, grads)     \n```\n\n## Stochastic Gradient Descent\n\n```python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost = compute_cost(a, Y[:,j])\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads)\n```\n\n## Mini-Batch Gradient descent\n\n- **Shuffle**:\n\n<img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\">\n\n- **Partition**:\n\n<img src=\"/images/partition.png\" style=\"width:550px;height:300px;\">\n\nNote that the last mini-batch might end up smaller than `mini_batch_size=64`. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is `math.floor(s)` in Python). If the total number of examples is not a multiple of `mini_batch_size=64` then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). \n\n```python\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    return mini_batches\n```\n\n## Momentum\n\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n\nMomentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. \n\n<img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\">\n<caption><center> <u><font color='purple'>**Figure 3**</u><font color='purple'>: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color='black'> </center>\n\n```python\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}    \n    # Initialize velocity\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape[0], parameters['W' + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape[0], parameters['b' + str(l+1)].shape[1]))\n        \n    return v\n```\n\n$$\\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}$$\n\n$$\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n\\end{cases}\\tag{4}$$\n\nwhere L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary.  Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that's a \"one\" on the superscript). So you will need to shift `l` to `l+1` when coding.\n\n```python\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks    \n    # Momentum update for each parameter\n    for l in range(L):        \n        # compute velocities\n        v[\"dW\" + str(l+1)] = beta * v['dW' + str(l+1)] + (1 - beta) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta * v['db' + str(l+1)] + (1 - beta) * grads['db' + str(l+1)]\n        # update parameters\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n    return parameters, v\n```\n\n\n**How do you choose $\\beta$?**\n\n- The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. \n- Common values for $\\beta$ range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. \n- Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. \n\n## Adam\n\nAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n\n**How does Adam work?**\n1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n\nThe update rule is, for $l = 1, ..., L$: \n\n<img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\">\n\nwhere:\n- t counts the number of steps taken of Adam \n- L is the number of layers\n- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n- $\\alpha$ is the learning rate\n- $\\varepsilon$ is a very small number to avoid dividing by zero\n\nAs usual, we will store all parameters in the `parameters` dictionary\n\n```python\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n    return v, s\n```\n\n```python\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients.\n        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)]\n        # Compute bias-corrected first moment estimate.\n        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t)\n        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t)\n        # Moving average of the squared gradients.\n        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads['dW' + str(l+1)] ** 2)\n        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads['db' + str(l+1)] ** 2)\n        # Compute bias-corrected second raw moment estimate.\n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n        # Update parameters. \n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n    return parameters, v, s\n```\n\n## Model with different optimization algorithms\n\n```python\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)  \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n    return parameters\n\ntrain_X, train_Y = load_dataset()\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n```\n## Summary\n\n![](/images/sgd.png)\n![](/images/minibatch.png)\n\n- **The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.**\n- **You have to tune a learning rate hyperparameter $\\alpha$.**\n- **With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).**\n- **Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.**\n\n- **Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.**\n\n- **Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster.**\n\n**Some advantages of Adam include:**\n- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n- Usually works well even with little tuning of hyperparameters (except $\\alpha$)","slug":"Gradient-Descent-Famliy","published":1,"updated":"2018-08-07T02:19:19.385Z","_id":"cjkizyt7v0000hovoiigd71jz","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"Batch-Gradient-Descent\"><a href=\"#Batch-Gradient-Descent\" class=\"headerlink\" title=\"(Batch) Gradient Descent\"></a>(Batch) Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    a, caches = forward_propagation(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">    cost = compute_cost(a, Y)</span><br><span class=\"line\">    <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">    grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Stochastic-Gradient-Descent\"><a href=\"#Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"Stochastic Gradient Descent\"></a>Stochastic Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, m):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">        cost = compute_cost(a, Y[:,j])</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">        grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Mini-Batch-Gradient-descent\"><a href=\"#Mini-Batch-Gradient-descent\" class=\"headerlink\" title=\"Mini-Batch Gradient descent\"></a>Mini-Batch Gradient descent</h2><ul>\n<li><strong>Shuffle</strong>:</li>\n</ul>\n<p><img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\"></p>\n<ul>\n<li><strong>Partition</strong>:</li>\n</ul>\n<p><img src=\"/images/partition.png\" style=\"width:550px;height:300px;\"></p>\n<p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_mini_batches</span><span class=\"params\">(X, Y, mini_batch_size = <span class=\"number\">64</span>, seed = <span class=\"number\">0</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Creates a list of random minibatches from (X, Y)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    np.random.seed(seed)            <span class=\"comment\"># To make your \"random\" minibatches the same as ours</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]                  <span class=\"comment\"># number of training examples</span></span><br><span class=\"line\">    mini_batches = []</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\"># Step 1: Shuffle (X, Y)</span></span><br><span class=\"line\">    permutation = list(np.random.permutation(m))</span><br><span class=\"line\">    shuffled_X = X[:, permutation]</span><br><span class=\"line\">    shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class=\"line\">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class=\"comment\"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_complete_minibatches):</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> m % mini_batch_size != <span class=\"number\">0</span>:</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mini_batches</span><br></pre></td></tr></table></figure>\n<h2 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. </p>\n<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. </p>\n<p><img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\"></p>\n<p><caption><center> <u><font color=\"purple\"><strong>Figure 3</strong></font></u><font color=\"purple\">: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color=\"black\"> </font></font></center></caption></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_velocity</span><span class=\"params\">(parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes the velocity as a python dictionary with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity.</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize velocity</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<p>$$\\begin{cases}<br>v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\<br>W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}<br>\\end{cases}\\tag{3}$$</p>\n<p>$$\\begin{cases}<br>v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\<br>b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}<br>\\end{cases}\\tag{4}$$</p>\n<p>where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.  Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_momentum</span><span class=\"params\">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Momentum</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity:</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    beta -- the momentum hyperparameter, scalar</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing your updated velocities</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks    </span></span><br><span class=\"line\">    <span class=\"comment\"># Momentum update for each parameter</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):        </span><br><span class=\"line\">        <span class=\"comment\"># compute velocities</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># update parameters</span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v</span><br></pre></td></tr></table></figure>\n<p><strong>How do you choose $\\beta$?</strong></p>\n<ul>\n<li>The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. </li>\n<li>Common values for $\\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. </li>\n<li>Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. </li>\n</ul>\n<h2 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. </p>\n<p><strong>How does Adam work?</strong></p>\n<ol>\n<li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li>\n<li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li>\n<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>\n</ol>\n<p>The update rule is, for $l = 1, …, L$: </p>\n<p><img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\"></p>\n<p>where:</p>\n<ul>\n<li>t counts the number of steps taken of Adam </li>\n<li>L is the number of layers</li>\n<li>$\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li>\n<li>$\\alpha$ is the learning rate</li>\n<li>$\\varepsilon$ is a very small number to avoid dividing by zero</li>\n</ul>\n<p>As usual, we will store all parameters in the <code>parameters</code> dictionary</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_adam</span><span class=\"params\">(parameters)</span> :</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes v and s as two python dictionaries with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"W\" + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"b\" + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns: </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;</span><br><span class=\"line\">    s = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> v, s</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_adam</span><span class=\"params\">(parameters, grads, v, s, t, learning_rate = <span class=\"number\">0.01</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                                beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Adam</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                 <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing first moment estimate, python dictionary</span></span><br><span class=\"line\">    s_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing second moment estimate, python dictionary    </span></span><br><span class=\"line\">    <span class=\"comment\"># Perform Adam update on all parameters</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the gradients.</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected first moment estimate.</span></span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the squared gradients.</span></span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected second raw moment estimate.</span></span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters. </span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v, s</span><br></pre></td></tr></table></figure>\n<h2 id=\"Model-with-different-optimization-algorithms\"><a href=\"#Model-with-different-optimization-algorithms\" class=\"headerlink\" title=\"Model with different optimization algorithms\"></a>Model with different optimization algorithms</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(X, Y, layers_dims, optimizer, learning_rate = <span class=\"number\">0.0007</span>, mini_batch_size = <span class=\"number\">64</span>, beta = <span class=\"number\">0.9</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">          beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>, num_epochs = <span class=\"number\">10000</span>, print_cost = True)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (2, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- python list, containing the size of each layer</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- the size of a mini batch</span></span><br><span class=\"line\"><span class=\"string\">    beta -- Momentum hyperparameter</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\">    num_epochs -- number of epochs</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    L = len(layers_dims)             <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    costs = []                       <span class=\"comment\"># to keep track of the cost</span></span><br><span class=\"line\">    t = <span class=\"number\">0</span>                            <span class=\"comment\"># initializing the counter required for Adam update</span></span><br><span class=\"line\">    seed = <span class=\"number\">10</span>                        <span class=\"comment\"># For grading purposes, so that your \"random\" minibatches are the same as ours</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize parameters</span></span><br><span class=\"line\">    parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Initialize the optimizer</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">        <span class=\"keyword\">pass</span> <span class=\"comment\"># no initialization required for gradient descent</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">        v = initialize_velocity(parameters)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">        v, s = initialize_adam(parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Optimization loop</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class=\"line\">        seed = seed + <span class=\"number\">1</span></span><br><span class=\"line\">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> minibatch <span class=\"keyword\">in</span> minibatches:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Select a minibatch</span></span><br><span class=\"line\">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">            cost = compute_cost(a3, minibatch_Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Update parameters</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">                t = t + <span class=\"number\">1</span> <span class=\"comment\"># Adam counter</span></span><br><span class=\"line\">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 1000 epoch</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">print</span> (<span class=\"string\">\"Cost after epoch %i: %f\"</span> %(i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)  </span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(costs)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'epochs (per 100)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate = \"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br><span class=\"line\"></span><br><span class=\"line\">train_X, train_Y = load_dataset()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># train 3-layer model</span></span><br><span class=\"line\">layers_dims = [train_X.shape[<span class=\"number\">0</span>], <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class=\"string\">\"gd\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Predict</span></span><br><span class=\"line\">predictions = predict(train_X, train_Y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Plot decision boundary</span></span><br><span class=\"line\">plt.title(<span class=\"string\">\"Model with Gradient Descent optimization\"</span>)</span><br><span class=\"line\">axes = plt.gca()</span><br><span class=\"line\">axes.set_xlim([<span class=\"number\">-1.5</span>,<span class=\"number\">2.5</span>])</span><br><span class=\"line\">axes.set_ylim([<span class=\"number\">-1</span>,<span class=\"number\">1.5</span>])</span><br><span class=\"line\">plot_decision_boundary(<span class=\"keyword\">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p><img src=\"/images/sgd.png\" alt=\"\"><br><img src=\"/images/minibatch.png\" alt=\"\"></p>\n<ul>\n<li><strong>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.</strong></li>\n<li><strong>You have to tune a learning rate hyperparameter $\\alpha$.</strong></li>\n<li><strong>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</strong></li>\n<li><p><strong>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</strong></p>\n</li>\n<li><p><strong>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</strong></p>\n</li>\n<li><p><strong>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</strong></p>\n</li>\n</ul>\n<p><strong>Some advantages of Adam include:</strong></p>\n<ul>\n<li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) </li>\n<li>Usually works well even with little tuning of hyperparameters (except $\\alpha$)</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Batch-Gradient-Descent\"><a href=\"#Batch-Gradient-Descent\" class=\"headerlink\" title=\"(Batch) Gradient Descent\"></a>(Batch) Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    a, caches = forward_propagation(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">    cost = compute_cost(a, Y)</span><br><span class=\"line\">    <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">    grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Stochastic-Gradient-Descent\"><a href=\"#Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"Stochastic Gradient Descent\"></a>Stochastic Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, m):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">        cost = compute_cost(a, Y[:,j])</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">        grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Mini-Batch-Gradient-descent\"><a href=\"#Mini-Batch-Gradient-descent\" class=\"headerlink\" title=\"Mini-Batch Gradient descent\"></a>Mini-Batch Gradient descent</h2><ul>\n<li><strong>Shuffle</strong>:</li>\n</ul>\n<p><img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\"></p>\n<ul>\n<li><strong>Partition</strong>:</li>\n</ul>\n<p><img src=\"/images/partition.png\" style=\"width:550px;height:300px;\"></p>\n<p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_mini_batches</span><span class=\"params\">(X, Y, mini_batch_size = <span class=\"number\">64</span>, seed = <span class=\"number\">0</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Creates a list of random minibatches from (X, Y)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    np.random.seed(seed)            <span class=\"comment\"># To make your \"random\" minibatches the same as ours</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]                  <span class=\"comment\"># number of training examples</span></span><br><span class=\"line\">    mini_batches = []</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\"># Step 1: Shuffle (X, Y)</span></span><br><span class=\"line\">    permutation = list(np.random.permutation(m))</span><br><span class=\"line\">    shuffled_X = X[:, permutation]</span><br><span class=\"line\">    shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class=\"line\">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class=\"comment\"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_complete_minibatches):</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> m % mini_batch_size != <span class=\"number\">0</span>:</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mini_batches</span><br></pre></td></tr></table></figure>\n<h2 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. </p>\n<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. </p>\n<p><img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\"></p>\n<p><caption><center> <u><font color=\"purple\"><strong>Figure 3</strong></font></u><font color=\"purple\">: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color=\"black\"> </font></font></center></caption></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_velocity</span><span class=\"params\">(parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes the velocity as a python dictionary with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity.</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize velocity</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<p>$$\\begin{cases}<br>v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\<br>W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}<br>\\end{cases}\\tag{3}$$</p>\n<p>$$\\begin{cases}<br>v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\<br>b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}<br>\\end{cases}\\tag{4}$$</p>\n<p>where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.  Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_momentum</span><span class=\"params\">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Momentum</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity:</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    beta -- the momentum hyperparameter, scalar</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing your updated velocities</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks    </span></span><br><span class=\"line\">    <span class=\"comment\"># Momentum update for each parameter</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):        </span><br><span class=\"line\">        <span class=\"comment\"># compute velocities</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># update parameters</span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v</span><br></pre></td></tr></table></figure>\n<p><strong>How do you choose $\\beta$?</strong></p>\n<ul>\n<li>The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. </li>\n<li>Common values for $\\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. </li>\n<li>Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. </li>\n</ul>\n<h2 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. </p>\n<p><strong>How does Adam work?</strong></p>\n<ol>\n<li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li>\n<li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li>\n<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>\n</ol>\n<p>The update rule is, for $l = 1, …, L$: </p>\n<p><img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\"></p>\n<p>where:</p>\n<ul>\n<li>t counts the number of steps taken of Adam </li>\n<li>L is the number of layers</li>\n<li>$\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li>\n<li>$\\alpha$ is the learning rate</li>\n<li>$\\varepsilon$ is a very small number to avoid dividing by zero</li>\n</ul>\n<p>As usual, we will store all parameters in the <code>parameters</code> dictionary</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_adam</span><span class=\"params\">(parameters)</span> :</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes v and s as two python dictionaries with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"W\" + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"b\" + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns: </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;</span><br><span class=\"line\">    s = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> v, s</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_adam</span><span class=\"params\">(parameters, grads, v, s, t, learning_rate = <span class=\"number\">0.01</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                                beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Adam</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                 <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing first moment estimate, python dictionary</span></span><br><span class=\"line\">    s_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing second moment estimate, python dictionary    </span></span><br><span class=\"line\">    <span class=\"comment\"># Perform Adam update on all parameters</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the gradients.</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected first moment estimate.</span></span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the squared gradients.</span></span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected second raw moment estimate.</span></span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters. </span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v, s</span><br></pre></td></tr></table></figure>\n<h2 id=\"Model-with-different-optimization-algorithms\"><a href=\"#Model-with-different-optimization-algorithms\" class=\"headerlink\" title=\"Model with different optimization algorithms\"></a>Model with different optimization algorithms</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(X, Y, layers_dims, optimizer, learning_rate = <span class=\"number\">0.0007</span>, mini_batch_size = <span class=\"number\">64</span>, beta = <span class=\"number\">0.9</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">          beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>, num_epochs = <span class=\"number\">10000</span>, print_cost = True)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (2, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- python list, containing the size of each layer</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- the size of a mini batch</span></span><br><span class=\"line\"><span class=\"string\">    beta -- Momentum hyperparameter</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\">    num_epochs -- number of epochs</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    L = len(layers_dims)             <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    costs = []                       <span class=\"comment\"># to keep track of the cost</span></span><br><span class=\"line\">    t = <span class=\"number\">0</span>                            <span class=\"comment\"># initializing the counter required for Adam update</span></span><br><span class=\"line\">    seed = <span class=\"number\">10</span>                        <span class=\"comment\"># For grading purposes, so that your \"random\" minibatches are the same as ours</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize parameters</span></span><br><span class=\"line\">    parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Initialize the optimizer</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">        <span class=\"keyword\">pass</span> <span class=\"comment\"># no initialization required for gradient descent</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">        v = initialize_velocity(parameters)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">        v, s = initialize_adam(parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Optimization loop</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class=\"line\">        seed = seed + <span class=\"number\">1</span></span><br><span class=\"line\">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> minibatch <span class=\"keyword\">in</span> minibatches:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Select a minibatch</span></span><br><span class=\"line\">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">            cost = compute_cost(a3, minibatch_Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Update parameters</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">                t = t + <span class=\"number\">1</span> <span class=\"comment\"># Adam counter</span></span><br><span class=\"line\">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 1000 epoch</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">print</span> (<span class=\"string\">\"Cost after epoch %i: %f\"</span> %(i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)  </span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(costs)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'epochs (per 100)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate = \"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br><span class=\"line\"></span><br><span class=\"line\">train_X, train_Y = load_dataset()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># train 3-layer model</span></span><br><span class=\"line\">layers_dims = [train_X.shape[<span class=\"number\">0</span>], <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class=\"string\">\"gd\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Predict</span></span><br><span class=\"line\">predictions = predict(train_X, train_Y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Plot decision boundary</span></span><br><span class=\"line\">plt.title(<span class=\"string\">\"Model with Gradient Descent optimization\"</span>)</span><br><span class=\"line\">axes = plt.gca()</span><br><span class=\"line\">axes.set_xlim([<span class=\"number\">-1.5</span>,<span class=\"number\">2.5</span>])</span><br><span class=\"line\">axes.set_ylim([<span class=\"number\">-1</span>,<span class=\"number\">1.5</span>])</span><br><span class=\"line\">plot_decision_boundary(<span class=\"keyword\">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p><img src=\"/images/sgd.png\" alt=\"\"><br><img src=\"/images/minibatch.png\" alt=\"\"></p>\n<ul>\n<li><strong>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.</strong></li>\n<li><strong>You have to tune a learning rate hyperparameter $\\alpha$.</strong></li>\n<li><strong>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</strong></li>\n<li><p><strong>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</strong></p>\n</li>\n<li><p><strong>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</strong></p>\n</li>\n<li><p><strong>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</strong></p>\n</li>\n</ul>\n<p><strong>Some advantages of Adam include:</strong></p>\n<ul>\n<li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) </li>\n<li>Usually works well even with little tuning of hyperparameters (except $\\alpha$)</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjkhjrl9c00003bcpyg6ta5u9","category_id":"cjkhjrl9k00023bcph8rood3t","_id":"cjkhjrl9o00073bcpc0ulqnhb"},{"post_id":"cjkhjrl9i00013bcp7u68pocb","category_id":"cjkhjrl9k00023bcph8rood3t","_id":"cjkhjrl9p00093bcpjxlb346h"},{"post_id":"cjkhjrlan000a3bcp2eqwt6n0","category_id":"cjkhjrlap000c3bcp889b57c7","_id":"cjkhjrlav000l3bcp1tsx1jfs"},{"post_id":"cjkhjrlao000b3bcptfl6xk6j","category_id":"cjkhjrlat000h3bcpkv1d5xiu","_id":"cjkhjrlaw000p3bcpkus4ywh6"},{"post_id":"cjkhjrlaq000e3bcpqzriecmo","category_id":"cjkhjrlav000m3bcp1n1uywc0","_id":"cjkhjrlay000u3bcpno9zw3fb"},{"post_id":"cjkhjrlar000f3bcppm2z2umx","category_id":"cjkhjrlat000h3bcpkv1d5xiu","_id":"cjkhjrlaz000x3bcp60mskhvr"},{"post_id":"cjkhjrlas000g3bcplvyckq5p","category_id":"cjkhjrlay000t3bcpj7ycv82m","_id":"cjkhjrlb000103bcp97scs2kc"},{"post_id":"cjkhjrlau000k3bcp9jowogki","category_id":"cjkhjrlaz000y3bcpb4oojgoj","_id":"cjkhjrlb000113bcpwtdqw4xh"},{"post_id":"cjkhjrlfy00123bcp408tow62","category_id":"cjkhjrlav000m3bcp1n1uywc0","_id":"cjkhjrlga00193bcpbfqeibtv"},{"post_id":"cjkhjrlga001a3bcp9t71butm","category_id":"cjkhjrlav000m3bcp1n1uywc0","_id":"cjkhjrlgf001i3bcpcxjrhel6"},{"post_id":"cjkhjrlg100143bcp4sme643c","category_id":"cjkhjrlg800173bcpfmwcv2mi","_id":"cjkhjrlgg001m3bcpusy8yv1c"},{"post_id":"cjkhjrlgc001f3bcp8ajkkkch","category_id":"cjkhjrlav000m3bcp1n1uywc0","_id":"cjkhjrlgh001p3bcpzbhl2gyg"},{"post_id":"cjkhjrlg900183bcp1jx8siam","category_id":"cjkhjrlgc001e3bcpq2l5dsk5","_id":"cjkhjrlgi001s3bcpr2kfrvsj"},{"post_id":"cjkhjrlge001h3bcpzu3ta8k7","category_id":"cjkhjrlav000m3bcp1n1uywc0","_id":"cjkhjrlgj001v3bcphvu0d4fv"},{"post_id":"cjkhjrlgb001d3bcpij3w5val","category_id":"cjkhjrlgf001k3bcp8rz52r0y","_id":"cjkhjrlgl001y3bcpwxksac13"},{"post_id":"cjkhjrlgh001o3bcp1gcot466","category_id":"cjkhjrlav000m3bcp1n1uywc0","_id":"cjkhjrlgm00203bcpex13cfbu"},{"post_id":"cjkhjrlgi001r3bcp9a8lay5y","category_id":"cjkhjrlav000m3bcp1n1uywc0","_id":"cjkhjrlgn00243bcpecxslkq9"},{"post_id":"cjkhjrlgj001u3bcpnrecmmr1","category_id":"cjkhjrlay000t3bcpj7ycv82m","_id":"cjkhjrlgq00283bcp04i8ko6y"},{"post_id":"cjkhjrlgk001x3bcpen67q7lv","category_id":"cjkhjrlgc001e3bcpq2l5dsk5","_id":"cjkhjrlgs002b3bcp0pdwy6ya"},{"post_id":"cjkhjrlgo00273bcpg5xblft8","category_id":"cjkhjrlgc001e3bcpq2l5dsk5","_id":"cjkhjrlgv002h3bcp8wrg3yiw"},{"post_id":"cjkhjrlgr002a3bcp3p0310pj","category_id":"cjkhjrlap000c3bcp889b57c7","_id":"cjkhjrlgv002j3bcp1pwyfo5l"},{"post_id":"cjkhjrlgl001z3bcpvcg84t3s","category_id":"cjkhjrlgn00253bcp64338gxo","_id":"cjkhjrlgw002l3bcp8bgigxwn"},{"post_id":"cjkhjrlgt002d3bcpbz1nh009","category_id":"cjkhjrlap000c3bcp889b57c7","_id":"cjkhjrlgw002n3bcpisbt892h"},{"post_id":"cjkhjrlgm00233bcpao1as9lb","category_id":"cjkhjrlgu002e3bcp7rxrgmt1","_id":"cjkhjrlgw002o3bcpjapohebw"},{"post_id":"cjkizyt7v0000hovoiigd71jz","category_id":"cjkhjrlap000c3bcp889b57c7","_id":"cjkj049300002hovo69su7iju"}],"PostTag":[{"post_id":"cjkhjrl9c00003bcpyg6ta5u9","tag_id":"cjkhjrl9l00033bcphrlfucmu","_id":"cjkhjrl9n00063bcpddoigt7p"},{"post_id":"cjkhjrl9i00013bcp7u68pocb","tag_id":"cjkhjrl9m00053bcp0y8c65jr","_id":"cjkhjrl9p00083bcp5zjkl5pe"},{"post_id":"cjkhjrlan000a3bcp2eqwt6n0","tag_id":"cjkhjrlaq000d3bcp028w19db","_id":"cjkhjrlau000j3bcp1h5dd9re"},{"post_id":"cjkhjrlao000b3bcptfl6xk6j","tag_id":"cjkhjrlat000i3bcpcuw84lt0","_id":"cjkhjrlaw000o3bcpss6m0ltr"},{"post_id":"cjkhjrlaq000e3bcpqzriecmo","tag_id":"cjkhjrlaw000n3bcpe0ii9o5k","_id":"cjkhjrlax000s3bcpltd0np98"},{"post_id":"cjkhjrlas000g3bcplvyckq5p","tag_id":"cjkhjrlax000r3bcpxcu6vu6v","_id":"cjkhjrlaz000w3bcpcal49zbl"},{"post_id":"cjkhjrlau000k3bcp9jowogki","tag_id":"cjkhjrlay000v3bcppc0bpzlh","_id":"cjkhjrlb0000z3bcpducobgmo"},{"post_id":"cjkhjrlfy00123bcp408tow62","tag_id":"cjkhjrlg600153bcpqeyef1m8","_id":"cjkhjrlgb001c3bcpz8dvn6rw"},{"post_id":"cjkhjrlg100143bcp4sme643c","tag_id":"cjkhjrlgb001b3bcppd0yutdn","_id":"cjkhjrlgf001j3bcppol0bs2v"},{"post_id":"cjkhjrlg600163bcpoii0n3n8","tag_id":"cjkhjrlgd001g3bcp9xiyjh1d","_id":"cjkhjrlgh001q3bcpzo8hoqyl"},{"post_id":"cjkhjrlg900183bcp1jx8siam","tag_id":"cjkhjrlgg001n3bcpytpg53oe","_id":"cjkhjrlgk001w3bcp7nbf7twj"},{"post_id":"cjkhjrlgk001x3bcpen67q7lv","tag_id":"cjkhjrlgg001n3bcpytpg53oe","_id":"cjkhjrlgm00223bcpofbea798"},{"post_id":"cjkhjrlga001a3bcp9t71butm","tag_id":"cjkhjrlgi001t3bcpzxg30p9m","_id":"cjkhjrlgo00263bcpqyrcmaet"},{"post_id":"cjkhjrlgb001d3bcpij3w5val","tag_id":"cjkhjrlgm00213bcpy71cseet","_id":"cjkhjrlgs002c3bcpspzz05nl"},{"post_id":"cjkhjrlgo00273bcpg5xblft8","tag_id":"cjkhjrlgg001n3bcpytpg53oe","_id":"cjkhjrlgu002f3bcpag3ei7sn"},{"post_id":"cjkhjrlgc001f3bcp8ajkkkch","tag_id":"cjkhjrlgq00293bcpjckveltf","_id":"cjkhjrlgv002i3bcp5qgcf4ic"},{"post_id":"cjkhjrlge001h3bcpzu3ta8k7","tag_id":"cjkhjrlgv002g3bcp1krm8def","_id":"cjkhjrlgw002m3bcp92v2vx80"},{"post_id":"cjkhjrlgf001l3bcph0lt2itj","tag_id":"cjkhjrlgw002k3bcp1pvyje77","_id":"cjkhjrlgx002q3bcp6o8fudg4"},{"post_id":"cjkhjrlgh001o3bcp1gcot466","tag_id":"cjkhjrlgw002p3bcpcuqk0b2c","_id":"cjkhjrlgx002s3bcp2oslokg6"},{"post_id":"cjkhjrlgi001r3bcp9a8lay5y","tag_id":"cjkhjrlgx002r3bcp6u4ai5pm","_id":"cjkhjrlgx002u3bcpcmdhjscr"},{"post_id":"cjkhjrlgj001u3bcpnrecmmr1","tag_id":"cjkhjrlgx002t3bcp8h5en2v7","_id":"cjkhjrlgy002w3bcpu42hbks0"},{"post_id":"cjkhjrlgl001z3bcpvcg84t3s","tag_id":"cjkhjrlgy002v3bcpabj9bct4","_id":"cjkhjrlgz002y3bcp32a1x4lq"},{"post_id":"cjkhjrlgm00233bcpao1as9lb","tag_id":"cjkhjrlgy002x3bcpbpxf9gw1","_id":"cjkhjrlh000303bcpj9u8pw34"},{"post_id":"cjkhjrlgr002a3bcp3p0310pj","tag_id":"cjkhjrlgz002z3bcpap8o34lm","_id":"cjkhjrlh000323bcp9qxwo0ep"},{"post_id":"cjkhjrlgt002d3bcpbz1nh009","tag_id":"cjkhjrlh000313bcpywpqnx0s","_id":"cjkhjrlh000343bcpu6xwgvtv"},{"post_id":"cjkhjrlgt002d3bcpbz1nh009","tag_id":"cjkhjrlh000333bcptq46i8u6","_id":"cjkhjrlh000353bcpi1mq26pf"},{"post_id":"cjkizyt7v0000hovoiigd71jz","tag_id":"cjkhjrlgz002z3bcpap8o34lm","_id":"cjkj239z50000n4vo94hxx4m5"}],"Tag":[{"name":"进化算法","_id":"cjkhjrl9l00033bcphrlfucmu"},{"name":"遗传算法","_id":"cjkhjrl9m00053bcp0y8c65jr"},{"name":"DNN应用","_id":"cjkhjrlaq000d3bcp028w19db"},{"name":"hexo","_id":"cjkhjrlat000i3bcpcuw84lt0"},{"name":"dropout","_id":"cjkhjrlaw000n3bcpe0ii9o5k"},{"name":"程序员实用工具","_id":"cjkhjrlax000r3bcpxcu6vu6v"},{"name":"路遥, 人生","_id":"cjkhjrlay000v3bcppc0bpzlh"},{"name":"初始化参数","_id":"cjkhjrlg600153bcpqeyef1m8"},{"name":"动态规划","_id":"cjkhjrlgb001b3bcppd0yutdn"},{"name":"策略游戏","_id":"cjkhjrlgd001g3bcp9xiyjh1d"},{"name":"爱情心理学","_id":"cjkhjrlgg001n3bcpytpg53oe"},{"name":"数据划分","_id":"cjkhjrlgi001t3bcpzxg30p9m"},{"name":"通用数据操作","_id":"cjkhjrlgm00213bcpy71cseet"},{"name":"标准化输入","_id":"cjkhjrlgq00293bcpjckveltf"},{"name":"梯度检验","_id":"cjkhjrlgv002g3bcp1krm8def"},{"name":"梯度消失和梯度爆炸","_id":"cjkhjrlgw002k3bcp1pvyje77"},{"name":"模型估计","_id":"cjkhjrlgw002p3bcpcuqk0b2c"},{"name":"正则化","_id":"cjkhjrlgx002r3bcp6u4ai5pm"},{"name":"git","_id":"cjkhjrlgx002t3bcp8h5en2v7"},{"name":"现代诗","_id":"cjkhjrlgy002v3bcpabj9bct4"},{"name":"计算限制","_id":"cjkhjrlgy002x3bcpbpxf9gw1"},{"name":"优化算法","_id":"cjkhjrlgz002z3bcpap8o34lm"},{"name":"神经网络","_id":"cjkhjrlh000313bcpywpqnx0s"},{"name":"通用函数代码","_id":"cjkhjrlh000333bcptq46i8u6"},{"name":"梯度下降","_id":"cjkj0x8mx0000ykvo9qazdrth"}]}}