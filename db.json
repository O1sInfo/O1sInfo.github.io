{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/Convolutions-on-RGB-image.png","path":"images/Convolutions-on-RGB-image.png","modified":1,"renderable":0},{"_id":"source/images/Stride.jpg","path":"images/Stride.jpg","modified":1,"renderable":0},{"_id":"source/images/adam.PNG","path":"images/adam.PNG","modified":1,"renderable":0},{"_id":"source/images/Padding.jpg","path":"images/Padding.jpg","modified":1,"renderable":0},{"_id":"source/images/alexnet_ilsvrc.PNG","path":"images/alexnet_ilsvrc.PNG","modified":1,"renderable":0},{"_id":"source/images/res1.PNG","path":"images/res1.PNG","modified":1,"renderable":0},{"_id":"source/images/eas.png","path":"images/eas.png","modified":1,"renderable":0},{"_id":"source/images/partition.png","path":"images/partition.png","modified":1,"renderable":0},{"_id":"source/images/sgd.png","path":"images/sgd.png","modified":1,"renderable":0},{"_id":"source/images/minibatch.png","path":"images/minibatch.png","modified":1,"renderable":0},{"_id":"source/images/my_image.jpg","path":"images/my_image.jpg","modified":1,"renderable":0},{"_id":"source/images/alexnet_architecture.PNG","path":"images/alexnet_architecture.PNG","modified":1,"renderable":0},{"_id":"source/images/momentum.png","path":"images/momentum.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.jpeg","path":"images/avatar.jpeg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"source/images/shuffle.png","path":"images/shuffle.png","modified":1,"renderable":0},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/images/LlayerNN.png","path":"images/LlayerNN.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"source/images/动态规划.jpg","path":"images/动态规划.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"source/images/dl_pic9_2.jpg","path":"images/dl_pic9_2.jpg","modified":1,"renderable":0},{"_id":"source/images/dl_pic9_8.jpg","path":"images/dl_pic9_8.jpg","modified":1,"renderable":0},{"_id":"source/images/dl_pic9_5.jpg","path":"images/dl_pic9_5.jpg","modified":1,"renderable":0},{"_id":"source/images/dl_pic9_7.jpg","path":"images/dl_pic9_7.jpg","modified":1,"renderable":0}],"Cache":[{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1534643975934},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1534643975929},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1534643975935},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1534643975928},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1534643975935},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1534643975936},{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1534643975928},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1534643975938},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1534643975937},{"_id":"themes/next/README.md","hash":"50abff86ffe4113051a409c1ed9261195d2aead0","modified":1534643975940},{"_id":"themes/next/README.cn.md","hash":"23e92a2599725db2f8dbd524fbef2087c6d11c7b","modified":1534643975939},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1534643975938},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1534643975943},{"_id":"source/_posts/BeautifulSoup.md","hash":"1008211f5e6229cbcb008945f37f9b4bf99e2d08","modified":1535687506501},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1534643975944},{"_id":"source/_posts/DNN应用1-识别猫.md","hash":"b1710cf7ffc930619cf1a4e3e32099755527c42c","modified":1535687326687},{"_id":"source/_posts/Evolutionary-Algorithms.md","hash":"ed4b9fa58dce005de7894dfc88b27cdb0cd4a0c7","modified":1534643975787},{"_id":"source/_posts/Genetic-Algorithms.md","hash":"dcdf9134d8883b1761660673c4727eb14fca42ac","modified":1534643975788},{"_id":"source/_posts/Gradient-Descent-Famliy.md","hash":"0629933ed0327d4bf998cc82768e1fe547f88bdd","modified":1534643975789},{"_id":"source/_posts/How-to-set-up-a-blog-with-hexo-on-github-io.md","hash":"b2d62d7d9a8e27d598ac19144990ec4b6e184ab7","modified":1534643975790},{"_id":"source/_posts/ImageNet-Classification-wih-Deep-Convolutional-Neural-Network.md","hash":"3a148b4726ef28f262e7e796f3347a7cfb38e00a","modified":1535441142186},{"_id":"source/_posts/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recongnition.md","hash":"a848892bcf6d245a7237463bdc2578633f559878","modified":1535687566414},{"_id":"source/_posts/dropout.md","hash":"6a99f299ac939dd3a41df7835a8a087b927687f9","modified":1535687345231},{"_id":"source/_posts/github使用手册.md","hash":"0a47a341a124c82322d4ad91955e92a6f71856b3","modified":1534643975791},{"_id":"source/_posts/hello-world.md","hash":"b4f283c1f275e64526f5fc48cbe315056937d25b","modified":1534643975792},{"_id":"source/_posts/matplotlib.md","hash":"eba9b1652a8c19b6105ad07694a047dcc0807a80","modified":1535687505318},{"_id":"source/_posts/python内置小工具.md","hash":"ef8b4eb12c79a9dbd58d7b8f336736ae2d803158","modified":1535687502853},{"_id":"source/_posts/requests.md","hash":"59529895800fb8482113452f10018a54111e2e9a","modified":1535687504175},{"_id":"source/_posts/人生-路遥.md","hash":"f5d4a086290f8ade7329baa72e8b935674369c12","modified":1535687293715},{"_id":"source/_posts/初始化参数.md","hash":"ef2ef9e89b5e89136093e6ec6f764a7efc06a593","modified":1535687536653},{"_id":"source/_posts/动态规划.md","hash":"6808f1f158916d3969678bb88c1f18bf48e30aa7","modified":1534643975797},{"_id":"source/_posts/十个策略故事.md","hash":"b2a623b9b7c150872b3c66dd7a7b0a9942eed343","modified":1534643975798},{"_id":"source/_posts/卷积神经网络.md","hash":"cb922c50498f59cb74d8ec93d09061dd459c10e9","modified":1535276910935},{"_id":"source/_posts/布雷默曼极限.md","hash":"afbce1ad2cbc988ae99ded453c349980077e09de","modified":1535687596775},{"_id":"source/_posts/恋爱领域中普遍存在的贬低倾向.md","hash":"fa3c3e6eaa81ff2235909606a34ad5588f6874ab","modified":1535687606675},{"_id":"source/_posts/数据划分.md","hash":"83654aaf4f99fcac17baa8c6e66d37e792ffbae3","modified":1535687627798},{"_id":"source/_posts/数据的加载-预处理-可视化.md","hash":"439ff19283a8609dbcf47360d3cb02f425f40cdd","modified":1535687642897},{"_id":"source/_posts/标准化输入.md","hash":"9d482d1a4842e31ebeb7b919203eac7ff0de3d9d","modified":1535687652437},{"_id":"source/_posts/梯度检验.md","hash":"aa5e2f7a95c3a877a6bbc5253c7e6ce0f2166d72","modified":1535687665694},{"_id":"source/_posts/梯度消失和梯度爆炸.md","hash":"7be3d8c157b23cf72a7f1a4ed88a68127662bfc5","modified":1535687676141},{"_id":"source/_posts/模型估计.md","hash":"ec16ec21c42690d5968df84bddb38f7b28ec8227","modified":1535687684876},{"_id":"source/_posts/正则化.md","hash":"260fbe654114812d2997c0bfcac8176bcd5c5d3e","modified":1535687689461},{"_id":"source/_posts/深度卷积神经网络-实例探究.md","hash":"2969f6cc6248d108ccb75a2ba125f812cf5b8540","modified":1535276918084},{"_id":"source/_posts/深度学习中的优化算法.md","hash":"f16e813ad88a268779cc2010e4f1be080e326aa4","modified":1534643975807},{"_id":"source/_posts/男人的对象选择中的一种特殊类型.md","hash":"0c97240879e4daa25f626715bd7fe8e4c25764f1","modified":1535687701946},{"_id":"source/_posts/短诗三首.md","hash":"68fdc78b87c059c5298a688f39c513313fd98385","modified":1535687708779},{"_id":"source/_posts/神经网络中的通用函数代码.md","hash":"32f35f0e124c47947232cc4a1f6efa54c4299111","modified":1535687723326},{"_id":"source/_posts/贞洁禁忌.md","hash":"4f13f4b09d5ec839d729dcbab2b3d2f1dcbd9231","modified":1535687727511},{"_id":"source/categories/index.md","hash":"a8ee8e11f9d453a17eb3467620d7b43ff97856b3","modified":1534643975811},{"_id":"source/tags/index.md","hash":"a75687420abbf3ff738d0fea7fe2634cbd4340e9","modified":1534643975834},{"_id":"themes/next/_config.yml","hash":"70a30c95b33b0833feebf579410f2043e1a8fec4","modified":1534643975942},{"_id":"themes/next/package.json","hash":"3963ad558a24c78a3fd4ef23cf5f73f421854627","modified":1534643976022},{"_id":"source/images/Convolutions-on-RGB-image.png","hash":"b4de22ca607e562f21ec385e5dd14d8b11a93914","modified":1531227954186},{"_id":"source/images/Stride.jpg","hash":"56c57dbb8d74628ac798fa3c23fa1ec969146576","modified":1531227954214},{"_id":"source/images/adam.PNG","hash":"f2adc403d4012046172c37175835fefb8bec932f","modified":1534643975815},{"_id":"source/images/Padding.jpg","hash":"e53f0396b1e806eeb94effbd33ed1d0ea1977e12","modified":1531227954202},{"_id":"source/images/alexnet_ilsvrc.PNG","hash":"9f57a90d68124ad1290137cf824e9f0485f33bf7","modified":1535439784780},{"_id":"source/images/res1.PNG","hash":"f528eca6822e71539139aa1b2795847135dc2b07","modified":1534643975825},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1534643975930},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"a0a82dbfabdef9a9d7c17a08ceebfb4052d98d81","modified":1534643975931},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1534643975932},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1534643975946},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1534643975945},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1534643975947},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1534643975948},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1534643975950},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1534643975948},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1534643975951},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1534643975950},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1534643975952},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1534643975953},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1534643975954},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1534643975953},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1534643975933},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1534643975955},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1534643975957},{"_id":"themes/next/languages/zh-Hans.yml","hash":"66b9b42f143c3cb2f782a94abd4c4cbd5fd7f55f","modified":1534643975956},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1534643975958},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1534643976017},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1534643975959},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1534643976016},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1534643976018},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1534643976019},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1534643976019},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1534643976023},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1534643976024},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1534643976274},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1534643976275},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1534643976021},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1534643976020},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1534643976276},{"_id":"source/images/eas.png","hash":"9d12e21703b5d1dab464bd5f8f0d2f194145b43c","modified":1534643975817},{"_id":"source/images/partition.png","hash":"1c29a499253b64f0834f8253ea85efcb716872ac","modified":1534643975824},{"_id":"source/images/sgd.png","hash":"43fac22e9aa802e1fae6e25d791d23d3f82c616f","modified":1534643975826},{"_id":"source/images/minibatch.png","hash":"1705433ef878be5688ab474c7f3461c94b224dde","modified":1534643975818},{"_id":"source/images/my_image.jpg","hash":"931bed91b176c20eee98be0a6f0af356d37b2330","modified":1534643975822},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643976163},{"_id":"source/images/alexnet_architecture.PNG","hash":"b4f692d1b6e69eacbb6e21d20a1a8cb878adf2d7","modified":1535424925483},{"_id":"source/images/momentum.png","hash":"97d12c75f9b2ecd40f0a6d0d47a21f83672ede97","modified":1534643975820},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1534643975958},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1534643975959},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1534643975962},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1534643975963},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1534643975965},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1534643975961},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1534643975960},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1534643975966},{"_id":"themes/next/layout/_partials/footer.swig","hash":"26e93336dc57a39590ba8dc80564a1d2ad5ff93b","modified":1534643975967},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9c7343fd470e0943ebd75f227a083a980816290b","modified":1534643975964},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1534643975971},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1534643975970},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1534643975974},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1534643975972},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1534643975982},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1534643975968},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1534643976006},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1534643975981},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1534643976005},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1534643976008},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1534643975985},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1534643976007},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1534643976010},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1534643976026},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1534643976025},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1534643976026},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1534643976027},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1534643976029},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1534643976028},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1534643976030},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1534643976030},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1534643976031},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1534643976008},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1534643976163},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1534643976164},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1534643976165},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1534643976009},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1534643976166},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1534643976168},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1534643976169},{"_id":"themes/next/source/images/avatar.jpeg","hash":"f1aa09fc418f5d6f05af58e81a9ca95790a075b0","modified":1534643976167},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1534643976169},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1534643976171},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1534643976173},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1534643976170},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1534643976172},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1534643976174},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1534643976174},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1534643976171},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1534643976175},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1534643976176},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1534643976177},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1534643976176},{"_id":"source/images/shuffle.png","hash":"e844a5da26c91ccd40f9c5186a7fd5db7fe4f2b7","modified":1534643975829},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643975983},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643975983},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1534643976178},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643976108},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643976107},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643976111},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643976160},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1534643976162},{"_id":"source/images/LlayerNN.png","hash":"939d9808c2d757e7097f22cbb4b4083c40f3da9c","modified":1534643975814},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1534643975968},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1534643975975},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1534643975976},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1534643975977},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1534643975976},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1534643975969},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1534643975979},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1534643975983},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1534643975980},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1534643975982},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1534643975984},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1534643975998},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1534643975978},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1534643976000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1534643976001},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1534643976001},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1534643975998},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1534643975997},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1534643976004},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1534643976002},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1534643975987},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1534643976003},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1534643975989},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1534643975990},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1534643975987},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1534643975991},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1534643975988},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1534643975993},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1534643975992},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1534643975992},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1534643975996},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1534643975995},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1534643975994},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1534643975994},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1534643976013},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1534643976016},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1534643976107},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1534643976015},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1534643976013},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1534643976105},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1534643976109},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1534643976160},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1534643976110},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1534643976180},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1534643976159},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1534643976162},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1534643976178},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1534643976179},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1534643976180},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1534643976181},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1534643976182},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1534643976183},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1534643976182},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1534643976161},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1534643976185},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1534643976185},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1534643976186},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1534643976196},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1534643976201},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1534643976215},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1534643976215},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1534643976217},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1534643976220},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1534643976216},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1534643976221},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1534643976202},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1534643976222},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1534643976223},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1534643976241},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1534643976242},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1534643976242},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1534643976219},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1534643976239},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1534643976244},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1534643976245},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1534643976247},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1534643976246},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1534643976249},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1534643976250},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1534643976248},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1534643976251},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1534643976249},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1534643976251},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1534643976252},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1534643976253},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1534643976254},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1534643976253},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1534643976255},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1534643976255},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1534643976257},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1534643976257},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1534643976256},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1534643976258},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1534643976243},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1534643976259},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1534643976260},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1534643976268},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1534643976268},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1534643976272},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1534643976240},{"_id":"source/images/动态规划.jpg","hash":"771e3ceb5404dcb7ef349c6aa6e7853a7d61934e","modified":1534643975833},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1534643976274},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1534643976273},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1534643976012},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1534643976011},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1534643976032},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1534643976036},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1534643976034},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1534643976033},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1534643976035},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1534643976078},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1534643976051},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1534643976098},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1534643976098},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1534643976100},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1534643976102},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1534643976097},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1534643976103},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1534643976100},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1534643976119},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1534643976114},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1534643976124},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1534643976117},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1534643976121},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1534643976154},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1534643976123},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1534643976155},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1534643976125},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1534643976157},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"ad2dcedf393ed1f3f5afd2508d24969c916d02fc","modified":1534643976157},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1534643976158},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1534643976155},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1534643976130},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1534643976132},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1534643976151},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e695e58f714129ca292c2e54cd62c251aca7f7fe","modified":1534643976133},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1534643976152},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1534643976112},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1534643976192},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1534643976184},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1534643976203},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1534643976194},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1534643976195},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1534643976204},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1534643976204},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1534643976206},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1534643976205},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1534643976207},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1534643976212},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1534643976214},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1534643976213},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1534643976218},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1534643976224},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1534643976225},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1534643976219},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1534643976226},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1534643976267},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1534643976193},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1534643976271},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1534643976237},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1534643976238},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1534643976038},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1534643976040},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1534643976037},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1534643976266},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1534643976043},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1534643976044},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1534643976045},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1534643976042},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1534643976046},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1534643976047},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1534643976047},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1534643976041},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1534643976049},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1534643976048},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1534643976053},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1534643976050},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1534643976057},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1534643976056},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1534643976054},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1534643976058},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1534643976060},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1534643976061},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1534643976063},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1534643976065},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1534643976064},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1534643976063},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1534643976062},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1534643976066},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1534643976069},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1534643976068},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1534643976067},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1534643976071},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1534643976072},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1534643976070},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1534643976073},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1534643976075},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1534643976074},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1534643976075},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1534643976076},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1534643976077},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1534643976080},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1534643976081},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1534643976079},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1534643976083},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1534643976082},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1534643976084},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1534643976085},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1534643976086},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1534643976088},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1534643976089},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1534643976085},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1534643976091},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1534643976092},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1534643976087},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1534643976093},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1534643976095},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1534643976090},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1534643976096},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1534643976128},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1534643976127},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1534643976094},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1534643976153},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1534643976187},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1534643976188},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1534643976189},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1534643976190},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1534643976208},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1534643976191},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1534643976208},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1534643976211},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1534643976209},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1534643976210},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1534643976228},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1534643976231},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1534643976200},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1534643976236},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1534643976211},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1534643976265},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1534643976234},{"_id":"source/images/dl_pic9_2.jpg","hash":"1de175528c89642a2520d640861ff5237da9aefc","modified":1535080882042},{"_id":"source/images/dl_pic9_8.jpg","hash":"7c99fba6503ff5a23a3d8e39278c5dc2bdd3b9ee","modified":1535082369512},{"_id":"source/images/dl_pic9_5.jpg","hash":"b1ccd9c91f99bf4bb2d0804901a11aa840283c5b","modified":1535081541240},{"_id":"source/images/dl_pic9_7.jpg","hash":"3d8b3e0b5dc8a8a76fb67cd86f23d125c896809f","modified":1535082325324},{"_id":"public/categories/index.html","hash":"33dcef5489a1f1466a53a600fc0a8c28daa758fd","modified":1535687876177},{"_id":"public/tags/index.html","hash":"40ef88f722cd295aa569c54d45b04e7384f88ec9","modified":1535687876193},{"_id":"public/2018/08/28/ImageNet-Classification-wih-Deep-Convolutional-Neural-Network/index.html","hash":"9b09ea63045c6070d906165baa0f82236f48bbb2","modified":1535687876193},{"_id":"public/2018/08/26/深度卷积神经网络-实例探究/index.html","hash":"5cf2a6bc75be127984308a7b2943665cc99eaec8","modified":1535687876194},{"_id":"public/2018/08/26/卷积神经网络/index.html","hash":"8a17579507d709f179cf6d671565a2315585abb5","modified":1535687876194},{"_id":"public/2018/08/19/hello-world/index.html","hash":"88a04c718d047b3104b550a5d736689bb376ac51","modified":1535687876194},{"_id":"public/2018/08/16/matplotlib/index.html","hash":"303da7e86dff4bf3401b882a52e6f2c0bad44002","modified":1535687876194},{"_id":"public/2018/08/15/BeautifulSoup/index.html","hash":"8879a6c520ffa5d89ec04e761d7d207ddae19ffb","modified":1535687876194},{"_id":"public/2018/08/14/requests/index.html","hash":"c3861bda2fb36783fb696d4b455bb1c1eb1ccf24","modified":1535687876194},{"_id":"public/2018/08/07/Gradient-Descent-Famliy/index.html","hash":"f2b4c76827527ef8acc9d6611dda9267ef68389c","modified":1535687876194},{"_id":"public/2018/08/05/Genetic-Algorithms/index.html","hash":"59e69cc09d1098fc92f39b238f0feba3ba78d277","modified":1535687876194},{"_id":"public/2018/08/05/布雷默曼极限/index.html","hash":"6b45eed6faa9cbb7c65dc6c0f1d020961e0d8057","modified":1535687876194},{"_id":"public/2018/08/05/Evolutionary-Algorithms/index.html","hash":"856a91c77006a82cc50e54f201150791d4376c0f","modified":1535687876195},{"_id":"public/2018/08/05/github使用手册/index.html","hash":"5b5833125e02d2607196df6cfc388d8075ef4e41","modified":1535687876195},{"_id":"public/2018/08/05/python内置小工具/index.html","hash":"84238e053fd9b588dd7bccf782c4b72066206979","modified":1535687876195},{"_id":"public/2018/08/04/深度学习中的优化算法/index.html","hash":"31349f771bdd3ca5b894e71940f8e993db9f49e2","modified":1535687876195},{"_id":"public/2018/08/03/DNN应用1-识别猫/index.html","hash":"ee181a3619df9e5b4a26f51d3d289f187e6f1d09","modified":1535687876195},{"_id":"public/2018/07/22/初始化参数/index.html","hash":"b4b257a87138527570cceb2257a013f0e9b66794","modified":1535687876195},{"_id":"public/2018/07/21/动态规划/index.html","hash":"30a960b5a3bda1510d4fdf0dae86482a396be90d","modified":1535687876195},{"_id":"public/2018/07/21/数据的加载-预处理-可视化/index.html","hash":"cc6078712d65af1a0c91ce3bffebdeaf269ae4c8","modified":1535687876195},{"_id":"public/2018/07/21/神经网络中的通用函数代码/index.html","hash":"e12d21708140e4593e8c59066bc832b212b220ce","modified":1535687876195},{"_id":"public/2018/07/21/贞洁禁忌/index.html","hash":"5a0ad23d8d6c4f19e3e2f7b960031dce6897a090","modified":1535687876196},{"_id":"public/2018/07/20/梯度检验/index.html","hash":"cc89943fb6f3ca1ad03d0de00e53ebd32d82fff2","modified":1535687876196},{"_id":"public/2018/07/20/标准化输入/index.html","hash":"af0a175aad2f81766cc11ec671d18b55e9b73934","modified":1535687876196},{"_id":"public/2018/07/20/梯度消失和梯度爆炸/index.html","hash":"37f4919afdd6b6ffe91ee02b86179d9fce6bd6da","modified":1535687876196},{"_id":"public/2018/07/20/dropout/index.html","hash":"2cc9e07c62ebea0c989d757fbc2f5064f2db31c2","modified":1535687876196},{"_id":"public/2018/07/20/正则化/index.html","hash":"5edeaf294ea800c620f018e505b5fdf803bb6343","modified":1535687876196},{"_id":"public/2018/07/20/模型估计/index.html","hash":"822f7f08fbbc929fade1369371ce42bf56a8e55e","modified":1535687876196},{"_id":"public/2018/07/20/数据划分/index.html","hash":"51ba83e1b75b60edf8f917c90b206d819efd850e","modified":1535687876196},{"_id":"public/2018/07/20/十个策略故事/index.html","hash":"97a4e7df1be559b8ea1a02dd6e43ec8127280ac3","modified":1535687876197},{"_id":"public/2018/07/19/人生-路遥/index.html","hash":"ecffa351dcbfc593a6bda6947f8ad73ae15f83ee","modified":1535687876197},{"_id":"public/2018/07/19/恋爱领域中普遍存在的贬低倾向/index.html","hash":"b57a64f599b536152f993a058ca2040277d22b2b","modified":1535687876197},{"_id":"public/2018/07/19/男人的对象选择中的一种特殊类型/index.html","hash":"f949197e46d39594d1bba3f856c9581f0b4fdc9a","modified":1535687876197},{"_id":"public/2018/07/19/短诗三首/index.html","hash":"6ff0f44b98bedeb88e8935037ed53c884280405e","modified":1535687876197},{"_id":"public/2018/07/18/How-to-set-up-a-blog-with-hexo-on-github-io/index.html","hash":"310dea60e7e942571940655b9befbc6542a2144b","modified":1535687876197},{"_id":"public/categories/python包和模块/index.html","hash":"f41ca2af9c0fa5171b355add87c58e4f549beeeb","modified":1535687876863},{"_id":"public/categories/进化计算/index.html","hash":"79d64b48d6e9e080fa16cbcacf5a2ae78fd4046d","modified":1535687876865},{"_id":"public/categories/web/index.html","hash":"e95a8f9f1b31e2675380ace8f0847ac855601ac2","modified":1535687876865},{"_id":"public/categories/程序员实用工具/index.html","hash":"d079e1110bb1b87edd8270e4cafc81b800d8f2c1","modified":1535687876865},{"_id":"public/categories/文学/index.html","hash":"9a621f1cdc62b8cdcf771b526439668e58e83c9e","modified":1535687876865},{"_id":"public/categories/算法导论/index.html","hash":"dec197daf1160096496428f2a1c2ab0e7f18658a","modified":1535687876865},{"_id":"public/categories/计算机科学/index.html","hash":"1e42f50c41d6fa042d4db760f817907f0e3a8ead","modified":1535687876865},{"_id":"public/tags/进化算法/index.html","hash":"5c45eb8c5840e4a4e33e77a08f474849cd1dffe5","modified":1535687876866},{"_id":"public/tags/遗传算法/index.html","hash":"226d24b0df42abc9058fa014bf18144ba768b98f","modified":1535687876866},{"_id":"public/tags/优化算法/index.html","hash":"5ee86956d5c7f646efa04a9769f961f3ecb05178","modified":1535687876866},{"_id":"public/tags/hexo/index.html","hash":"02d180cba91d7bff5530f753b379e80b4e4be6f0","modified":1535687876866},{"_id":"public/tags/CNN/index.html","hash":"3ecb4b76fcae9d03f2bcd8dccf0d8d863e350ccb","modified":1535687876866},{"_id":"public/tags/dropout/index.html","hash":"9f1126f1a1d753ce8d0fb2dee6ee4d7058afe2b7","modified":1535687876866},{"_id":"public/tags/git/index.html","hash":"54fa470a71f4fea63b81b4f67460fce82bb27cd9","modified":1535687876866},{"_id":"public/tags/动态规划/index.html","hash":"de3d9506b0ad46564ad367f6df29dda96d5fdbea","modified":1535687876866},{"_id":"public/tags/策略游戏/index.html","hash":"b58fcc874bce82148f304af39833c3198267039f","modified":1535687876867},{"_id":"public/tags/爱情心理学/index.html","hash":"84b98d0f5460d6a031dadb3921a35b7934336c63","modified":1535687876867},{"_id":"public/tags/模型估计/index.html","hash":"6c01755cc8aeb34cc8f89c023ac9b7574f6735bd","modified":1535687876867},{"_id":"public/tags/正则化/index.html","hash":"452d4af1436acfd264774d560618da7e5307811c","modified":1535687876867},{"_id":"public/tags/现代诗/index.html","hash":"b646e99a4b61ae229f6ea5aba55e0b9042101802","modified":1535687876867},{"_id":"public/tags/神经网络/index.html","hash":"a4953591101e1117acc7e535455f78bc03a2e0bc","modified":1535687876867},{"_id":"public/archives/page/4/index.html","hash":"e23cc0616bfb1dc5467b32c42d87f4f2665dc9dc","modified":1535687876868},{"_id":"public/archives/2018/page/4/index.html","hash":"fcdf2a756e9e36121d3829a34947b44d9ea90972","modified":1535687876868},{"_id":"public/archives/2018/08/page/2/index.html","hash":"afed45c4374b59ea83e6b87a3bba3ace14080461","modified":1535687876868},{"_id":"public/2018/08/31/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recongnition/index.html","hash":"b56892b2c61c78530fc96a557aee40258b44c021","modified":1535687876868},{"_id":"public/categories/深度学习/index.html","hash":"967d4bb2abaf76ac452c996c1b4bdd6f2b04e677","modified":1535687876868},{"_id":"public/index.html","hash":"081cf9b92e523eae333b9dfd1966748f1bf3a037","modified":1535687876868},{"_id":"public/page/2/index.html","hash":"859e24675f251e4782a3b6135aafd9e9c6e8f71c","modified":1535687876868},{"_id":"public/page/3/index.html","hash":"5225037c3397a0dd8b141d9b4a0f325d3332a488","modified":1535687876868},{"_id":"public/page/4/index.html","hash":"c2e502c21082cebfb7153afa7c33f8028a5c0cad","modified":1535687876869},{"_id":"public/archives/index.html","hash":"e874aee2ff5532de16989faf8838da4837948d8e","modified":1535687876869},{"_id":"public/archives/page/2/index.html","hash":"dbdfcf3f10e2d9b131ac63e39432fee842885719","modified":1535687876869},{"_id":"public/archives/page/3/index.html","hash":"53eae37aac74450749146f9b55fc271d14ab7f9c","modified":1535687876869},{"_id":"public/archives/2018/index.html","hash":"d01e293996eb879c5603c545896cb80f3648f06c","modified":1535687876869},{"_id":"public/archives/2018/page/2/index.html","hash":"c06ec3afc1f66332f77c8dcd4661a8dc6edb27e3","modified":1535687876870},{"_id":"public/archives/2018/page/3/index.html","hash":"cdebe927294e1dc787b5ddb47ee460bc03e8b113","modified":1535687876870},{"_id":"public/archives/2018/07/index.html","hash":"4af3f15b554945eb63f8b778b2d2f247bbf019b3","modified":1535687876870},{"_id":"public/archives/2018/07/page/2/index.html","hash":"cbe931de4036ff4b83ed9943e72561446d1a3267","modified":1535687876870},{"_id":"public/archives/2018/08/index.html","hash":"b4b5dd584e91cbb8d4681736689b67858d6f9970","modified":1535687876870},{"_id":"public/categories/深度学习/page/2/index.html","hash":"e22cafc7bf75913c5253212b9d4afb6aa2167521","modified":1535687876887},{"_id":"public/categories/心理学/index.html","hash":"54eba4b260bfb528edbc079409f73a976650c111","modified":1535687876887},{"_id":"public/tags/python/index.html","hash":"420b9e79e174948adeeaf0adc5df791e7bf871c8","modified":1535687876887},{"_id":"public/tags/DNN/index.html","hash":"ef5bdd10176790e6798c6fc1a0c50303a8d29403","modified":1535687876887},{"_id":"public/tags/CNN-VGG/index.html","hash":"55fcf6c71329d10889fb4b6226468f1fd4eb2fb5","modified":1535687876887},{"_id":"public/tags/路遥/index.html","hash":"bfbdf7c6eb3b86e8a9f38ecbc28f385e8f2b84db","modified":1535687876887},{"_id":"public/tags/物理/index.html","hash":"3c73e5af7bc1e3cfa45d2fb9e43f5c5aff97687c","modified":1535687876888},{"_id":"public/tags/数据/index.html","hash":"43e984fc89e5c2f1d0ffb03aeb5ab6a900b56fb0","modified":1535687876888},{"_id":"public/images/Stride.jpg","hash":"56c57dbb8d74628ac798fa3c23fa1ec969146576","modified":1535687876888},{"_id":"public/images/Convolutions-on-RGB-image.png","hash":"b4de22ca607e562f21ec385e5dd14d8b11a93914","modified":1535687876888},{"_id":"public/images/adam.PNG","hash":"f2adc403d4012046172c37175835fefb8bec932f","modified":1535687876888},{"_id":"public/images/Padding.jpg","hash":"e53f0396b1e806eeb94effbd33ed1d0ea1977e12","modified":1535687876888},{"_id":"public/images/alexnet_ilsvrc.PNG","hash":"9f57a90d68124ad1290137cf824e9f0485f33bf7","modified":1535687876888},{"_id":"public/images/res1.PNG","hash":"f528eca6822e71539139aa1b2795847135dc2b07","modified":1535687876888},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1535687876889},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1535687876889},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1535687876889},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1535687876889},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1535687876889},{"_id":"public/images/avatar.jpeg","hash":"f1aa09fc418f5d6f05af58e81a9ca95790a075b0","modified":1535687876889},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1535687876889},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1535687876889},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1535687876889},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1535687876889},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1535687876889},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1535687876889},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1535687876889},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1535687876889},{"_id":"public/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1535687876890},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1535687876890},{"_id":"public/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1535687876890},{"_id":"public/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1535687876890},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1535687876890},{"_id":"public/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1535687876890},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1535687876890},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1535687876890},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1535687876890},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1535687876890},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1535687876891},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1535687876891},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1535687876891},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1535687876891},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1535687876891},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1535687876891},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1535687876891},{"_id":"public/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1535687876891},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1535687883080},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1535687883082},{"_id":"public/images/eas.png","hash":"9d12e21703b5d1dab464bd5f8f0d2f194145b43c","modified":1535687883101},{"_id":"public/images/sgd.png","hash":"43fac22e9aa802e1fae6e25d791d23d3f82c616f","modified":1535687883101},{"_id":"public/images/partition.png","hash":"1c29a499253b64f0834f8253ea85efcb716872ac","modified":1535687883101},{"_id":"public/images/minibatch.png","hash":"1705433ef878be5688ab474c7f3461c94b224dde","modified":1535687883102},{"_id":"public/images/my_image.jpg","hash":"931bed91b176c20eee98be0a6f0af356d37b2330","modified":1535687883102},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1535687883102},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1535687883102},{"_id":"public/js/src/bootstrap.js","hash":"034bc8113e0966fe2096ba5b56061bbf10ef0512","modified":1535687883109},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1535687883109},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1535687883109},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1535687883109},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1535687883110},{"_id":"public/js/src/motion.js","hash":"754b294394f102c8fd9423a1789ddb1201677898","modified":1535687883110},{"_id":"public/js/src/post-details.js","hash":"a13f45f7aa8291cf7244ec5ba93907d119c5dbdd","modified":1535687883110},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1535687883110},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1535687883110},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1535687883110},{"_id":"public/js/src/utils.js","hash":"9b1325801d27213083d1487a12b1a62b539ab6f8","modified":1535687883110},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1535687883110},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1535687883111},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1535687883111},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1535687883111},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"ff5915eb2596e890a2fc6697c864f861a1995ec0","modified":1535687883111},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1535687883111},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1535687883111},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1535687883111},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1535687883111},{"_id":"public/lib/needsharebutton/needsharebutton.css","hash":"3ef0020a1815ca6151ea4886cd0d37421ae3695c","modified":1535687883111},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1535687883111},{"_id":"public/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1535687883112},{"_id":"public/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1535687883113},{"_id":"public/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1535687883113},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1535687883113},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1535687883113},{"_id":"public/lib/pace/pace.min.js","hash":"9944dfb7814b911090e96446cea4d36e2b487234","modified":1535687883113},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1535687883113},{"_id":"public/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1535687883113},{"_id":"public/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1535687883113},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1535687883114},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1535687883114},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1535687883114},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1535687883114},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1535687883114},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1535687883114},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1535687883114},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1535687883114},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1535687883115},{"_id":"public/css/main.css","hash":"13bd323d0aa4b32c19f28ef3fc3631378d290dbb","modified":1535687883115},{"_id":"public/lib/needsharebutton/needsharebutton.js","hash":"9885fd9bea5e7ebafc5b1de9d17be5e106248d96","modified":1535687883115},{"_id":"public/lib/needsharebutton/font-embedded.css","hash":"c39d37278c1e178838732af21bd26cd0baeddfe0","modified":1535687883115},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1535687883115},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1535687883115},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1535687883115},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1535687883115},{"_id":"public/lib/Han/dist/han.css","hash":"bd40da3fba8735df5850956814e312bd7b3193d7","modified":1535687883115},{"_id":"public/lib/Han/dist/han.min.css","hash":"a0c9e32549a8b8cf327ab9227b037f323cdb60ee","modified":1535687883115},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1535687883115},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1535687883116},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1535687883116},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1535687883116},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1535687883116},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1535687883116},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1535687883116},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1535687883116},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1535687883116},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1535687883116},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1535687883117},{"_id":"public/images/alexnet_architecture.PNG","hash":"b4f692d1b6e69eacbb6e21d20a1a8cb878adf2d7","modified":1535687883165},{"_id":"public/images/momentum.png","hash":"97d12c75f9b2ecd40f0a6d0d47a21f83672ede97","modified":1535687883165},{"_id":"public/images/shuffle.png","hash":"e844a5da26c91ccd40f9c5186a7fd5db7fe4f2b7","modified":1535687883166},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1535687883166},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1535687883167},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1535687883167},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1535687883178},{"_id":"public/images/LlayerNN.png","hash":"939d9808c2d757e7097f22cbb4b4083c40f3da9c","modified":1535687883637},{"_id":"public/images/动态规划.jpg","hash":"771e3ceb5404dcb7ef349c6aa6e7853a7d61934e","modified":1535687883720},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1535687884152},{"_id":"public/images/dl_pic9_2.jpg","hash":"1de175528c89642a2520d640861ff5237da9aefc","modified":1535687884367},{"_id":"public/images/dl_pic9_8.jpg","hash":"7c99fba6503ff5a23a3d8e39278c5dc2bdd3b9ee","modified":1535687884370},{"_id":"public/images/dl_pic9_5.jpg","hash":"b1ccd9c91f99bf4bb2d0804901a11aa840283c5b","modified":1535687884370},{"_id":"public/images/dl_pic9_7.jpg","hash":"3d8b3e0b5dc8a8a76fb67cd86f23d125c896809f","modified":1535687884382}],"Category":[{"name":"python包和模块","_id":"cjlhgwh1e0004zkvo52fipkqj"},{"name":"深度学习","_id":"cjlhgwh1v0009zkvoffdyomm8"},{"name":"进化计算","_id":"cjlhgwh2a000fzkvoeie1esf2"},{"name":"web","_id":"cjlhgwh3k000zzkvopenw2xna"},{"name":"程序员实用工具","_id":"cjlhgwh3w0018zkvoo8nvly4q"},{"name":"文学","_id":"cjlhgwh4p001xzkvoydm8tglv"},{"name":"算法导论","_id":"cjlhgwh4z0024zkvos6tyxq4w"},{"name":"计算机科学","_id":"cjlhgwh5a002bzkvo5cg8wr05"},{"name":"心理学","_id":"cjlhgwh5i002kzkvoj73roypo"}],"Data":[],"Page":[{"title":"categories","date":"2018-07-18T16:23:02.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-07-19 00:23:02\ntype: \"categories\"\n---\n","updated":"2018-08-19T01:59:35.811Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjlhgwh120001zkvocllcsb0c","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-07-18T16:21:54.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-07-19 00:21:54\ntype: \"tags\"\n---\n","updated":"2018-08-19T01:59:35.834Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjlhgwh190003zkvo014gcmaq","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"BeautifulSoup","date":"2018-08-15T03:40:51.000Z","_content":"## [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)\n\nYou didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects.\n\n## 如何使用\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(open('index.html'))\n```\n\n## 对象的种类\n\n### Tag\n\nTag 对象与XML或HTML原生文档中的tag相同:\n\n```python\nsoup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\ntag = soup.b\ntype(tag)\n# <class 'bs4.element.Tag'>\n```\n\nTag的属性：`tag.name, tag.attrs`\n\ntag的属性的操作方法与字典相同: `tag['class']`\n\n### 可以遍历的字符串\n\n字符串常被包含在tag内.Beautiful Soup用 NavigableString 类来包装tag中的字符串:`tag.string`\n\n`unicode_string = unicode(tag.string)`\n\ntag中包含的字符串不能编辑,但是可以被替换成其它的字符串,用 replace_with() 方法:`tag.string.replace_with(\"No\")`\n\n### BeautifulSoup\n\nBeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.\n\n### 注释及特殊字符串\n\nComment 对象是一个特殊类型的 NavigableString 对象:\n\n```python\nmarkup = \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\"\nsoup = BeautifulSoup(markup)\ncomment = soup.b.string\ntype(comment)\n# <class 'bs4.element.Comment'>\n```\n\n## 遍历文档树\n\n### 子节点\n\n一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.\n\n#### tag的名字\n\n`soup.head, soup.title, soup.body.b, soup.a, soup.find_all('a')`\n\n#### .contents和.children\n\ntag的 .contents 属性可以将tag的子节点以列表的方式输出:\n\n通过tag的 .children 生成器,可以对tag的子节点进行循环:\n\n```python\nfor child in title_tag.children:\n    print(child)\n```\ndescendants 属性可以对所有tag的子孙节点进行递归循环:\n\n```python\nfor child in title_tag.descendants:\n    print(child)\n```\n\n### .string\n\n如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点:`tag.string`\n\n如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,输出结果与当前唯一子节点的 .string 结果相同\n\n如果tag包含了多个子节点,tag就无法确定 .string 方法应该调用哪个子节点的内容, .string 的输出结果是 None :\n\n### .strings和stripped__strings\n\n如果tag中包含多个字符串 ,可以使用 .strings 来循环获取, 输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容\n\n```python\nfor string in soup.strings:\n    print(repr(string))\n\nfor string in soup.stripped_strings:\n    ...\n```\n\n### 父节点\n\n#### .parent\n\n`tag.parent, tag.string.parent`\n\n#### .parents\n\n```python\nlink = soup.a\nfor parent in link.parents:\n    if parent is None:\n        print(parent)\n    else:\n        print(parent.name)\n```\n\n### 兄弟节点\n\n#### .next_sibling和.previous_sibling\n\n实际文档中的tag的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白.\n\n#### .next_siblings和.previous_siblings\n\n通过 .next_siblings 和 .previous_siblings 属性可以对当前节点的兄弟节点迭代输出\n\n```python\nfor sibling in soup.a.next_siblings:\n    print(repr(sibling))\n```\n\n### 回退和前进\n\n```html\n<html><head><title>The Dormouse's story</title></head>\n<p class=\"title\"><b>The Dormouse's story</b></p>\n```\n\nHTML解析器把这段字符串转换成一连串的事件: “打开html标签”,”打开一个head标签”,”打开一个title标签”,”添加一段字符串”,”关闭title标签”,”关闭p标签”,等等.Beautiful Soup提供了重现解析器初始化过程的方法\n\n#### .next_element和.previous_element\n\n.previous_element 属性刚好与 .next_element 相反,它指向当前被解析的对象的前一个解析对象, next_element 属性指向解析过程中下一个被解析的对象(字符串或tag),\n\n#### .next_elements和.previous_elements\n\n通过 .next_elements 和 .previous_elements 的迭代器就可以向前或向后访问文档的解析内容,就好像文档正在被解析一样:\n\n```python\nfor elementin a_tag.next_elements:\n    print(repr(element))\n```\n\n## 搜索文档树\n\n### 过滤器\n\n#### 字符串\n\n最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的\\<b>标签:\n\n`soup.find_all('b')`\n\n#### 正则表达式\n\n如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示\\<body>和\\<b>标签都应该被找到:\n\n```python\nimport re\nfor tag in soup.find_all(re.compile(\"^b\"):\n    print(tag.name)\n```\n\n#### 列表\n\n如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有\\<a>标签和\\<b>标签:\n\n`soup.find_all(['a', 'b'])`\n\n#### True\n\nTrue 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点\n\n```python\nfor tag in soup.find_all(True):\n    print(tag.name)\n```\n\n#### 方法\n\n如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 False\n\n下面方法校验了当前元素,如果包含 class 属性却不包含 id 属性,那么将返回 True:\n\n```python\ndef has_class_but_no_id(tag):\n    return tag.has_attr('class') and not tag.has_attr('id')\n```\n\n下面代码找到所有被文字包含的节点内容\n\n```python\nfrom bs4 import NavigableString\n\ndef surrounded_by_strings(tag):\n    return (isinstance(tag.next_element, NavigableString)) and isinstance(tag.previous_element, NavigableString)\n```\n\n### find_all()\n\n`find_all(name, attrs, recursive, text, **kwargs)`\n\nfind_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件\n\n#### name参数\n\n搜索 name 参数的值可以使任一类型的 过滤器 ,字符窜,正则表达式,列表,方法或是 True .\n\nname 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉.\n\n#### keyword参数\n\n`soup.find_all(id='link2')`\n`soup.find_all(href=re.compile('elsie'))`\n`soup.find_all(id=True)`\n`soup.find_all(attrs={'id':'link2'})`\n\n#### 按CSS搜索\n\n通过 \".class__\"_ 参数搜索有指定CSS类名的tag, \".class__\"_ 参数同样接受不同类型的 过滤器 ,字符串,正则表达式,方法或 True\n\n`soup.find_all('a', class_='sister')`\n\n#### text参数\n\n通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, True.\n\n`soup.find_all(text=re.compile(\"Dormouse\"))`\n\n#### limit参数\n\nfind_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果.\n\n`soup.find_all('a', limit=2)`\n\n#### recursive参数\n\n调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False.\n\n`soup.find_all('title', recursive=False)`\n\n### 像调用find_all()一样调用tag\n\nfind_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. BeautifulSoup 对象和 tag 对象可以被当作一个方法来使用,这个方法的执行结果与调用这个对象的 find_all() 方法相同,下面两行代码是等价的:\n\n`soup.find_all('a')`\n`soup('a')`\n\n### find()\n\n`find(name, attrs, recursive, text, **kwargs)`\n\n```python\nsoup.find_all('title', limit=1)\n# [<title>The Dormouse's story</title>]\n\nsoup.find('title')\n# <title>The Dormouse's story</title>\n```\n\n唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.\n\nfind_all() 方法没有找到目标是返回空列表, find() 方法找不到目标时,返回 None.\n\n### find_parents()和find_parent()\n\nfind_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同\n\n### find_next_siblings()和find_next_sibling()\n\n这2个方法通过 .next_siblings 属性对当tag的所有后面解析的兄弟tag节点进行迭代, find_next_siblings() 方法返回所有符合条件的后面的兄弟节点, find_next_sibling() 只返回符合条件的后面的第一个tag节点.\n\n### find_previous_siblings和find_previous_sibling()\n\nfind_previous_siblings() 方法返回所有符合条件的前面的兄弟节点, find_previous_sibling() 方法返回第一个符合条件的前面的兄弟节点:\n\n### find_all_next()和find_next()\n\n这2个方法通过 .next_elements 属性对当前tag的之后的tag和字符串进行迭代, find_all_next() 方法返回所有符合条件的节点, find_next() 方法返回第一个符合条件的节点:\n\n### find_all_previous()和find_previous()\n\n这2个方法通过 .previous_elements 属性对当前节点前面的tag和字符串进行迭代, find_all_previous() 方法返回所有符合条件的节点, find_previous() 方法返回第一个符合条件的节点.\n\n### CSS选择器\n\n通过tag标签逐层查找 `soup.select(\"body a\")`\n\n找到某个tag标签的直接子标签 `soup.select(\"p > #link1\")`\n\n找到兄弟节点标签 `soup.select(\"#link1 + .sister\")`\n\n通过CSS类名查找 `soup.select(\".sister\"), soup.select(\"[class~=sister]\")`\n\n通过tag的id查找 `soup.select(\"#link1\")`\n\n通过是否存在某个属性查找 `soup.slect('a[href]')`\n\n通过属性的值来查找 `soup.select('a[href^=\"https:\"]', [href$='title'], [href*=\".com/\"]`\n\n## 修改文档树\n\n## 输出\n\n### 格式化输出\n\nprettify() 方法将Beautiful Soup的文档树格式化后以Unicode编码输出,每个XML/HTML标签都独占一行\n\n```python\nmarkup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\nsoup = BeautifulSoup(markup)\nsoup.prettify()\n# '<html>\\n <head>\\n </head>\\n <body>\\n  <a href=\"http://example.com/\">\\n...'\n\nprint(soup.prettify())\n# <html>\n#  <head>\n#  </head>\n#  <body>\n#   <a href=\"http://example.com/\">\n#    I linked to\n#    <i>\n#     example.com\n#    </i>\n#   </a>\n#  </body>\n# </html>\n```\n\n### 压缩输出\n\n如果只想得到结果字符串,不重视格式,那么可以对一个 BeautifulSoup 对象或 Tag 对象使用Python的 unicode() 或 str() 方法:\n\n`str(soup), Unicode(soup.a)`\n\n### 输出格式\n\nBeautiful Soup输出是会将HTML中的特殊字符转换成Unicode,比如“\\&lquot;”:\n如果将文档转换成字符串,Unicode编码会被编码成UTF-8.这样就无法正确显示HTML特殊字符了:\n\n### get_text()\n\n如果只想得到tag中包含的文本内容,那么可以用 get_text() 方法,这个方法获取到tag中包含的所有文本内容包括子孙tag中的内容,并将结果作为Unicode字符串返回:\n\n可以通过参数指定tag的文本内容的分隔符, 还可以去除前后空白: `soup.get_text('|', strip=True)`\n\n## 指定文档解析器\n\n## 编码\n","source":"_posts/BeautifulSoup.md","raw":"---\ntitle: BeautifulSoup\ndate: 2018-08-15 11:40:51\ntags: python\ncategories: python包和模块\n---\n## [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)\n\nYou didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects.\n\n## 如何使用\n\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(open('index.html'))\n```\n\n## 对象的种类\n\n### Tag\n\nTag 对象与XML或HTML原生文档中的tag相同:\n\n```python\nsoup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\ntag = soup.b\ntype(tag)\n# <class 'bs4.element.Tag'>\n```\n\nTag的属性：`tag.name, tag.attrs`\n\ntag的属性的操作方法与字典相同: `tag['class']`\n\n### 可以遍历的字符串\n\n字符串常被包含在tag内.Beautiful Soup用 NavigableString 类来包装tag中的字符串:`tag.string`\n\n`unicode_string = unicode(tag.string)`\n\ntag中包含的字符串不能编辑,但是可以被替换成其它的字符串,用 replace_with() 方法:`tag.string.replace_with(\"No\")`\n\n### BeautifulSoup\n\nBeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.\n\n### 注释及特殊字符串\n\nComment 对象是一个特殊类型的 NavigableString 对象:\n\n```python\nmarkup = \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\"\nsoup = BeautifulSoup(markup)\ncomment = soup.b.string\ntype(comment)\n# <class 'bs4.element.Comment'>\n```\n\n## 遍历文档树\n\n### 子节点\n\n一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.\n\n#### tag的名字\n\n`soup.head, soup.title, soup.body.b, soup.a, soup.find_all('a')`\n\n#### .contents和.children\n\ntag的 .contents 属性可以将tag的子节点以列表的方式输出:\n\n通过tag的 .children 生成器,可以对tag的子节点进行循环:\n\n```python\nfor child in title_tag.children:\n    print(child)\n```\ndescendants 属性可以对所有tag的子孙节点进行递归循环:\n\n```python\nfor child in title_tag.descendants:\n    print(child)\n```\n\n### .string\n\n如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点:`tag.string`\n\n如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,输出结果与当前唯一子节点的 .string 结果相同\n\n如果tag包含了多个子节点,tag就无法确定 .string 方法应该调用哪个子节点的内容, .string 的输出结果是 None :\n\n### .strings和stripped__strings\n\n如果tag中包含多个字符串 ,可以使用 .strings 来循环获取, 输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容\n\n```python\nfor string in soup.strings:\n    print(repr(string))\n\nfor string in soup.stripped_strings:\n    ...\n```\n\n### 父节点\n\n#### .parent\n\n`tag.parent, tag.string.parent`\n\n#### .parents\n\n```python\nlink = soup.a\nfor parent in link.parents:\n    if parent is None:\n        print(parent)\n    else:\n        print(parent.name)\n```\n\n### 兄弟节点\n\n#### .next_sibling和.previous_sibling\n\n实际文档中的tag的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白.\n\n#### .next_siblings和.previous_siblings\n\n通过 .next_siblings 和 .previous_siblings 属性可以对当前节点的兄弟节点迭代输出\n\n```python\nfor sibling in soup.a.next_siblings:\n    print(repr(sibling))\n```\n\n### 回退和前进\n\n```html\n<html><head><title>The Dormouse's story</title></head>\n<p class=\"title\"><b>The Dormouse's story</b></p>\n```\n\nHTML解析器把这段字符串转换成一连串的事件: “打开html标签”,”打开一个head标签”,”打开一个title标签”,”添加一段字符串”,”关闭title标签”,”关闭p标签”,等等.Beautiful Soup提供了重现解析器初始化过程的方法\n\n#### .next_element和.previous_element\n\n.previous_element 属性刚好与 .next_element 相反,它指向当前被解析的对象的前一个解析对象, next_element 属性指向解析过程中下一个被解析的对象(字符串或tag),\n\n#### .next_elements和.previous_elements\n\n通过 .next_elements 和 .previous_elements 的迭代器就可以向前或向后访问文档的解析内容,就好像文档正在被解析一样:\n\n```python\nfor elementin a_tag.next_elements:\n    print(repr(element))\n```\n\n## 搜索文档树\n\n### 过滤器\n\n#### 字符串\n\n最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的\\<b>标签:\n\n`soup.find_all('b')`\n\n#### 正则表达式\n\n如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示\\<body>和\\<b>标签都应该被找到:\n\n```python\nimport re\nfor tag in soup.find_all(re.compile(\"^b\"):\n    print(tag.name)\n```\n\n#### 列表\n\n如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有\\<a>标签和\\<b>标签:\n\n`soup.find_all(['a', 'b'])`\n\n#### True\n\nTrue 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点\n\n```python\nfor tag in soup.find_all(True):\n    print(tag.name)\n```\n\n#### 方法\n\n如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 False\n\n下面方法校验了当前元素,如果包含 class 属性却不包含 id 属性,那么将返回 True:\n\n```python\ndef has_class_but_no_id(tag):\n    return tag.has_attr('class') and not tag.has_attr('id')\n```\n\n下面代码找到所有被文字包含的节点内容\n\n```python\nfrom bs4 import NavigableString\n\ndef surrounded_by_strings(tag):\n    return (isinstance(tag.next_element, NavigableString)) and isinstance(tag.previous_element, NavigableString)\n```\n\n### find_all()\n\n`find_all(name, attrs, recursive, text, **kwargs)`\n\nfind_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件\n\n#### name参数\n\n搜索 name 参数的值可以使任一类型的 过滤器 ,字符窜,正则表达式,列表,方法或是 True .\n\nname 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉.\n\n#### keyword参数\n\n`soup.find_all(id='link2')`\n`soup.find_all(href=re.compile('elsie'))`\n`soup.find_all(id=True)`\n`soup.find_all(attrs={'id':'link2'})`\n\n#### 按CSS搜索\n\n通过 \".class__\"_ 参数搜索有指定CSS类名的tag, \".class__\"_ 参数同样接受不同类型的 过滤器 ,字符串,正则表达式,方法或 True\n\n`soup.find_all('a', class_='sister')`\n\n#### text参数\n\n通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, True.\n\n`soup.find_all(text=re.compile(\"Dormouse\"))`\n\n#### limit参数\n\nfind_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果.\n\n`soup.find_all('a', limit=2)`\n\n#### recursive参数\n\n调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False.\n\n`soup.find_all('title', recursive=False)`\n\n### 像调用find_all()一样调用tag\n\nfind_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. BeautifulSoup 对象和 tag 对象可以被当作一个方法来使用,这个方法的执行结果与调用这个对象的 find_all() 方法相同,下面两行代码是等价的:\n\n`soup.find_all('a')`\n`soup('a')`\n\n### find()\n\n`find(name, attrs, recursive, text, **kwargs)`\n\n```python\nsoup.find_all('title', limit=1)\n# [<title>The Dormouse's story</title>]\n\nsoup.find('title')\n# <title>The Dormouse's story</title>\n```\n\n唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.\n\nfind_all() 方法没有找到目标是返回空列表, find() 方法找不到目标时,返回 None.\n\n### find_parents()和find_parent()\n\nfind_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同\n\n### find_next_siblings()和find_next_sibling()\n\n这2个方法通过 .next_siblings 属性对当tag的所有后面解析的兄弟tag节点进行迭代, find_next_siblings() 方法返回所有符合条件的后面的兄弟节点, find_next_sibling() 只返回符合条件的后面的第一个tag节点.\n\n### find_previous_siblings和find_previous_sibling()\n\nfind_previous_siblings() 方法返回所有符合条件的前面的兄弟节点, find_previous_sibling() 方法返回第一个符合条件的前面的兄弟节点:\n\n### find_all_next()和find_next()\n\n这2个方法通过 .next_elements 属性对当前tag的之后的tag和字符串进行迭代, find_all_next() 方法返回所有符合条件的节点, find_next() 方法返回第一个符合条件的节点:\n\n### find_all_previous()和find_previous()\n\n这2个方法通过 .previous_elements 属性对当前节点前面的tag和字符串进行迭代, find_all_previous() 方法返回所有符合条件的节点, find_previous() 方法返回第一个符合条件的节点.\n\n### CSS选择器\n\n通过tag标签逐层查找 `soup.select(\"body a\")`\n\n找到某个tag标签的直接子标签 `soup.select(\"p > #link1\")`\n\n找到兄弟节点标签 `soup.select(\"#link1 + .sister\")`\n\n通过CSS类名查找 `soup.select(\".sister\"), soup.select(\"[class~=sister]\")`\n\n通过tag的id查找 `soup.select(\"#link1\")`\n\n通过是否存在某个属性查找 `soup.slect('a[href]')`\n\n通过属性的值来查找 `soup.select('a[href^=\"https:\"]', [href$='title'], [href*=\".com/\"]`\n\n## 修改文档树\n\n## 输出\n\n### 格式化输出\n\nprettify() 方法将Beautiful Soup的文档树格式化后以Unicode编码输出,每个XML/HTML标签都独占一行\n\n```python\nmarkup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>'\nsoup = BeautifulSoup(markup)\nsoup.prettify()\n# '<html>\\n <head>\\n </head>\\n <body>\\n  <a href=\"http://example.com/\">\\n...'\n\nprint(soup.prettify())\n# <html>\n#  <head>\n#  </head>\n#  <body>\n#   <a href=\"http://example.com/\">\n#    I linked to\n#    <i>\n#     example.com\n#    </i>\n#   </a>\n#  </body>\n# </html>\n```\n\n### 压缩输出\n\n如果只想得到结果字符串,不重视格式,那么可以对一个 BeautifulSoup 对象或 Tag 对象使用Python的 unicode() 或 str() 方法:\n\n`str(soup), Unicode(soup.a)`\n\n### 输出格式\n\nBeautiful Soup输出是会将HTML中的特殊字符转换成Unicode,比如“\\&lquot;”:\n如果将文档转换成字符串,Unicode编码会被编码成UTF-8.这样就无法正确显示HTML特殊字符了:\n\n### get_text()\n\n如果只想得到tag中包含的文本内容,那么可以用 get_text() 方法,这个方法获取到tag中包含的所有文本内容包括子孙tag中的内容,并将结果作为Unicode字符串返回:\n\n可以通过参数指定tag的文本内容的分隔符, 还可以去除前后空白: `soup.get_text('|', strip=True)`\n\n## 指定文档解析器\n\n## 编码\n","slug":"BeautifulSoup","published":1,"updated":"2018-08-31T03:51:46.501Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh0q0000zkvor5zlsqia","content":"<h2 id=\"BeautifulSoup\"><a href=\"#BeautifulSoup\" class=\"headerlink\" title=\"BeautifulSoup\"></a><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html\" target=\"_blank\" rel=\"noopener\">BeautifulSoup</a></h2><p>You didn’t write that awful page. You’re just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it’s been saving programmers hours or days of work on quick-turnaround screen scraping projects.</p>\n<h2 id=\"如何使用\"><a href=\"#如何使用\" class=\"headerlink\" title=\"如何使用\"></a>如何使用</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup</span><br><span class=\"line\">soup = BeautifulSoup(open(<span class=\"string\">'index.html'</span>))</span><br></pre></td></tr></table></figure>\n<h2 id=\"对象的种类\"><a href=\"#对象的种类\" class=\"headerlink\" title=\"对象的种类\"></a>对象的种类</h2><h3 id=\"Tag\"><a href=\"#Tag\" class=\"headerlink\" title=\"Tag\"></a>Tag</h3><p>Tag 对象与XML或HTML原生文档中的tag相同:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">soup = BeautifulSoup(<span class=\"string\">'&lt;b class=\"boldest\"&gt;Extremely bold&lt;/b&gt;'</span>)</span><br><span class=\"line\">tag = soup.b</span><br><span class=\"line\">type(tag)</span><br><span class=\"line\"><span class=\"comment\"># &lt;class 'bs4.element.Tag'&gt;</span></span><br></pre></td></tr></table></figure>\n<p>Tag的属性：<code>tag.name, tag.attrs</code></p>\n<p>tag的属性的操作方法与字典相同: <code>tag[&#39;class&#39;]</code></p>\n<h3 id=\"可以遍历的字符串\"><a href=\"#可以遍历的字符串\" class=\"headerlink\" title=\"可以遍历的字符串\"></a>可以遍历的字符串</h3><p>字符串常被包含在tag内.Beautiful Soup用 NavigableString 类来包装tag中的字符串:<code>tag.string</code></p>\n<p><code>unicode_string = unicode(tag.string)</code></p>\n<p>tag中包含的字符串不能编辑,但是可以被替换成其它的字符串,用 replace_with() 方法:<code>tag.string.replace_with(&quot;No&quot;)</code></p>\n<h3 id=\"BeautifulSoup-1\"><a href=\"#BeautifulSoup-1\" class=\"headerlink\" title=\"BeautifulSoup\"></a>BeautifulSoup</h3><p>BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.</p>\n<h3 id=\"注释及特殊字符串\"><a href=\"#注释及特殊字符串\" class=\"headerlink\" title=\"注释及特殊字符串\"></a>注释及特殊字符串</h3><p>Comment 对象是一个特殊类型的 NavigableString 对象:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">markup = <span class=\"string\">\"&lt;b&gt;&lt;!--Hey, buddy. Want to buy a used parser?--&gt;&lt;/b&gt;\"</span></span><br><span class=\"line\">soup = BeautifulSoup(markup)</span><br><span class=\"line\">comment = soup.b.string</span><br><span class=\"line\">type(comment)</span><br><span class=\"line\"><span class=\"comment\"># &lt;class 'bs4.element.Comment'&gt;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"遍历文档树\"><a href=\"#遍历文档树\" class=\"headerlink\" title=\"遍历文档树\"></a>遍历文档树</h2><h3 id=\"子节点\"><a href=\"#子节点\" class=\"headerlink\" title=\"子节点\"></a>子节点</h3><p>一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.</p>\n<h4 id=\"tag的名字\"><a href=\"#tag的名字\" class=\"headerlink\" title=\"tag的名字\"></a>tag的名字</h4><p><code>soup.head, soup.title, soup.body.b, soup.a, soup.find_all(&#39;a&#39;)</code></p>\n<h4 id=\"contents和-children\"><a href=\"#contents和-children\" class=\"headerlink\" title=\".contents和.children\"></a>.contents和.children</h4><p>tag的 .contents 属性可以将tag的子节点以列表的方式输出:</p>\n<p>通过tag的 .children 生成器,可以对tag的子节点进行循环:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> child <span class=\"keyword\">in</span> title_tag.children:</span><br><span class=\"line\">    print(child)</span><br></pre></td></tr></table></figure>\n<p>descendants 属性可以对所有tag的子孙节点进行递归循环:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> child <span class=\"keyword\">in</span> title_tag.descendants:</span><br><span class=\"line\">    print(child)</span><br></pre></td></tr></table></figure>\n<h3 id=\"string\"><a href=\"#string\" class=\"headerlink\" title=\".string\"></a>.string</h3><p>如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点:<code>tag.string</code></p>\n<p>如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,输出结果与当前唯一子节点的 .string 结果相同</p>\n<p>如果tag包含了多个子节点,tag就无法确定 .string 方法应该调用哪个子节点的内容, .string 的输出结果是 None :</p>\n<h3 id=\"strings和stripped-strings\"><a href=\"#strings和stripped-strings\" class=\"headerlink\" title=\".strings和stripped__strings\"></a>.strings和stripped__strings</h3><p>如果tag中包含多个字符串 ,可以使用 .strings 来循环获取, 输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> string <span class=\"keyword\">in</span> soup.strings:</span><br><span class=\"line\">    print(repr(string))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> string <span class=\"keyword\">in</span> soup.stripped_strings:</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure>\n<h3 id=\"父节点\"><a href=\"#父节点\" class=\"headerlink\" title=\"父节点\"></a>父节点</h3><h4 id=\"parent\"><a href=\"#parent\" class=\"headerlink\" title=\".parent\"></a>.parent</h4><p><code>tag.parent, tag.string.parent</code></p>\n<h4 id=\"parents\"><a href=\"#parents\" class=\"headerlink\" title=\".parents\"></a>.parents</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">link = soup.a</span><br><span class=\"line\"><span class=\"keyword\">for</span> parent <span class=\"keyword\">in</span> link.parents:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> parent <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        print(parent)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        print(parent.name)</span><br></pre></td></tr></table></figure>\n<h3 id=\"兄弟节点\"><a href=\"#兄弟节点\" class=\"headerlink\" title=\"兄弟节点\"></a>兄弟节点</h3><h4 id=\"next-sibling和-previous-sibling\"><a href=\"#next-sibling和-previous-sibling\" class=\"headerlink\" title=\".next_sibling和.previous_sibling\"></a>.next_sibling和.previous_sibling</h4><p>实际文档中的tag的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白.</p>\n<h4 id=\"next-siblings和-previous-siblings\"><a href=\"#next-siblings和-previous-siblings\" class=\"headerlink\" title=\".next_siblings和.previous_siblings\"></a>.next_siblings和.previous_siblings</h4><p>通过 .next_siblings 和 .previous_siblings 属性可以对当前节点的兄弟节点迭代输出</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> sibling <span class=\"keyword\">in</span> soup.a.next_siblings:</span><br><span class=\"line\">    print(repr(sibling))</span><br></pre></td></tr></table></figure>\n<h3 id=\"回退和前进\"><a href=\"#回退和前进\" class=\"headerlink\" title=\"回退和前进\"></a>回退和前进</h3><figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>The Dormouse's story<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">\"title\"</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">b</span>&gt;</span>The Dormouse's story<span class=\"tag\">&lt;/<span class=\"name\">b</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>HTML解析器把这段字符串转换成一连串的事件: “打开html标签”,”打开一个head标签”,”打开一个title标签”,”添加一段字符串”,”关闭title标签”,”关闭p标签”,等等.Beautiful Soup提供了重现解析器初始化过程的方法</p>\n<h4 id=\"next-element和-previous-element\"><a href=\"#next-element和-previous-element\" class=\"headerlink\" title=\".next_element和.previous_element\"></a>.next_element和.previous_element</h4><p>.previous_element 属性刚好与 .next_element 相反,它指向当前被解析的对象的前一个解析对象, next_element 属性指向解析过程中下一个被解析的对象(字符串或tag),</p>\n<h4 id=\"next-elements和-previous-elements\"><a href=\"#next-elements和-previous-elements\" class=\"headerlink\" title=\".next_elements和.previous_elements\"></a>.next_elements和.previous_elements</h4><p>通过 .next_elements 和 .previous_elements 的迭代器就可以向前或向后访问文档的解析内容,就好像文档正在被解析一样:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> elementin a_tag.next_elements:</span><br><span class=\"line\">    print(repr(element))</span><br></pre></td></tr></table></figure>\n<h2 id=\"搜索文档树\"><a href=\"#搜索文档树\" class=\"headerlink\" title=\"搜索文档树\"></a>搜索文档树</h2><h3 id=\"过滤器\"><a href=\"#过滤器\" class=\"headerlink\" title=\"过滤器\"></a>过滤器</h3><h4 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a>字符串</h4><p>最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的\\<b>标签:</b></p>\n<p><code>soup.find_all(&#39;b&#39;)</code></p>\n<h4 id=\"正则表达式\"><a href=\"#正则表达式\" class=\"headerlink\" title=\"正则表达式\"></a>正则表达式</h4><p>如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示\\<body>和\\<b>标签都应该被找到:</b></body></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">for</span> tag <span class=\"keyword\">in</span> soup.find_all(re.compile(<span class=\"string\">\"^b\"</span>):</span><br><span class=\"line\">    print(tag.name)</span><br></pre></td></tr></table></figure>\n<h4 id=\"列表\"><a href=\"#列表\" class=\"headerlink\" title=\"列表\"></a>列表</h4><p>如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有\\<a>标签和\\<b>标签:</b></a></p>\n<p><code>soup.find_all([&#39;a&#39;, &#39;b&#39;])</code></p>\n<h4 id=\"True\"><a href=\"#True\" class=\"headerlink\" title=\"True\"></a>True</h4><p>True 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> tag <span class=\"keyword\">in</span> soup.find_all(<span class=\"keyword\">True</span>):</span><br><span class=\"line\">    print(tag.name)</span><br></pre></td></tr></table></figure>\n<h4 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h4><p>如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 False</p>\n<p>下面方法校验了当前元素,如果包含 class 属性却不包含 id 属性,那么将返回 True:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">has_class_but_no_id</span><span class=\"params\">(tag)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tag.has_attr(<span class=\"string\">'class'</span>) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> tag.has_attr(<span class=\"string\">'id'</span>)</span><br></pre></td></tr></table></figure>\n<p>下面代码找到所有被文字包含的节点内容</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> NavigableString</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">surrounded_by_strings</span><span class=\"params\">(tag)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (isinstance(tag.next_element, NavigableString)) <span class=\"keyword\">and</span> isinstance(tag.previous_element, NavigableString)</span><br></pre></td></tr></table></figure>\n<h3 id=\"find-all\"><a href=\"#find-all\" class=\"headerlink\" title=\"find_all()\"></a>find_all()</h3><p><code>find_all(name, attrs, recursive, text, **kwargs)</code></p>\n<p>find_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件</p>\n<h4 id=\"name参数\"><a href=\"#name参数\" class=\"headerlink\" title=\"name参数\"></a>name参数</h4><p>搜索 name 参数的值可以使任一类型的 过滤器 ,字符窜,正则表达式,列表,方法或是 True .</p>\n<p>name 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉.</p>\n<h4 id=\"keyword参数\"><a href=\"#keyword参数\" class=\"headerlink\" title=\"keyword参数\"></a>keyword参数</h4><p><code>soup.find_all(id=&#39;link2&#39;)</code><br><code>soup.find_all(href=re.compile(&#39;elsie&#39;))</code><br><code>soup.find_all(id=True)</code><br><code>soup.find_all(attrs={&#39;id&#39;:&#39;link2&#39;})</code></p>\n<h4 id=\"按CSS搜索\"><a href=\"#按CSS搜索\" class=\"headerlink\" title=\"按CSS搜索\"></a>按CSS搜索</h4><p>通过 “.class__”_ 参数搜索有指定CSS类名的tag, “.class__”_ 参数同样接受不同类型的 过滤器 ,字符串,正则表达式,方法或 True</p>\n<p><code>soup.find_all(&#39;a&#39;, class_=&#39;sister&#39;)</code></p>\n<h4 id=\"text参数\"><a href=\"#text参数\" class=\"headerlink\" title=\"text参数\"></a>text参数</h4><p>通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, True.</p>\n<p><code>soup.find_all(text=re.compile(&quot;Dormouse&quot;))</code></p>\n<h4 id=\"limit参数\"><a href=\"#limit参数\" class=\"headerlink\" title=\"limit参数\"></a>limit参数</h4><p>find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果.</p>\n<p><code>soup.find_all(&#39;a&#39;, limit=2)</code></p>\n<h4 id=\"recursive参数\"><a href=\"#recursive参数\" class=\"headerlink\" title=\"recursive参数\"></a>recursive参数</h4><p>调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False.</p>\n<p><code>soup.find_all(&#39;title&#39;, recursive=False)</code></p>\n<h3 id=\"像调用find-all-一样调用tag\"><a href=\"#像调用find-all-一样调用tag\" class=\"headerlink\" title=\"像调用find_all()一样调用tag\"></a>像调用find_all()一样调用tag</h3><p>find_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. BeautifulSoup 对象和 tag 对象可以被当作一个方法来使用,这个方法的执行结果与调用这个对象的 find_all() 方法相同,下面两行代码是等价的:</p>\n<p><code>soup.find_all(&#39;a&#39;)</code><br><code>soup(&#39;a&#39;)</code></p>\n<h3 id=\"find\"><a href=\"#find\" class=\"headerlink\" title=\"find()\"></a>find()</h3><p><code>find(name, attrs, recursive, text, **kwargs)</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">soup.find_all(<span class=\"string\">'title'</span>, limit=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></span><br><span class=\"line\"></span><br><span class=\"line\">soup.find(<span class=\"string\">'title'</span>)</span><br><span class=\"line\"><span class=\"comment\"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></span><br></pre></td></tr></table></figure>\n<p>唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.</p>\n<p>find_all() 方法没有找到目标是返回空列表, find() 方法找不到目标时,返回 None.</p>\n<h3 id=\"find-parents-和find-parent\"><a href=\"#find-parents-和find-parent\" class=\"headerlink\" title=\"find_parents()和find_parent()\"></a>find_parents()和find_parent()</h3><p>find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同</p>\n<h3 id=\"find-next-siblings-和find-next-sibling\"><a href=\"#find-next-siblings-和find-next-sibling\" class=\"headerlink\" title=\"find_next_siblings()和find_next_sibling()\"></a>find_next_siblings()和find_next_sibling()</h3><p>这2个方法通过 .next_siblings 属性对当tag的所有后面解析的兄弟tag节点进行迭代, find_next_siblings() 方法返回所有符合条件的后面的兄弟节点, find_next_sibling() 只返回符合条件的后面的第一个tag节点.</p>\n<h3 id=\"find-previous-siblings和find-previous-sibling\"><a href=\"#find-previous-siblings和find-previous-sibling\" class=\"headerlink\" title=\"find_previous_siblings和find_previous_sibling()\"></a>find_previous_siblings和find_previous_sibling()</h3><p>find_previous_siblings() 方法返回所有符合条件的前面的兄弟节点, find_previous_sibling() 方法返回第一个符合条件的前面的兄弟节点:</p>\n<h3 id=\"find-all-next-和find-next\"><a href=\"#find-all-next-和find-next\" class=\"headerlink\" title=\"find_all_next()和find_next()\"></a>find_all_next()和find_next()</h3><p>这2个方法通过 .next_elements 属性对当前tag的之后的tag和字符串进行迭代, find_all_next() 方法返回所有符合条件的节点, find_next() 方法返回第一个符合条件的节点:</p>\n<h3 id=\"find-all-previous-和find-previous\"><a href=\"#find-all-previous-和find-previous\" class=\"headerlink\" title=\"find_all_previous()和find_previous()\"></a>find_all_previous()和find_previous()</h3><p>这2个方法通过 .previous_elements 属性对当前节点前面的tag和字符串进行迭代, find_all_previous() 方法返回所有符合条件的节点, find_previous() 方法返回第一个符合条件的节点.</p>\n<h3 id=\"CSS选择器\"><a href=\"#CSS选择器\" class=\"headerlink\" title=\"CSS选择器\"></a>CSS选择器</h3><p>通过tag标签逐层查找 <code>soup.select(&quot;body a&quot;)</code></p>\n<p>找到某个tag标签的直接子标签 <code>soup.select(&quot;p &gt; #link1&quot;)</code></p>\n<p>找到兄弟节点标签 <code>soup.select(&quot;#link1 + .sister&quot;)</code></p>\n<p>通过CSS类名查找 <code>soup.select(&quot;.sister&quot;), soup.select(&quot;[class~=sister]&quot;)</code></p>\n<p>通过tag的id查找 <code>soup.select(&quot;#link1&quot;)</code></p>\n<p>通过是否存在某个属性查找 <code>soup.slect(&#39;a[href]&#39;)</code></p>\n<p>通过属性的值来查找 <code>soup.select(&#39;a[href^=&quot;https:&quot;]&#39;, [href$=&#39;title&#39;], [href*=&quot;.com/&quot;]</code></p>\n<h2 id=\"修改文档树\"><a href=\"#修改文档树\" class=\"headerlink\" title=\"修改文档树\"></a>修改文档树</h2><h2 id=\"输出\"><a href=\"#输出\" class=\"headerlink\" title=\"输出\"></a>输出</h2><h3 id=\"格式化输出\"><a href=\"#格式化输出\" class=\"headerlink\" title=\"格式化输出\"></a>格式化输出</h3><p>prettify() 方法将Beautiful Soup的文档树格式化后以Unicode编码输出,每个XML/HTML标签都独占一行</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">markup = <span class=\"string\">'&lt;a href=\"http://example.com/\"&gt;I linked to &lt;i&gt;example.com&lt;/i&gt;&lt;/a&gt;'</span></span><br><span class=\"line\">soup = BeautifulSoup(markup)</span><br><span class=\"line\">soup.prettify()</span><br><span class=\"line\"><span class=\"comment\"># '&lt;html&gt;\\n &lt;head&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n  &lt;a href=\"http://example.com/\"&gt;\\n...'</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(soup.prettify())</span><br><span class=\"line\"><span class=\"comment\"># &lt;html&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;head&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;/head&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;body&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#   &lt;a href=\"http://example.com/\"&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#    I linked to</span></span><br><span class=\"line\"><span class=\"comment\">#    &lt;i&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#     example.com</span></span><br><span class=\"line\"><span class=\"comment\">#    &lt;/i&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#   &lt;/a&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;/body&gt;</span></span><br><span class=\"line\"><span class=\"comment\"># &lt;/html&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"压缩输出\"><a href=\"#压缩输出\" class=\"headerlink\" title=\"压缩输出\"></a>压缩输出</h3><p>如果只想得到结果字符串,不重视格式,那么可以对一个 BeautifulSoup 对象或 Tag 对象使用Python的 unicode() 或 str() 方法:</p>\n<p><code>str(soup), Unicode(soup.a)</code></p>\n<h3 id=\"输出格式\"><a href=\"#输出格式\" class=\"headerlink\" title=\"输出格式\"></a>输出格式</h3><p>Beautiful Soup输出是会将HTML中的特殊字符转换成Unicode,比如“\\&lquot;”:<br>如果将文档转换成字符串,Unicode编码会被编码成UTF-8.这样就无法正确显示HTML特殊字符了:</p>\n<h3 id=\"get-text\"><a href=\"#get-text\" class=\"headerlink\" title=\"get_text()\"></a>get_text()</h3><p>如果只想得到tag中包含的文本内容,那么可以用 get_text() 方法,这个方法获取到tag中包含的所有文本内容包括子孙tag中的内容,并将结果作为Unicode字符串返回:</p>\n<p>可以通过参数指定tag的文本内容的分隔符, 还可以去除前后空白: <code>soup.get_text(&#39;|&#39;, strip=True)</code></p>\n<h2 id=\"指定文档解析器\"><a href=\"#指定文档解析器\" class=\"headerlink\" title=\"指定文档解析器\"></a>指定文档解析器</h2><h2 id=\"编码\"><a href=\"#编码\" class=\"headerlink\" title=\"编码\"></a>编码</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"BeautifulSoup\"><a href=\"#BeautifulSoup\" class=\"headerlink\" title=\"BeautifulSoup\"></a><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html\" target=\"_blank\" rel=\"noopener\">BeautifulSoup</a></h2><p>You didn’t write that awful page. You’re just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it’s been saving programmers hours or days of work on quick-turnaround screen scraping projects.</p>\n<h2 id=\"如何使用\"><a href=\"#如何使用\" class=\"headerlink\" title=\"如何使用\"></a>如何使用</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> BeautifulSoup</span><br><span class=\"line\">soup = BeautifulSoup(open(<span class=\"string\">'index.html'</span>))</span><br></pre></td></tr></table></figure>\n<h2 id=\"对象的种类\"><a href=\"#对象的种类\" class=\"headerlink\" title=\"对象的种类\"></a>对象的种类</h2><h3 id=\"Tag\"><a href=\"#Tag\" class=\"headerlink\" title=\"Tag\"></a>Tag</h3><p>Tag 对象与XML或HTML原生文档中的tag相同:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">soup = BeautifulSoup(<span class=\"string\">'&lt;b class=\"boldest\"&gt;Extremely bold&lt;/b&gt;'</span>)</span><br><span class=\"line\">tag = soup.b</span><br><span class=\"line\">type(tag)</span><br><span class=\"line\"><span class=\"comment\"># &lt;class 'bs4.element.Tag'&gt;</span></span><br></pre></td></tr></table></figure>\n<p>Tag的属性：<code>tag.name, tag.attrs</code></p>\n<p>tag的属性的操作方法与字典相同: <code>tag[&#39;class&#39;]</code></p>\n<h3 id=\"可以遍历的字符串\"><a href=\"#可以遍历的字符串\" class=\"headerlink\" title=\"可以遍历的字符串\"></a>可以遍历的字符串</h3><p>字符串常被包含在tag内.Beautiful Soup用 NavigableString 类来包装tag中的字符串:<code>tag.string</code></p>\n<p><code>unicode_string = unicode(tag.string)</code></p>\n<p>tag中包含的字符串不能编辑,但是可以被替换成其它的字符串,用 replace_with() 方法:<code>tag.string.replace_with(&quot;No&quot;)</code></p>\n<h3 id=\"BeautifulSoup-1\"><a href=\"#BeautifulSoup-1\" class=\"headerlink\" title=\"BeautifulSoup\"></a>BeautifulSoup</h3><p>BeautifulSoup 对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法.</p>\n<h3 id=\"注释及特殊字符串\"><a href=\"#注释及特殊字符串\" class=\"headerlink\" title=\"注释及特殊字符串\"></a>注释及特殊字符串</h3><p>Comment 对象是一个特殊类型的 NavigableString 对象:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">markup = <span class=\"string\">\"&lt;b&gt;&lt;!--Hey, buddy. Want to buy a used parser?--&gt;&lt;/b&gt;\"</span></span><br><span class=\"line\">soup = BeautifulSoup(markup)</span><br><span class=\"line\">comment = soup.b.string</span><br><span class=\"line\">type(comment)</span><br><span class=\"line\"><span class=\"comment\"># &lt;class 'bs4.element.Comment'&gt;</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"遍历文档树\"><a href=\"#遍历文档树\" class=\"headerlink\" title=\"遍历文档树\"></a>遍历文档树</h2><h3 id=\"子节点\"><a href=\"#子节点\" class=\"headerlink\" title=\"子节点\"></a>子节点</h3><p>一个Tag可能包含多个字符串或其它的Tag,这些都是这个Tag的子节点.</p>\n<h4 id=\"tag的名字\"><a href=\"#tag的名字\" class=\"headerlink\" title=\"tag的名字\"></a>tag的名字</h4><p><code>soup.head, soup.title, soup.body.b, soup.a, soup.find_all(&#39;a&#39;)</code></p>\n<h4 id=\"contents和-children\"><a href=\"#contents和-children\" class=\"headerlink\" title=\".contents和.children\"></a>.contents和.children</h4><p>tag的 .contents 属性可以将tag的子节点以列表的方式输出:</p>\n<p>通过tag的 .children 生成器,可以对tag的子节点进行循环:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> child <span class=\"keyword\">in</span> title_tag.children:</span><br><span class=\"line\">    print(child)</span><br></pre></td></tr></table></figure>\n<p>descendants 属性可以对所有tag的子孙节点进行递归循环:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> child <span class=\"keyword\">in</span> title_tag.descendants:</span><br><span class=\"line\">    print(child)</span><br></pre></td></tr></table></figure>\n<h3 id=\"string\"><a href=\"#string\" class=\"headerlink\" title=\".string\"></a>.string</h3><p>如果tag只有一个 NavigableString 类型子节点,那么这个tag可以使用 .string 得到子节点:<code>tag.string</code></p>\n<p>如果一个tag仅有一个子节点,那么这个tag也可以使用 .string 方法,输出结果与当前唯一子节点的 .string 结果相同</p>\n<p>如果tag包含了多个子节点,tag就无法确定 .string 方法应该调用哪个子节点的内容, .string 的输出结果是 None :</p>\n<h3 id=\"strings和stripped-strings\"><a href=\"#strings和stripped-strings\" class=\"headerlink\" title=\".strings和stripped__strings\"></a>.strings和stripped__strings</h3><p>如果tag中包含多个字符串 ,可以使用 .strings 来循环获取, 输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> string <span class=\"keyword\">in</span> soup.strings:</span><br><span class=\"line\">    print(repr(string))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> string <span class=\"keyword\">in</span> soup.stripped_strings:</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure>\n<h3 id=\"父节点\"><a href=\"#父节点\" class=\"headerlink\" title=\"父节点\"></a>父节点</h3><h4 id=\"parent\"><a href=\"#parent\" class=\"headerlink\" title=\".parent\"></a>.parent</h4><p><code>tag.parent, tag.string.parent</code></p>\n<h4 id=\"parents\"><a href=\"#parents\" class=\"headerlink\" title=\".parents\"></a>.parents</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">link = soup.a</span><br><span class=\"line\"><span class=\"keyword\">for</span> parent <span class=\"keyword\">in</span> link.parents:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> parent <span class=\"keyword\">is</span> <span class=\"keyword\">None</span>:</span><br><span class=\"line\">        print(parent)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        print(parent.name)</span><br></pre></td></tr></table></figure>\n<h3 id=\"兄弟节点\"><a href=\"#兄弟节点\" class=\"headerlink\" title=\"兄弟节点\"></a>兄弟节点</h3><h4 id=\"next-sibling和-previous-sibling\"><a href=\"#next-sibling和-previous-sibling\" class=\"headerlink\" title=\".next_sibling和.previous_sibling\"></a>.next_sibling和.previous_sibling</h4><p>实际文档中的tag的 .next_sibling 和 .previous_sibling 属性通常是字符串或空白.</p>\n<h4 id=\"next-siblings和-previous-siblings\"><a href=\"#next-siblings和-previous-siblings\" class=\"headerlink\" title=\".next_siblings和.previous_siblings\"></a>.next_siblings和.previous_siblings</h4><p>通过 .next_siblings 和 .previous_siblings 属性可以对当前节点的兄弟节点迭代输出</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> sibling <span class=\"keyword\">in</span> soup.a.next_siblings:</span><br><span class=\"line\">    print(repr(sibling))</span><br></pre></td></tr></table></figure>\n<h3 id=\"回退和前进\"><a href=\"#回退和前进\" class=\"headerlink\" title=\"回退和前进\"></a>回退和前进</h3><figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">html</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">head</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">title</span>&gt;</span>The Dormouse's story<span class=\"tag\">&lt;/<span class=\"name\">title</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">head</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">p</span> <span class=\"attr\">class</span>=<span class=\"string\">\"title\"</span>&gt;</span><span class=\"tag\">&lt;<span class=\"name\">b</span>&gt;</span>The Dormouse's story<span class=\"tag\">&lt;/<span class=\"name\">b</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">p</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>HTML解析器把这段字符串转换成一连串的事件: “打开html标签”,”打开一个head标签”,”打开一个title标签”,”添加一段字符串”,”关闭title标签”,”关闭p标签”,等等.Beautiful Soup提供了重现解析器初始化过程的方法</p>\n<h4 id=\"next-element和-previous-element\"><a href=\"#next-element和-previous-element\" class=\"headerlink\" title=\".next_element和.previous_element\"></a>.next_element和.previous_element</h4><p>.previous_element 属性刚好与 .next_element 相反,它指向当前被解析的对象的前一个解析对象, next_element 属性指向解析过程中下一个被解析的对象(字符串或tag),</p>\n<h4 id=\"next-elements和-previous-elements\"><a href=\"#next-elements和-previous-elements\" class=\"headerlink\" title=\".next_elements和.previous_elements\"></a>.next_elements和.previous_elements</h4><p>通过 .next_elements 和 .previous_elements 的迭代器就可以向前或向后访问文档的解析内容,就好像文档正在被解析一样:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> elementin a_tag.next_elements:</span><br><span class=\"line\">    print(repr(element))</span><br></pre></td></tr></table></figure>\n<h2 id=\"搜索文档树\"><a href=\"#搜索文档树\" class=\"headerlink\" title=\"搜索文档树\"></a>搜索文档树</h2><h3 id=\"过滤器\"><a href=\"#过滤器\" class=\"headerlink\" title=\"过滤器\"></a>过滤器</h3><h4 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a>字符串</h4><p>最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的\\<b>标签:</b></p>\n<p><code>soup.find_all(&#39;b&#39;)</code></p>\n<h4 id=\"正则表达式\"><a href=\"#正则表达式\" class=\"headerlink\" title=\"正则表达式\"></a>正则表达式</h4><p>如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示\\<body>和\\<b>标签都应该被找到:</b></body></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\"><span class=\"keyword\">for</span> tag <span class=\"keyword\">in</span> soup.find_all(re.compile(<span class=\"string\">\"^b\"</span>):</span><br><span class=\"line\">    print(tag.name)</span><br></pre></td></tr></table></figure>\n<h4 id=\"列表\"><a href=\"#列表\" class=\"headerlink\" title=\"列表\"></a>列表</h4><p>如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有\\<a>标签和\\<b>标签:</b></a></p>\n<p><code>soup.find_all([&#39;a&#39;, &#39;b&#39;])</code></p>\n<h4 id=\"True\"><a href=\"#True\" class=\"headerlink\" title=\"True\"></a>True</h4><p>True 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> tag <span class=\"keyword\">in</span> soup.find_all(<span class=\"keyword\">True</span>):</span><br><span class=\"line\">    print(tag.name)</span><br></pre></td></tr></table></figure>\n<h4 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h4><p>如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 False</p>\n<p>下面方法校验了当前元素,如果包含 class 属性却不包含 id 属性,那么将返回 True:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">has_class_but_no_id</span><span class=\"params\">(tag)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tag.has_attr(<span class=\"string\">'class'</span>) <span class=\"keyword\">and</span> <span class=\"keyword\">not</span> tag.has_attr(<span class=\"string\">'id'</span>)</span><br></pre></td></tr></table></figure>\n<p>下面代码找到所有被文字包含的节点内容</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> bs4 <span class=\"keyword\">import</span> NavigableString</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">surrounded_by_strings</span><span class=\"params\">(tag)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (isinstance(tag.next_element, NavigableString)) <span class=\"keyword\">and</span> isinstance(tag.previous_element, NavigableString)</span><br></pre></td></tr></table></figure>\n<h3 id=\"find-all\"><a href=\"#find-all\" class=\"headerlink\" title=\"find_all()\"></a>find_all()</h3><p><code>find_all(name, attrs, recursive, text, **kwargs)</code></p>\n<p>find_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件</p>\n<h4 id=\"name参数\"><a href=\"#name参数\" class=\"headerlink\" title=\"name参数\"></a>name参数</h4><p>搜索 name 参数的值可以使任一类型的 过滤器 ,字符窜,正则表达式,列表,方法或是 True .</p>\n<p>name 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉.</p>\n<h4 id=\"keyword参数\"><a href=\"#keyword参数\" class=\"headerlink\" title=\"keyword参数\"></a>keyword参数</h4><p><code>soup.find_all(id=&#39;link2&#39;)</code><br><code>soup.find_all(href=re.compile(&#39;elsie&#39;))</code><br><code>soup.find_all(id=True)</code><br><code>soup.find_all(attrs={&#39;id&#39;:&#39;link2&#39;})</code></p>\n<h4 id=\"按CSS搜索\"><a href=\"#按CSS搜索\" class=\"headerlink\" title=\"按CSS搜索\"></a>按CSS搜索</h4><p>通过 “.class__”_ 参数搜索有指定CSS类名的tag, “.class__”_ 参数同样接受不同类型的 过滤器 ,字符串,正则表达式,方法或 True</p>\n<p><code>soup.find_all(&#39;a&#39;, class_=&#39;sister&#39;)</code></p>\n<h4 id=\"text参数\"><a href=\"#text参数\" class=\"headerlink\" title=\"text参数\"></a>text参数</h4><p>通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, True.</p>\n<p><code>soup.find_all(text=re.compile(&quot;Dormouse&quot;))</code></p>\n<h4 id=\"limit参数\"><a href=\"#limit参数\" class=\"headerlink\" title=\"limit参数\"></a>limit参数</h4><p>find_all() 方法返回全部的搜索结构,如果文档树很大那么搜索会很慢.如果我们不需要全部结果,可以使用 limit 参数限制返回结果的数量.效果与SQL中的limit关键字类似,当搜索到的结果数量达到 limit 的限制时,就停止搜索返回结果.</p>\n<p><code>soup.find_all(&#39;a&#39;, limit=2)</code></p>\n<h4 id=\"recursive参数\"><a href=\"#recursive参数\" class=\"headerlink\" title=\"recursive参数\"></a>recursive参数</h4><p>调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False.</p>\n<p><code>soup.find_all(&#39;title&#39;, recursive=False)</code></p>\n<h3 id=\"像调用find-all-一样调用tag\"><a href=\"#像调用find-all-一样调用tag\" class=\"headerlink\" title=\"像调用find_all()一样调用tag\"></a>像调用find_all()一样调用tag</h3><p>find_all() 几乎是Beautiful Soup中最常用的搜索方法,所以我们定义了它的简写方法. BeautifulSoup 对象和 tag 对象可以被当作一个方法来使用,这个方法的执行结果与调用这个对象的 find_all() 方法相同,下面两行代码是等价的:</p>\n<p><code>soup.find_all(&#39;a&#39;)</code><br><code>soup(&#39;a&#39;)</code></p>\n<h3 id=\"find\"><a href=\"#find\" class=\"headerlink\" title=\"find()\"></a>find()</h3><p><code>find(name, attrs, recursive, text, **kwargs)</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">soup.find_all(<span class=\"string\">'title'</span>, limit=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></span><br><span class=\"line\"></span><br><span class=\"line\">soup.find(<span class=\"string\">'title'</span>)</span><br><span class=\"line\"><span class=\"comment\"># &lt;title&gt;The Dormouse's story&lt;/title&gt;</span></span><br></pre></td></tr></table></figure>\n<p>唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.</p>\n<p>find_all() 方法没有找到目标是返回空列表, find() 方法找不到目标时,返回 None.</p>\n<h3 id=\"find-parents-和find-parent\"><a href=\"#find-parents-和find-parent\" class=\"headerlink\" title=\"find_parents()和find_parent()\"></a>find_parents()和find_parent()</h3><p>find_parents() 和 find_parent() 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同</p>\n<h3 id=\"find-next-siblings-和find-next-sibling\"><a href=\"#find-next-siblings-和find-next-sibling\" class=\"headerlink\" title=\"find_next_siblings()和find_next_sibling()\"></a>find_next_siblings()和find_next_sibling()</h3><p>这2个方法通过 .next_siblings 属性对当tag的所有后面解析的兄弟tag节点进行迭代, find_next_siblings() 方法返回所有符合条件的后面的兄弟节点, find_next_sibling() 只返回符合条件的后面的第一个tag节点.</p>\n<h3 id=\"find-previous-siblings和find-previous-sibling\"><a href=\"#find-previous-siblings和find-previous-sibling\" class=\"headerlink\" title=\"find_previous_siblings和find_previous_sibling()\"></a>find_previous_siblings和find_previous_sibling()</h3><p>find_previous_siblings() 方法返回所有符合条件的前面的兄弟节点, find_previous_sibling() 方法返回第一个符合条件的前面的兄弟节点:</p>\n<h3 id=\"find-all-next-和find-next\"><a href=\"#find-all-next-和find-next\" class=\"headerlink\" title=\"find_all_next()和find_next()\"></a>find_all_next()和find_next()</h3><p>这2个方法通过 .next_elements 属性对当前tag的之后的tag和字符串进行迭代, find_all_next() 方法返回所有符合条件的节点, find_next() 方法返回第一个符合条件的节点:</p>\n<h3 id=\"find-all-previous-和find-previous\"><a href=\"#find-all-previous-和find-previous\" class=\"headerlink\" title=\"find_all_previous()和find_previous()\"></a>find_all_previous()和find_previous()</h3><p>这2个方法通过 .previous_elements 属性对当前节点前面的tag和字符串进行迭代, find_all_previous() 方法返回所有符合条件的节点, find_previous() 方法返回第一个符合条件的节点.</p>\n<h3 id=\"CSS选择器\"><a href=\"#CSS选择器\" class=\"headerlink\" title=\"CSS选择器\"></a>CSS选择器</h3><p>通过tag标签逐层查找 <code>soup.select(&quot;body a&quot;)</code></p>\n<p>找到某个tag标签的直接子标签 <code>soup.select(&quot;p &gt; #link1&quot;)</code></p>\n<p>找到兄弟节点标签 <code>soup.select(&quot;#link1 + .sister&quot;)</code></p>\n<p>通过CSS类名查找 <code>soup.select(&quot;.sister&quot;), soup.select(&quot;[class~=sister]&quot;)</code></p>\n<p>通过tag的id查找 <code>soup.select(&quot;#link1&quot;)</code></p>\n<p>通过是否存在某个属性查找 <code>soup.slect(&#39;a[href]&#39;)</code></p>\n<p>通过属性的值来查找 <code>soup.select(&#39;a[href^=&quot;https:&quot;]&#39;, [href$=&#39;title&#39;], [href*=&quot;.com/&quot;]</code></p>\n<h2 id=\"修改文档树\"><a href=\"#修改文档树\" class=\"headerlink\" title=\"修改文档树\"></a>修改文档树</h2><h2 id=\"输出\"><a href=\"#输出\" class=\"headerlink\" title=\"输出\"></a>输出</h2><h3 id=\"格式化输出\"><a href=\"#格式化输出\" class=\"headerlink\" title=\"格式化输出\"></a>格式化输出</h3><p>prettify() 方法将Beautiful Soup的文档树格式化后以Unicode编码输出,每个XML/HTML标签都独占一行</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">markup = <span class=\"string\">'&lt;a href=\"http://example.com/\"&gt;I linked to &lt;i&gt;example.com&lt;/i&gt;&lt;/a&gt;'</span></span><br><span class=\"line\">soup = BeautifulSoup(markup)</span><br><span class=\"line\">soup.prettify()</span><br><span class=\"line\"><span class=\"comment\"># '&lt;html&gt;\\n &lt;head&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n  &lt;a href=\"http://example.com/\"&gt;\\n...'</span></span><br><span class=\"line\"></span><br><span class=\"line\">print(soup.prettify())</span><br><span class=\"line\"><span class=\"comment\"># &lt;html&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;head&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;/head&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;body&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#   &lt;a href=\"http://example.com/\"&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#    I linked to</span></span><br><span class=\"line\"><span class=\"comment\">#    &lt;i&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#     example.com</span></span><br><span class=\"line\"><span class=\"comment\">#    &lt;/i&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#   &lt;/a&gt;</span></span><br><span class=\"line\"><span class=\"comment\">#  &lt;/body&gt;</span></span><br><span class=\"line\"><span class=\"comment\"># &lt;/html&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"压缩输出\"><a href=\"#压缩输出\" class=\"headerlink\" title=\"压缩输出\"></a>压缩输出</h3><p>如果只想得到结果字符串,不重视格式,那么可以对一个 BeautifulSoup 对象或 Tag 对象使用Python的 unicode() 或 str() 方法:</p>\n<p><code>str(soup), Unicode(soup.a)</code></p>\n<h3 id=\"输出格式\"><a href=\"#输出格式\" class=\"headerlink\" title=\"输出格式\"></a>输出格式</h3><p>Beautiful Soup输出是会将HTML中的特殊字符转换成Unicode,比如“\\&lquot;”:<br>如果将文档转换成字符串,Unicode编码会被编码成UTF-8.这样就无法正确显示HTML特殊字符了:</p>\n<h3 id=\"get-text\"><a href=\"#get-text\" class=\"headerlink\" title=\"get_text()\"></a>get_text()</h3><p>如果只想得到tag中包含的文本内容,那么可以用 get_text() 方法,这个方法获取到tag中包含的所有文本内容包括子孙tag中的内容,并将结果作为Unicode字符串返回:</p>\n<p>可以通过参数指定tag的文本内容的分隔符, 还可以去除前后空白: <code>soup.get_text(&#39;|&#39;, strip=True)</code></p>\n<h2 id=\"指定文档解析器\"><a href=\"#指定文档解析器\" class=\"headerlink\" title=\"指定文档解析器\"></a>指定文档解析器</h2><h2 id=\"编码\"><a href=\"#编码\" class=\"headerlink\" title=\"编码\"></a>编码</h2>"},{"title":"DNN应用1--识别猫","date":"2018-08-03T00:52:05.000Z","_content":"## 实验目的\n\n使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。\n\n## 实验方案\n\n- 使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。\n- 各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理\n- 损失函数采用交叉熵，并使用L2正则化\n- 网络架构\n ![](/images/LlayerNN.png)\n\n## 详细设计\n\n### 数据预处理\n\n#### 加载数据\n\n```python\nimport numpy as np\nimport h5py\n\ndef load_data():\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n```\n\n```python\ntrain_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n```\n\n#### 数据集形状\n\n>Number of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n\n#### 展示数据图片\n\n```python\nimport matplotlib.pyplot as plt\n\nindex = 7\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\nplt.show()\n```\n\n#### 图像矩阵向量化\n\n```python\n# Reshape the training and test examples\ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n```\n\n#### 数据集最终形状\n\n>train_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)\n\n### 网络设计\n\n1. 初始化参数 / 定义超参数\n2. 迭代循环:\n    a. 前向传播\n    b. 计算代价函数\n    c. 反向传播\n    d. 更新参数\n3. 使用训练的参数去预测新的数据标签\n\n网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 实验结果\n\n### 训练集结果\n\n```python\nlayers_dims = [12288, 20, 7, 5, 1] #  5-layer model\nparameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n```\n\n![](/images/res1.PNG)\n\n```python\npred_train = predict(train_x, train_y, parameters)\n```\n\n>Accuracy: 0.9856459330143539\n\n### 测试集结果\n\n```python\npred_test = predict(test_x, test_y, parameters)\n```\n\n>Accuracy: 0.8\n\n### 数据集外结果\n\n```python\nfrom scipy import ndimage\nimport scipy.misc\n\nmy_image = \"my_image.jpg\"\nmy_label_y = [0]\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * 3, 1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\nplt.imshow(image)\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)), ].decode(\"utf-8\") + \"\\\" picture.\")\n```\n\n>Accuracy: 1.0\n>y = 1.0, your L-layer model predicts a \"cat\" picture.\n\n![](/images/my_image.jpg)\n","source":"_posts/DNN应用1-识别猫.md","raw":"---\ntitle: DNN应用1--识别猫\ndate: 2018-08-03 08:52:05\ntags: DNN\ncategories: 深度学习\n---\n## 实验目的\n\n使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。\n\n## 实验方案\n\n- 使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。\n- 各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理\n- 损失函数采用交叉熵，并使用L2正则化\n- 网络架构\n ![](/images/LlayerNN.png)\n\n## 详细设计\n\n### 数据预处理\n\n#### 加载数据\n\n```python\nimport numpy as np\nimport h5py\n\ndef load_data():\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n```\n\n```python\ntrain_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n```\n\n#### 数据集形状\n\n>Number of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n\n#### 展示数据图片\n\n```python\nimport matplotlib.pyplot as plt\n\nindex = 7\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\nplt.show()\n```\n\n#### 图像矩阵向量化\n\n```python\n# Reshape the training and test examples\ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n```\n\n#### 数据集最终形状\n\n>train_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)\n\n### 网络设计\n\n1. 初始化参数 / 定义超参数\n2. 迭代循环:\n    a. 前向传播\n    b. 计算代价函数\n    c. 反向传播\n    d. 更新参数\n3. 使用训练的参数去预测新的数据标签\n\n网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 实验结果\n\n### 训练集结果\n\n```python\nlayers_dims = [12288, 20, 7, 5, 1] #  5-layer model\nparameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)\n```\n\n![](/images/res1.PNG)\n\n```python\npred_train = predict(train_x, train_y, parameters)\n```\n\n>Accuracy: 0.9856459330143539\n\n### 测试集结果\n\n```python\npred_test = predict(test_x, test_y, parameters)\n```\n\n>Accuracy: 0.8\n\n### 数据集外结果\n\n```python\nfrom scipy import ndimage\nimport scipy.misc\n\nmy_image = \"my_image.jpg\"\nmy_label_y = [0]\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * 3, 1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\nplt.imshow(image)\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)), ].decode(\"utf-8\") + \"\\\" picture.\")\n```\n\n>Accuracy: 1.0\n>y = 1.0, your L-layer model predicts a \"cat\" picture.\n\n![](/images/my_image.jpg)\n","slug":"DNN应用1-识别猫","published":1,"updated":"2018-08-31T03:48:46.687Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh150002zkvo5xrne7jg","content":"<h2 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h2><p>使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。</p>\n<h2 id=\"实验方案\"><a href=\"#实验方案\" class=\"headerlink\" title=\"实验方案\"></a>实验方案</h2><ul>\n<li>使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。</li>\n<li>各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理</li>\n<li>损失函数采用交叉熵，并使用L2正则化</li>\n<li>网络架构<br><img src=\"/images/LlayerNN.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"详细设计\"><a href=\"#详细设计\" class=\"headerlink\" title=\"详细设计\"></a>详细设计</h2><h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><h4 id=\"加载数据\"><a href=\"#加载数据\" class=\"headerlink\" title=\"加载数据\"></a>加载数据</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> h5py</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:])  <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:])  <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:])  <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:])  <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:])  <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集形状\"><a href=\"#数据集形状\" class=\"headerlink\" title=\"数据集形状\"></a>数据集形状</h4><blockquote>\n<p>Number of training examples: 209<br>Number of testing examples: 50<br>Each image is of size: (64, 64, 3)<br>train_x_orig shape: (209, 64, 64, 3)<br>train_y shape: (1, 209)<br>test_x_orig shape: (50, 64, 64, 3)<br>test_y shape: (1, 50)</p>\n</blockquote>\n<h4 id=\"展示数据图片\"><a href=\"#展示数据图片\" class=\"headerlink\" title=\"展示数据图片\"></a>展示数据图片</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">index = <span class=\"number\">7</span></span><br><span class=\"line\">plt.imshow(train_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_y[<span class=\"number\">0</span>,index]) + <span class=\"string\">\". It's a \"</span> + classes[train_y[<span class=\"number\">0</span>,index]].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\" picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h4 id=\"图像矩阵向量化\"><a href=\"#图像矩阵向量化\" class=\"headerlink\" title=\"图像矩阵向量化\"></a>图像矩阵向量化</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Reshape the training and test examples</span></span><br><span class=\"line\">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T   <span class=\"comment\"># The \"-1\" makes reshape flatten the remaining dimensions</span></span><br><span class=\"line\">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Standardize data to have feature values between 0 and 1.</span></span><br><span class=\"line\">train_x = train_x_flatten/<span class=\"number\">255.</span></span><br><span class=\"line\">test_x = test_x_flatten/<span class=\"number\">255.</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集最终形状\"><a href=\"#数据集最终形状\" class=\"headerlink\" title=\"数据集最终形状\"></a>数据集最终形状</h4><blockquote>\n<p>train_x’s shape: (12288, 209)<br>test_x’s shape: (12288, 50)</p>\n</blockquote>\n<h3 id=\"网络设计\"><a href=\"#网络设计\" class=\"headerlink\" title=\"网络设计\"></a>网络设计</h3><ol>\n<li>初始化参数 / 定义超参数</li>\n<li>迭代循环:<br> a. 前向传播<br> b. 计算代价函数<br> c. 反向传播<br> d. 更新参数</li>\n<li>使用训练的参数去预测新的数据标签</li>\n</ol>\n<p>网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><h3 id=\"训练集结果\"><a href=\"#训练集结果\" class=\"headerlink\" title=\"训练集结果\"></a>训练集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layers_dims = [<span class=\"number\">12288</span>, <span class=\"number\">20</span>, <span class=\"number\">7</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>] <span class=\"comment\">#  5-layer model</span></span><br><span class=\"line\">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class=\"number\">2500</span>, print_cost = <span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/images/res1.PNG\" alt=\"\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.9856459330143539</p>\n</blockquote>\n<h3 id=\"测试集结果\"><a href=\"#测试集结果\" class=\"headerlink\" title=\"测试集结果\"></a>测试集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.8</p>\n</blockquote>\n<h3 id=\"数据集外结果\"><a href=\"#数据集外结果\" class=\"headerlink\" title=\"数据集外结果\"></a>数据集外结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy <span class=\"keyword\">import</span> ndimage</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.misc</span><br><span class=\"line\"></span><br><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">0</span>]</span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * <span class=\"number\">3</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\">print(<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)), ].decode(<span class=\"string\">\"utf-8\"</span>) + <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 1.0<br>y = 1.0, your L-layer model predicts a “cat” picture.</p>\n</blockquote>\n<p><img src=\"/images/my_image.jpg\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"实验目的\"><a href=\"#实验目的\" class=\"headerlink\" title=\"实验目的\"></a>实验目的</h2><p>使用深层全连接神经网络识别一副图片是否为猫，并将网络层数及每层单元数设为超参数。</p>\n<h2 id=\"实验方案\"><a href=\"#实验方案\" class=\"headerlink\" title=\"实验方案\"></a>实验方案</h2><ul>\n<li>使用python自行编码各运算单元，主要借助numpy库的数据结构和运算函数。</li>\n<li>各个隐藏层采用Relu激活函数，输出层采用Sigmod激活函数，隐藏层使用dropout处理</li>\n<li>损失函数采用交叉熵，并使用L2正则化</li>\n<li>网络架构<br><img src=\"/images/LlayerNN.png\" alt=\"\"></li>\n</ul>\n<h2 id=\"详细设计\"><a href=\"#详细设计\" class=\"headerlink\" title=\"详细设计\"></a>详细设计</h2><h3 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h3><h4 id=\"加载数据\"><a href=\"#加载数据\" class=\"headerlink\" title=\"加载数据\"></a>加载数据</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> h5py</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:])  <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:])  <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:])  <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:])  <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:])  <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集形状\"><a href=\"#数据集形状\" class=\"headerlink\" title=\"数据集形状\"></a>数据集形状</h4><blockquote>\n<p>Number of training examples: 209<br>Number of testing examples: 50<br>Each image is of size: (64, 64, 3)<br>train_x_orig shape: (209, 64, 64, 3)<br>train_y shape: (1, 209)<br>test_x_orig shape: (50, 64, 64, 3)<br>test_y shape: (1, 50)</p>\n</blockquote>\n<h4 id=\"展示数据图片\"><a href=\"#展示数据图片\" class=\"headerlink\" title=\"展示数据图片\"></a>展示数据图片</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">index = <span class=\"number\">7</span></span><br><span class=\"line\">plt.imshow(train_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_y[<span class=\"number\">0</span>,index]) + <span class=\"string\">\". It's a \"</span> + classes[train_y[<span class=\"number\">0</span>,index]].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\" picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h4 id=\"图像矩阵向量化\"><a href=\"#图像矩阵向量化\" class=\"headerlink\" title=\"图像矩阵向量化\"></a>图像矩阵向量化</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Reshape the training and test examples</span></span><br><span class=\"line\">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T   <span class=\"comment\"># The \"-1\" makes reshape flatten the remaining dimensions</span></span><br><span class=\"line\">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Standardize data to have feature values between 0 and 1.</span></span><br><span class=\"line\">train_x = train_x_flatten/<span class=\"number\">255.</span></span><br><span class=\"line\">test_x = test_x_flatten/<span class=\"number\">255.</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"数据集最终形状\"><a href=\"#数据集最终形状\" class=\"headerlink\" title=\"数据集最终形状\"></a>数据集最终形状</h4><blockquote>\n<p>train_x’s shape: (12288, 209)<br>test_x’s shape: (12288, 50)</p>\n</blockquote>\n<h3 id=\"网络设计\"><a href=\"#网络设计\" class=\"headerlink\" title=\"网络设计\"></a>网络设计</h3><ol>\n<li>初始化参数 / 定义超参数</li>\n<li>迭代循环:<br> a. 前向传播<br> b. 计算代价函数<br> c. 反向传播<br> d. 更新参数</li>\n<li>使用训练的参数去预测新的数据标签</li>\n</ol>\n<p>网络主框架代码，其他细节函数参见“神经网络中的通用函数代码”</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h2><h3 id=\"训练集结果\"><a href=\"#训练集结果\" class=\"headerlink\" title=\"训练集结果\"></a>训练集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layers_dims = [<span class=\"number\">12288</span>, <span class=\"number\">20</span>, <span class=\"number\">7</span>, <span class=\"number\">5</span>, <span class=\"number\">1</span>] <span class=\"comment\">#  5-layer model</span></span><br><span class=\"line\">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class=\"number\">2500</span>, print_cost = <span class=\"keyword\">True</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/images/res1.PNG\" alt=\"\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.9856459330143539</p>\n</blockquote>\n<h3 id=\"测试集结果\"><a href=\"#测试集结果\" class=\"headerlink\" title=\"测试集结果\"></a>测试集结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 0.8</p>\n</blockquote>\n<h3 id=\"数据集外结果\"><a href=\"#数据集外结果\" class=\"headerlink\" title=\"数据集外结果\"></a>数据集外结果</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy <span class=\"keyword\">import</span> ndimage</span><br><span class=\"line\"><span class=\"keyword\">import</span> scipy.misc</span><br><span class=\"line\"></span><br><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">0</span>]</span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px * num_px * <span class=\"number\">3</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\">print(<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)), ].decode(<span class=\"string\">\"utf-8\"</span>) + <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>Accuracy: 1.0<br>y = 1.0, your L-layer model predicts a “cat” picture.</p>\n</blockquote>\n<p><img src=\"/images/my_image.jpg\" alt=\"\"></p>\n"},{"title":"Evolutionary Algorithms","date":"2018-08-05T09:12:46.000Z","_content":"\n>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. -- Charles Darwin\n\n## Motivation of EAs\n\n1. What can EAs do for us?\n\t- Optimization\n\t- Help people understand the evolution in nature.\n\n2. What is optimizatin?\n\t- The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain **performance criteria**\n\n3. Produce maximum yields given litmited resources.\n\n## Key Concepts\n\n- Population-Based Stochastic Optimization Methods\n- Inherently Parallel\n- A Good Example of Bionics in Engineering\n- Survival of the Fittest\n- Chromosome, Crossover, Mutation\n- Metaheuristics\n- Bio-/Nature Inspired Computing\n\n## The Big Picture\n\n![](/images/eas.png)\n\n## EA Family\n\n- GA: Genetic Algorithm\n- GP: Genetic Programming\n- ES: Evolution Strategies\n- EP: Evolution Programming\n- EDA: Estimation of Distribution Algorithm\n- PSO: Particle Swarm Optimization\n- ACO: Ant Colony Optimization\n- DE: Differential Evolution\n\n## Optimization Problem Set\n\n- Portfolio Optimization\n- Travelling Salesman Problem\n- Knapsack Problem\n- Machine Learing Problems\n\n![](/images/local_optima.png)\n\nMany interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.\n\n## Solution: Parallel Search\n\n- Conduct searching in different areas simultaneously.\n\t- Population Based\n\t- Avoid unfortunate starting positions.\n- Employ heuristic methods to effectively explore the space.\n\t- Focus on promising areas.\n\t- Also keep an eye on other regions.\n\t- More than random restart strategies.\n\n## Publications\n\nTop Journals:\n- IEEE Transactions On Evolutionary Computation.\n- Evolutionary Compution Journal\n\nMajor Conference:\n- IEEE Congress On Evolution Computation(CEC)\n- Genetic and Evolution Computation Conference(GECCO)\n- Parallel Problem Solving from Nature(PPSN)\n\nGame:\n\t- Blondie24: Playing at the Edge of AI\n\nBook:\n\t- How to Solve It: Modern Heuristics\n","source":"_posts/Evolutionary-Algorithms.md","raw":"---\ntitle: Evolutionary Algorithms\ndate: 2018-08-05 17:12:46\ntags: 进化算法\ncategories: 进化计算\n---\n\n>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. -- Charles Darwin\n\n## Motivation of EAs\n\n1. What can EAs do for us?\n\t- Optimization\n\t- Help people understand the evolution in nature.\n\n2. What is optimizatin?\n\t- The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain **performance criteria**\n\n3. Produce maximum yields given litmited resources.\n\n## Key Concepts\n\n- Population-Based Stochastic Optimization Methods\n- Inherently Parallel\n- A Good Example of Bionics in Engineering\n- Survival of the Fittest\n- Chromosome, Crossover, Mutation\n- Metaheuristics\n- Bio-/Nature Inspired Computing\n\n## The Big Picture\n\n![](/images/eas.png)\n\n## EA Family\n\n- GA: Genetic Algorithm\n- GP: Genetic Programming\n- ES: Evolution Strategies\n- EP: Evolution Programming\n- EDA: Estimation of Distribution Algorithm\n- PSO: Particle Swarm Optimization\n- ACO: Ant Colony Optimization\n- DE: Differential Evolution\n\n## Optimization Problem Set\n\n- Portfolio Optimization\n- Travelling Salesman Problem\n- Knapsack Problem\n- Machine Learing Problems\n\n![](/images/local_optima.png)\n\nMany interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.\n\n## Solution: Parallel Search\n\n- Conduct searching in different areas simultaneously.\n\t- Population Based\n\t- Avoid unfortunate starting positions.\n- Employ heuristic methods to effectively explore the space.\n\t- Focus on promising areas.\n\t- Also keep an eye on other regions.\n\t- More than random restart strategies.\n\n## Publications\n\nTop Journals:\n- IEEE Transactions On Evolutionary Computation.\n- Evolutionary Compution Journal\n\nMajor Conference:\n- IEEE Congress On Evolution Computation(CEC)\n- Genetic and Evolution Computation Conference(GECCO)\n- Parallel Problem Solving from Nature(PPSN)\n\nGame:\n\t- Blondie24: Playing at the Edge of AI\n\nBook:\n\t- How to Solve It: Modern Heuristics\n","slug":"Evolutionary-Algorithms","published":1,"updated":"2018-08-19T01:59:35.787Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh1m0006zkvopbc79g5w","content":"<blockquote>\n<p>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. – Charles Darwin</p>\n</blockquote>\n<h2 id=\"Motivation-of-EAs\"><a href=\"#Motivation-of-EAs\" class=\"headerlink\" title=\"Motivation of EAs\"></a>Motivation of EAs</h2><ol>\n<li><p>What can EAs do for us?</p>\n<ul>\n<li>Optimization</li>\n<li>Help people understand the evolution in nature.</li>\n</ul>\n</li>\n<li><p>What is optimizatin?</p>\n<ul>\n<li>The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain <strong>performance criteria</strong></li>\n</ul>\n</li>\n<li><p>Produce maximum yields given litmited resources.</p>\n</li>\n</ol>\n<h2 id=\"Key-Concepts\"><a href=\"#Key-Concepts\" class=\"headerlink\" title=\"Key Concepts\"></a>Key Concepts</h2><ul>\n<li>Population-Based Stochastic Optimization Methods</li>\n<li>Inherently Parallel</li>\n<li>A Good Example of Bionics in Engineering</li>\n<li>Survival of the Fittest</li>\n<li>Chromosome, Crossover, Mutation</li>\n<li>Metaheuristics</li>\n<li>Bio-/Nature Inspired Computing</li>\n</ul>\n<h2 id=\"The-Big-Picture\"><a href=\"#The-Big-Picture\" class=\"headerlink\" title=\"The Big Picture\"></a>The Big Picture</h2><p><img src=\"/images/eas.png\" alt=\"\"></p>\n<h2 id=\"EA-Family\"><a href=\"#EA-Family\" class=\"headerlink\" title=\"EA Family\"></a>EA Family</h2><ul>\n<li>GA: Genetic Algorithm</li>\n<li>GP: Genetic Programming</li>\n<li>ES: Evolution Strategies</li>\n<li>EP: Evolution Programming</li>\n<li>EDA: Estimation of Distribution Algorithm</li>\n<li>PSO: Particle Swarm Optimization</li>\n<li>ACO: Ant Colony Optimization</li>\n<li>DE: Differential Evolution</li>\n</ul>\n<h2 id=\"Optimization-Problem-Set\"><a href=\"#Optimization-Problem-Set\" class=\"headerlink\" title=\"Optimization Problem Set\"></a>Optimization Problem Set</h2><ul>\n<li>Portfolio Optimization</li>\n<li>Travelling Salesman Problem</li>\n<li>Knapsack Problem</li>\n<li>Machine Learing Problems</li>\n</ul>\n<p><img src=\"/images/local_optima.png\" alt=\"\"></p>\n<p>Many interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.</p>\n<h2 id=\"Solution-Parallel-Search\"><a href=\"#Solution-Parallel-Search\" class=\"headerlink\" title=\"Solution: Parallel Search\"></a>Solution: Parallel Search</h2><ul>\n<li>Conduct searching in different areas simultaneously.<ul>\n<li>Population Based</li>\n<li>Avoid unfortunate starting positions.</li>\n</ul>\n</li>\n<li>Employ heuristic methods to effectively explore the space.<ul>\n<li>Focus on promising areas.</li>\n<li>Also keep an eye on other regions.</li>\n<li>More than random restart strategies.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><p>Top Journals:</p>\n<ul>\n<li>IEEE Transactions On Evolutionary Computation.</li>\n<li>Evolutionary Compution Journal</li>\n</ul>\n<p>Major Conference:</p>\n<ul>\n<li>IEEE Congress On Evolution Computation(CEC)</li>\n<li>Genetic and Evolution Computation Conference(GECCO)</li>\n<li>Parallel Problem Solving from Nature(PPSN)</li>\n</ul>\n<p>Game:</p>\n<pre><code>- Blondie24: Playing at the Edge of AI\n</code></pre><p>Book:</p>\n<pre><code>- How to Solve It: Modern Heuristics\n</code></pre>","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>It is not strongest of the species that survives, nor the most intelligent that survives. It is the one that is the most adaptable to change. – Charles Darwin</p>\n</blockquote>\n<h2 id=\"Motivation-of-EAs\"><a href=\"#Motivation-of-EAs\" class=\"headerlink\" title=\"Motivation of EAs\"></a>Motivation of EAs</h2><ol>\n<li><p>What can EAs do for us?</p>\n<ul>\n<li>Optimization</li>\n<li>Help people understand the evolution in nature.</li>\n</ul>\n</li>\n<li><p>What is optimizatin?</p>\n<ul>\n<li>The process of searching for the optimal solution from a set of candidates to the problem of interest based on certrain <strong>performance criteria</strong></li>\n</ul>\n</li>\n<li><p>Produce maximum yields given litmited resources.</p>\n</li>\n</ol>\n<h2 id=\"Key-Concepts\"><a href=\"#Key-Concepts\" class=\"headerlink\" title=\"Key Concepts\"></a>Key Concepts</h2><ul>\n<li>Population-Based Stochastic Optimization Methods</li>\n<li>Inherently Parallel</li>\n<li>A Good Example of Bionics in Engineering</li>\n<li>Survival of the Fittest</li>\n<li>Chromosome, Crossover, Mutation</li>\n<li>Metaheuristics</li>\n<li>Bio-/Nature Inspired Computing</li>\n</ul>\n<h2 id=\"The-Big-Picture\"><a href=\"#The-Big-Picture\" class=\"headerlink\" title=\"The Big Picture\"></a>The Big Picture</h2><p><img src=\"/images/eas.png\" alt=\"\"></p>\n<h2 id=\"EA-Family\"><a href=\"#EA-Family\" class=\"headerlink\" title=\"EA Family\"></a>EA Family</h2><ul>\n<li>GA: Genetic Algorithm</li>\n<li>GP: Genetic Programming</li>\n<li>ES: Evolution Strategies</li>\n<li>EP: Evolution Programming</li>\n<li>EDA: Estimation of Distribution Algorithm</li>\n<li>PSO: Particle Swarm Optimization</li>\n<li>ACO: Ant Colony Optimization</li>\n<li>DE: Differential Evolution</li>\n</ul>\n<h2 id=\"Optimization-Problem-Set\"><a href=\"#Optimization-Problem-Set\" class=\"headerlink\" title=\"Optimization Problem Set\"></a>Optimization Problem Set</h2><ul>\n<li>Portfolio Optimization</li>\n<li>Travelling Salesman Problem</li>\n<li>Knapsack Problem</li>\n<li>Machine Learing Problems</li>\n</ul>\n<p><img src=\"/images/local_optima.png\" alt=\"\"></p>\n<p>Many interesting optimization problems are not trivial.The optimal solution cannot always be found in polynomial time.</p>\n<h2 id=\"Solution-Parallel-Search\"><a href=\"#Solution-Parallel-Search\" class=\"headerlink\" title=\"Solution: Parallel Search\"></a>Solution: Parallel Search</h2><ul>\n<li>Conduct searching in different areas simultaneously.<ul>\n<li>Population Based</li>\n<li>Avoid unfortunate starting positions.</li>\n</ul>\n</li>\n<li>Employ heuristic methods to effectively explore the space.<ul>\n<li>Focus on promising areas.</li>\n<li>Also keep an eye on other regions.</li>\n<li>More than random restart strategies.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><p>Top Journals:</p>\n<ul>\n<li>IEEE Transactions On Evolutionary Computation.</li>\n<li>Evolutionary Compution Journal</li>\n</ul>\n<p>Major Conference:</p>\n<ul>\n<li>IEEE Congress On Evolution Computation(CEC)</li>\n<li>Genetic and Evolution Computation Conference(GECCO)</li>\n<li>Parallel Problem Solving from Nature(PPSN)</li>\n</ul>\n<p>Game:</p>\n<pre><code>- Blondie24: Playing at the Edge of AI\n</code></pre><p>Book:</p>\n<pre><code>- How to Solve It: Modern Heuristics\n</code></pre>"},{"title":"Genetic Algorithms","date":"2018-08-05T14:14:07.000Z","_content":"\n## Biology Background\n\n- Gene: A working subunit of DNA\n- Gene Trait(性状基因): For example colour of eyes\n- Allele(等位基因): Possible settings for a trait\n- Genotype(基因型): The actual genes carried by an individual\n- Phenotype(性状型): The physical characteristics into which genes are translated\n\n## Genetic Algorithms\n\nBy John Holland: \"Adaptation in Natural and Artificial Systems\"\n\n### Inspired by and (loosely) based on Darwin's Theory\n\n- Chromosome(染色体)\n- Crossover(交叉)\n- Mutation(变异)\n- Selection(Survival of the Fittest)\n\n### Basic Ideas\n\n- Each solution to the problem is represented as a chromosome.\n- The initial solutions may be randomly generated.\n- Solution are evolved during generations.\n- Improved gradually based on the principle of natural evolution.\n\n### Basic Components\n\n#### Representation\n\n- How to encode the parameters of the problem?\n- Binary Problems\n- Continuous Problems\n\n1. Individual(Chromosome)\n\nA vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.\n\n2. Population\n\nA set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.\n\n3. Offspring\n\nNew individuals generated via genetic operators. Hopefully contain better solutions.\n\n4. Encoding\n\nBinary vs. Gray. How to encode TSP problems?\n\n#### Genetic Operators\n\n1. Crossover:\n\tExchange genetic materials between two chromosomes.\n\t- One Point Crossover\n\t- Two Point Crossover\n\t- Uniform Crossover\n \n2. Mutation:\n\tRandomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.\n\n#### Selection Strategy\n\n- Which chromosomes should be involved in reproduction?\n- Which offspring should be able to survive?\n\n1. Roulette Wheel Selection: 根据适应度的高低按比例选择\n2. Rank Selection： 根据排名按固定比例选择\n3. Tournament Selection: 两两及以上互相竞争\n4. Elitism: 精英保留直接拷贝到下一代\n5. Offspring Selection: 子代直接进入下一代还是与父代一起竞争\n\n### Selection vs. Crossover vs. Mutation\n\n- Selection:\n\t- Bias the search effort towards promising individuals.\n\t- Loss of genetic diversity\n\n- Cossover:\n\t- Create better individuals by combining genes from good individuals\n\t- Building Block Hypothesis\n\t- Major search power of GAs\n\t- No effect on genetic diversity\n\n- Mutation:\n\t- Increase genetic diversity\n\t- Force the algorithm to search areas other than the current focus.\n\n**It is a trade off about Exploration vs. Exploitation**\n\n## GA Framework\n\n1. Intialization: Generate a random population P of M individuals\n2. Evaluation: Evaluate the fitness f(x) of each individual\n3. Repeat until the stopping criteria are met:\n\t1. Reproduction: Repeat the following steps until all offspring are generated\n\t\t1. Paraent Selection: Select two parents from P\n\t\t2. Crossover: Apply crossover on the parents with probability P_c\n\t\t3. Mutation: Apply mutation on offspring with probability P_m\n\t\t4. Evaluation: Evaluate the newly generated offspring\n\t2. Offspring Selection: Create a new population from oddspring and P\n\t3. Output: Return the best individual found\n\n## Parameters\n\n- Population Size:\n\tToo big: Slow convergence rate. Too small: Premature convergence\n\n- Crossover Rate:\n\tRecommended value: 0.8\n\n- Mutation Rate:\n\tRecommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.\n\n- Selection Strategy:\n\tTournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.\n","source":"_posts/Genetic-Algorithms.md","raw":"---\ntitle: Genetic Algorithms\ndate: 2018-08-05 22:14:07\ntags: 遗传算法\ncategories: 进化计算\n---\n\n## Biology Background\n\n- Gene: A working subunit of DNA\n- Gene Trait(性状基因): For example colour of eyes\n- Allele(等位基因): Possible settings for a trait\n- Genotype(基因型): The actual genes carried by an individual\n- Phenotype(性状型): The physical characteristics into which genes are translated\n\n## Genetic Algorithms\n\nBy John Holland: \"Adaptation in Natural and Artificial Systems\"\n\n### Inspired by and (loosely) based on Darwin's Theory\n\n- Chromosome(染色体)\n- Crossover(交叉)\n- Mutation(变异)\n- Selection(Survival of the Fittest)\n\n### Basic Ideas\n\n- Each solution to the problem is represented as a chromosome.\n- The initial solutions may be randomly generated.\n- Solution are evolved during generations.\n- Improved gradually based on the principle of natural evolution.\n\n### Basic Components\n\n#### Representation\n\n- How to encode the parameters of the problem?\n- Binary Problems\n- Continuous Problems\n\n1. Individual(Chromosome)\n\nA vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.\n\n2. Population\n\nA set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.\n\n3. Offspring\n\nNew individuals generated via genetic operators. Hopefully contain better solutions.\n\n4. Encoding\n\nBinary vs. Gray. How to encode TSP problems?\n\n#### Genetic Operators\n\n1. Crossover:\n\tExchange genetic materials between two chromosomes.\n\t- One Point Crossover\n\t- Two Point Crossover\n\t- Uniform Crossover\n \n2. Mutation:\n\tRandomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.\n\n#### Selection Strategy\n\n- Which chromosomes should be involved in reproduction?\n- Which offspring should be able to survive?\n\n1. Roulette Wheel Selection: 根据适应度的高低按比例选择\n2. Rank Selection： 根据排名按固定比例选择\n3. Tournament Selection: 两两及以上互相竞争\n4. Elitism: 精英保留直接拷贝到下一代\n5. Offspring Selection: 子代直接进入下一代还是与父代一起竞争\n\n### Selection vs. Crossover vs. Mutation\n\n- Selection:\n\t- Bias the search effort towards promising individuals.\n\t- Loss of genetic diversity\n\n- Cossover:\n\t- Create better individuals by combining genes from good individuals\n\t- Building Block Hypothesis\n\t- Major search power of GAs\n\t- No effect on genetic diversity\n\n- Mutation:\n\t- Increase genetic diversity\n\t- Force the algorithm to search areas other than the current focus.\n\n**It is a trade off about Exploration vs. Exploitation**\n\n## GA Framework\n\n1. Intialization: Generate a random population P of M individuals\n2. Evaluation: Evaluate the fitness f(x) of each individual\n3. Repeat until the stopping criteria are met:\n\t1. Reproduction: Repeat the following steps until all offspring are generated\n\t\t1. Paraent Selection: Select two parents from P\n\t\t2. Crossover: Apply crossover on the parents with probability P_c\n\t\t3. Mutation: Apply mutation on offspring with probability P_m\n\t\t4. Evaluation: Evaluate the newly generated offspring\n\t2. Offspring Selection: Create a new population from oddspring and P\n\t3. Output: Return the best individual found\n\n## Parameters\n\n- Population Size:\n\tToo big: Slow convergence rate. Too small: Premature convergence\n\n- Crossover Rate:\n\tRecommended value: 0.8\n\n- Mutation Rate:\n\tRecommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.\n\n- Selection Strategy:\n\tTournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.\n","slug":"Genetic-Algorithms","published":1,"updated":"2018-08-19T01:59:35.788Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh1q0007zkvoha9k7jtz","content":"<h2 id=\"Biology-Background\"><a href=\"#Biology-Background\" class=\"headerlink\" title=\"Biology Background\"></a>Biology Background</h2><ul>\n<li>Gene: A working subunit of DNA</li>\n<li>Gene Trait(性状基因): For example colour of eyes</li>\n<li>Allele(等位基因): Possible settings for a trait</li>\n<li>Genotype(基因型): The actual genes carried by an individual</li>\n<li>Phenotype(性状型): The physical characteristics into which genes are translated</li>\n</ul>\n<h2 id=\"Genetic-Algorithms\"><a href=\"#Genetic-Algorithms\" class=\"headerlink\" title=\"Genetic Algorithms\"></a>Genetic Algorithms</h2><p>By John Holland: “Adaptation in Natural and Artificial Systems”</p>\n<h3 id=\"Inspired-by-and-loosely-based-on-Darwin’s-Theory\"><a href=\"#Inspired-by-and-loosely-based-on-Darwin’s-Theory\" class=\"headerlink\" title=\"Inspired by and (loosely) based on Darwin’s Theory\"></a>Inspired by and (loosely) based on Darwin’s Theory</h3><ul>\n<li>Chromosome(染色体)</li>\n<li>Crossover(交叉)</li>\n<li>Mutation(变异)</li>\n<li>Selection(Survival of the Fittest)</li>\n</ul>\n<h3 id=\"Basic-Ideas\"><a href=\"#Basic-Ideas\" class=\"headerlink\" title=\"Basic Ideas\"></a>Basic Ideas</h3><ul>\n<li>Each solution to the problem is represented as a chromosome.</li>\n<li>The initial solutions may be randomly generated.</li>\n<li>Solution are evolved during generations.</li>\n<li>Improved gradually based on the principle of natural evolution.</li>\n</ul>\n<h3 id=\"Basic-Components\"><a href=\"#Basic-Components\" class=\"headerlink\" title=\"Basic Components\"></a>Basic Components</h3><h4 id=\"Representation\"><a href=\"#Representation\" class=\"headerlink\" title=\"Representation\"></a>Representation</h4><ul>\n<li>How to encode the parameters of the problem?</li>\n<li>Binary Problems</li>\n<li>Continuous Problems</li>\n</ul>\n<ol>\n<li>Individual(Chromosome)</li>\n</ol>\n<p>A vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.</p>\n<ol start=\"2\">\n<li>Population</li>\n</ol>\n<p>A set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.</p>\n<ol start=\"3\">\n<li>Offspring</li>\n</ol>\n<p>New individuals generated via genetic operators. Hopefully contain better solutions.</p>\n<ol start=\"4\">\n<li>Encoding</li>\n</ol>\n<p>Binary vs. Gray. How to encode TSP problems?</p>\n<h4 id=\"Genetic-Operators\"><a href=\"#Genetic-Operators\" class=\"headerlink\" title=\"Genetic Operators\"></a>Genetic Operators</h4><ol>\n<li><p>Crossover:<br> Exchange genetic materials between two chromosomes.</p>\n<ul>\n<li>One Point Crossover</li>\n<li>Two Point Crossover</li>\n<li>Uniform Crossover</li>\n</ul>\n</li>\n<li><p>Mutation:<br> Randomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.</p>\n</li>\n</ol>\n<h4 id=\"Selection-Strategy\"><a href=\"#Selection-Strategy\" class=\"headerlink\" title=\"Selection Strategy\"></a>Selection Strategy</h4><ul>\n<li>Which chromosomes should be involved in reproduction?</li>\n<li>Which offspring should be able to survive?</li>\n</ul>\n<ol>\n<li>Roulette Wheel Selection: 根据适应度的高低按比例选择</li>\n<li>Rank Selection： 根据排名按固定比例选择</li>\n<li>Tournament Selection: 两两及以上互相竞争</li>\n<li>Elitism: 精英保留直接拷贝到下一代</li>\n<li>Offspring Selection: 子代直接进入下一代还是与父代一起竞争</li>\n</ol>\n<h3 id=\"Selection-vs-Crossover-vs-Mutation\"><a href=\"#Selection-vs-Crossover-vs-Mutation\" class=\"headerlink\" title=\"Selection vs. Crossover vs. Mutation\"></a>Selection vs. Crossover vs. Mutation</h3><ul>\n<li><p>Selection:</p>\n<ul>\n<li>Bias the search effort towards promising individuals.</li>\n<li>Loss of genetic diversity</li>\n</ul>\n</li>\n<li><p>Cossover:</p>\n<ul>\n<li>Create better individuals by combining genes from good individuals</li>\n<li>Building Block Hypothesis</li>\n<li>Major search power of GAs</li>\n<li>No effect on genetic diversity</li>\n</ul>\n</li>\n<li><p>Mutation:</p>\n<ul>\n<li>Increase genetic diversity</li>\n<li>Force the algorithm to search areas other than the current focus.</li>\n</ul>\n</li>\n</ul>\n<p><strong>It is a trade off about Exploration vs. Exploitation</strong></p>\n<h2 id=\"GA-Framework\"><a href=\"#GA-Framework\" class=\"headerlink\" title=\"GA Framework\"></a>GA Framework</h2><ol>\n<li>Intialization: Generate a random population P of M individuals</li>\n<li>Evaluation: Evaluate the fitness f(x) of each individual</li>\n<li>Repeat until the stopping criteria are met:<ol>\n<li>Reproduction: Repeat the following steps until all offspring are generated<ol>\n<li>Paraent Selection: Select two parents from P</li>\n<li>Crossover: Apply crossover on the parents with probability P_c</li>\n<li>Mutation: Apply mutation on offspring with probability P_m</li>\n<li>Evaluation: Evaluate the newly generated offspring</li>\n</ol>\n</li>\n<li>Offspring Selection: Create a new population from oddspring and P</li>\n<li>Output: Return the best individual found</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"Parameters\"><a href=\"#Parameters\" class=\"headerlink\" title=\"Parameters\"></a>Parameters</h2><ul>\n<li><p>Population Size:<br>  Too big: Slow convergence rate. Too small: Premature convergence</p>\n</li>\n<li><p>Crossover Rate:<br>  Recommended value: 0.8</p>\n</li>\n<li><p>Mutation Rate:<br>  Recommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.</p>\n</li>\n<li><p>Selection Strategy:<br>  Tournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Biology-Background\"><a href=\"#Biology-Background\" class=\"headerlink\" title=\"Biology Background\"></a>Biology Background</h2><ul>\n<li>Gene: A working subunit of DNA</li>\n<li>Gene Trait(性状基因): For example colour of eyes</li>\n<li>Allele(等位基因): Possible settings for a trait</li>\n<li>Genotype(基因型): The actual genes carried by an individual</li>\n<li>Phenotype(性状型): The physical characteristics into which genes are translated</li>\n</ul>\n<h2 id=\"Genetic-Algorithms\"><a href=\"#Genetic-Algorithms\" class=\"headerlink\" title=\"Genetic Algorithms\"></a>Genetic Algorithms</h2><p>By John Holland: “Adaptation in Natural and Artificial Systems”</p>\n<h3 id=\"Inspired-by-and-loosely-based-on-Darwin’s-Theory\"><a href=\"#Inspired-by-and-loosely-based-on-Darwin’s-Theory\" class=\"headerlink\" title=\"Inspired by and (loosely) based on Darwin’s Theory\"></a>Inspired by and (loosely) based on Darwin’s Theory</h3><ul>\n<li>Chromosome(染色体)</li>\n<li>Crossover(交叉)</li>\n<li>Mutation(变异)</li>\n<li>Selection(Survival of the Fittest)</li>\n</ul>\n<h3 id=\"Basic-Ideas\"><a href=\"#Basic-Ideas\" class=\"headerlink\" title=\"Basic Ideas\"></a>Basic Ideas</h3><ul>\n<li>Each solution to the problem is represented as a chromosome.</li>\n<li>The initial solutions may be randomly generated.</li>\n<li>Solution are evolved during generations.</li>\n<li>Improved gradually based on the principle of natural evolution.</li>\n</ul>\n<h3 id=\"Basic-Components\"><a href=\"#Basic-Components\" class=\"headerlink\" title=\"Basic Components\"></a>Basic Components</h3><h4 id=\"Representation\"><a href=\"#Representation\" class=\"headerlink\" title=\"Representation\"></a>Representation</h4><ul>\n<li>How to encode the parameters of the problem?</li>\n<li>Binary Problems</li>\n<li>Continuous Problems</li>\n</ul>\n<ol>\n<li>Individual(Chromosome)</li>\n</ol>\n<p>A vector that represents a specific solution to the problem.Each element on the vector corresponds to a certain variable/parameter.</p>\n<ol start=\"2\">\n<li>Population</li>\n</ol>\n<p>A set of individuals, GAs maintain and evolve a population of individuals, Parallel Search to get Global Optimization.</p>\n<ol start=\"3\">\n<li>Offspring</li>\n</ol>\n<p>New individuals generated via genetic operators. Hopefully contain better solutions.</p>\n<ol start=\"4\">\n<li>Encoding</li>\n</ol>\n<p>Binary vs. Gray. How to encode TSP problems?</p>\n<h4 id=\"Genetic-Operators\"><a href=\"#Genetic-Operators\" class=\"headerlink\" title=\"Genetic Operators\"></a>Genetic Operators</h4><ol>\n<li><p>Crossover:<br> Exchange genetic materials between two chromosomes.</p>\n<ul>\n<li>One Point Crossover</li>\n<li>Two Point Crossover</li>\n<li>Uniform Crossover</li>\n</ul>\n</li>\n<li><p>Mutation:<br> Randomly modify gene values at selected locations.Mutation is mainly used to maintain the genetiv divesity.Loss of genetic diversity will result in Permature Convergence.</p>\n</li>\n</ol>\n<h4 id=\"Selection-Strategy\"><a href=\"#Selection-Strategy\" class=\"headerlink\" title=\"Selection Strategy\"></a>Selection Strategy</h4><ul>\n<li>Which chromosomes should be involved in reproduction?</li>\n<li>Which offspring should be able to survive?</li>\n</ul>\n<ol>\n<li>Roulette Wheel Selection: 根据适应度的高低按比例选择</li>\n<li>Rank Selection： 根据排名按固定比例选择</li>\n<li>Tournament Selection: 两两及以上互相竞争</li>\n<li>Elitism: 精英保留直接拷贝到下一代</li>\n<li>Offspring Selection: 子代直接进入下一代还是与父代一起竞争</li>\n</ol>\n<h3 id=\"Selection-vs-Crossover-vs-Mutation\"><a href=\"#Selection-vs-Crossover-vs-Mutation\" class=\"headerlink\" title=\"Selection vs. Crossover vs. Mutation\"></a>Selection vs. Crossover vs. Mutation</h3><ul>\n<li><p>Selection:</p>\n<ul>\n<li>Bias the search effort towards promising individuals.</li>\n<li>Loss of genetic diversity</li>\n</ul>\n</li>\n<li><p>Cossover:</p>\n<ul>\n<li>Create better individuals by combining genes from good individuals</li>\n<li>Building Block Hypothesis</li>\n<li>Major search power of GAs</li>\n<li>No effect on genetic diversity</li>\n</ul>\n</li>\n<li><p>Mutation:</p>\n<ul>\n<li>Increase genetic diversity</li>\n<li>Force the algorithm to search areas other than the current focus.</li>\n</ul>\n</li>\n</ul>\n<p><strong>It is a trade off about Exploration vs. Exploitation</strong></p>\n<h2 id=\"GA-Framework\"><a href=\"#GA-Framework\" class=\"headerlink\" title=\"GA Framework\"></a>GA Framework</h2><ol>\n<li>Intialization: Generate a random population P of M individuals</li>\n<li>Evaluation: Evaluate the fitness f(x) of each individual</li>\n<li>Repeat until the stopping criteria are met:<ol>\n<li>Reproduction: Repeat the following steps until all offspring are generated<ol>\n<li>Paraent Selection: Select two parents from P</li>\n<li>Crossover: Apply crossover on the parents with probability P_c</li>\n<li>Mutation: Apply mutation on offspring with probability P_m</li>\n<li>Evaluation: Evaluate the newly generated offspring</li>\n</ol>\n</li>\n<li>Offspring Selection: Create a new population from oddspring and P</li>\n<li>Output: Return the best individual found</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"Parameters\"><a href=\"#Parameters\" class=\"headerlink\" title=\"Parameters\"></a>Parameters</h2><ul>\n<li><p>Population Size:<br>  Too big: Slow convergence rate. Too small: Premature convergence</p>\n</li>\n<li><p>Crossover Rate:<br>  Recommended value: 0.8</p>\n</li>\n<li><p>Mutation Rate:<br>  Recommeded value: 1/L. Too big: Disrupt the evolution process. Too small: Not enough to maintain diversity.</p>\n</li>\n<li><p>Selection Strategy:<br>  Tournament Selection. Truncation Selection(Select top T individuals). Need to be careful about the selection pressure.</p>\n</li>\n</ul>\n"},{"title":"Gradient Descent Famliy","date":"2018-08-07T00:59:26.000Z","mathjax":true,"_content":"## (Batch) Gradient Descent\n\n``` python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost.\n    cost = compute_cost(a, Y)\n    # Backward propagation.\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters.\n    parameters = update_parameters(parameters, grads)     \n```\n\n## Stochastic Gradient Descent\n\n```python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost = compute_cost(a, Y[:,j])\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads)\n```\n\n## Mini-Batch Gradient descent\n\n- **Shuffle**:\n\n<img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\">\n\n- **Partition**:\n\n<img src=\"/images/partition.png\" style=\"width:550px;height:300px;\">\n\nNote that the last mini-batch might end up smaller than `mini_batch_size=64`. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is `math.floor(s)` in Python). If the total number of examples is not a multiple of `mini_batch_size=64` then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). \n\n```python\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    return mini_batches\n```\n\n## Momentum\n\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n\nMomentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. \n\n<img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\">\n<caption><center> <u><font color='purple'>**Figure 3**</u><font color='purple'>: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color='black'> </center>\n\n```python\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}    \n    # Initialize velocity\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape[0], parameters['W' + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape[0], parameters['b' + str(l+1)].shape[1]))\n        \n    return v\n```\n\n$$\\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}$$\n\n$$\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n\\end{cases}\\tag{4}$$\n\nwhere L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary.  Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that's a \"one\" on the superscript). So you will need to shift `l` to `l+1` when coding.\n\n```python\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks    \n    # Momentum update for each parameter\n    for l in range(L):        \n        # compute velocities\n        v[\"dW\" + str(l+1)] = beta * v['dW' + str(l+1)] + (1 - beta) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta * v['db' + str(l+1)] + (1 - beta) * grads['db' + str(l+1)]\n        # update parameters\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n    return parameters, v\n```\n\n\n**How do you choose $\\beta$?**\n\n- The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. \n- Common values for $\\beta$ range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. \n- Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. \n\n## Adam\n\nAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n\n**How does Adam work?**\n1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n\nThe update rule is, for $l = 1, ..., L$: \n\n<img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\">\n\nwhere:\n- t counts the number of steps taken of Adam \n- L is the number of layers\n- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n- $\\alpha$ is the learning rate\n- $\\varepsilon$ is a very small number to avoid dividing by zero\n\nAs usual, we will store all parameters in the `parameters` dictionary\n\n```python\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n    return v, s\n```\n\n```python\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients.\n        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)]\n        # Compute bias-corrected first moment estimate.\n        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t)\n        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t)\n        # Moving average of the squared gradients.\n        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads['dW' + str(l+1)] ** 2)\n        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads['db' + str(l+1)] ** 2)\n        # Compute bias-corrected second raw moment estimate.\n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n        # Update parameters. \n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n    return parameters, v, s\n```\n\n## Model with different optimization algorithms\n\n```python\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)  \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n    return parameters\n\ntrain_X, train_Y = load_dataset()\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n```\n## Summary\n\n![](/images/sgd.png)\n![](/images/minibatch.png)\n\n- **The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.**\n- **You have to tune a learning rate hyperparameter $\\alpha$.**\n- **With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).**\n- **Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.**\n\n- **Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.**\n\n- **Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster.**\n\n**Some advantages of Adam include:**\n- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n- Usually works well even with little tuning of hyperparameters (except $\\alpha$)","source":"_posts/Gradient-Descent-Famliy.md","raw":"---\ntitle: Gradient Descent Famliy\ndate: 2018-08-07 08:59:26\ntags: 优化算法\ncategories: 深度学习\nmathjax: true\n---\n## (Batch) Gradient Descent\n\n``` python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost.\n    cost = compute_cost(a, Y)\n    # Backward propagation.\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters.\n    parameters = update_parameters(parameters, grads)     \n```\n\n## Stochastic Gradient Descent\n\n```python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost = compute_cost(a, Y[:,j])\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads)\n```\n\n## Mini-Batch Gradient descent\n\n- **Shuffle**:\n\n<img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\">\n\n- **Partition**:\n\n<img src=\"/images/partition.png\" style=\"width:550px;height:300px;\">\n\nNote that the last mini-batch might end up smaller than `mini_batch_size=64`. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is `math.floor(s)` in Python). If the total number of examples is not a multiple of `mini_batch_size=64` then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). \n\n```python\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    return mini_batches\n```\n\n## Momentum\n\nBecause mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n\nMomentum takes into account the past gradients to smooth out the update. We will store the 'direction' of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. \n\n<img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\">\n<caption><center> <u><font color='purple'>**Figure 3**</u><font color='purple'>: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color='black'> </center>\n\n```python\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}    \n    # Initialize velocity\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters['W' + str(l+1)].shape[0], parameters['W' + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters['b' + str(l+1)].shape[0], parameters['b' + str(l+1)].shape[1]))\n        \n    return v\n```\n\n$$\\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}$$\n\n$$\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n\\end{cases}\\tag{4}$$\n\nwhere L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary.  Note that the iterator `l` starts at 0 in the `for` loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that's a \"one\" on the superscript). So you will need to shift `l` to `l+1` when coding.\n\n```python\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks    \n    # Momentum update for each parameter\n    for l in range(L):        \n        # compute velocities\n        v[\"dW\" + str(l+1)] = beta * v['dW' + str(l+1)] + (1 - beta) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta * v['db' + str(l+1)] + (1 - beta) * grads['db' + str(l+1)]\n        # update parameters\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n    return parameters, v\n```\n\n\n**How do you choose $\\beta$?**\n\n- The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. \n- Common values for $\\beta$ range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. \n- Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. \n\n## Adam\n\nAdam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n\n**How does Adam work?**\n1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n\nThe update rule is, for $l = 1, ..., L$: \n\n<img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\">\n\nwhere:\n- t counts the number of steps taken of Adam \n- L is the number of layers\n- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n- $\\alpha$ is the learning rate\n- $\\varepsilon$ is a very small number to avoid dividing by zero\n\nAs usual, we will store all parameters in the `parameters` dictionary\n\n```python\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n    return v, s\n```\n\n```python\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients.\n        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads['dW' + str(l+1)]\n        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)]\n        # Compute bias-corrected first moment estimate.\n        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t)\n        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t)\n        # Moving average of the squared gradients.\n        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads['dW' + str(l+1)] ** 2)\n        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads['db' + str(l+1)] ** 2)\n        # Compute bias-corrected second raw moment estimate.\n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n        # Update parameters. \n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n    return parameters, v, s\n```\n\n## Model with different optimization algorithms\n\n```python\ndef model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)  \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n    return parameters\n\ntrain_X, train_Y = load_dataset()\n\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n```\n## Summary\n\n![](/images/sgd.png)\n![](/images/minibatch.png)\n\n- **The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.**\n- **You have to tune a learning rate hyperparameter $\\alpha$.**\n- **With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).**\n- **Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.**\n\n- **Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.**\n\n- **Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster.**\n\n**Some advantages of Adam include:**\n- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) \n- Usually works well even with little tuning of hyperparameters (except $\\alpha$)","slug":"Gradient-Descent-Famliy","published":1,"updated":"2018-08-19T01:59:35.789Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh1t0008zkvoibplber8","content":"<h2 id=\"Batch-Gradient-Descent\"><a href=\"#Batch-Gradient-Descent\" class=\"headerlink\" title=\"(Batch) Gradient Descent\"></a>(Batch) Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    a, caches = forward_propagation(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">    cost = compute_cost(a, Y)</span><br><span class=\"line\">    <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">    grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Stochastic-Gradient-Descent\"><a href=\"#Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"Stochastic Gradient Descent\"></a>Stochastic Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, m):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">        cost = compute_cost(a, Y[:,j])</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">        grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Mini-Batch-Gradient-descent\"><a href=\"#Mini-Batch-Gradient-descent\" class=\"headerlink\" title=\"Mini-Batch Gradient descent\"></a>Mini-Batch Gradient descent</h2><ul>\n<li><strong>Shuffle</strong>:</li>\n</ul>\n<p><img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\"></p>\n<ul>\n<li><strong>Partition</strong>:</li>\n</ul>\n<p><img src=\"/images/partition.png\" style=\"width:550px;height:300px;\"></p>\n<p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_mini_batches</span><span class=\"params\">(X, Y, mini_batch_size = <span class=\"number\">64</span>, seed = <span class=\"number\">0</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Creates a list of random minibatches from (X, Y)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    np.random.seed(seed)            <span class=\"comment\"># To make your \"random\" minibatches the same as ours</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]                  <span class=\"comment\"># number of training examples</span></span><br><span class=\"line\">    mini_batches = []</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\"># Step 1: Shuffle (X, Y)</span></span><br><span class=\"line\">    permutation = list(np.random.permutation(m))</span><br><span class=\"line\">    shuffled_X = X[:, permutation]</span><br><span class=\"line\">    shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class=\"line\">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class=\"comment\"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_complete_minibatches):</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> m % mini_batch_size != <span class=\"number\">0</span>:</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mini_batches</span><br></pre></td></tr></table></figure>\n<h2 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. </p>\n<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. </p>\n<p><img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\"></p>\n<p><caption><center> <u><font color=\"purple\"><strong>Figure 3</strong></font></u><font color=\"purple\">: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color=\"black\"> </font></font></center></caption></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_velocity</span><span class=\"params\">(parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes the velocity as a python dictionary with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity.</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize velocity</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<p>$$\\begin{cases}<br>v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\<br>W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}<br>\\end{cases}\\tag{3}$$</p>\n<p>$$\\begin{cases}<br>v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\<br>b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}<br>\\end{cases}\\tag{4}$$</p>\n<p>where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.  Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_momentum</span><span class=\"params\">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Momentum</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity:</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    beta -- the momentum hyperparameter, scalar</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing your updated velocities</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks    </span></span><br><span class=\"line\">    <span class=\"comment\"># Momentum update for each parameter</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):        </span><br><span class=\"line\">        <span class=\"comment\"># compute velocities</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># update parameters</span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v</span><br></pre></td></tr></table></figure>\n<p><strong>How do you choose $\\beta$?</strong></p>\n<ul>\n<li>The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. </li>\n<li>Common values for $\\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. </li>\n<li>Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. </li>\n</ul>\n<h2 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. </p>\n<p><strong>How does Adam work?</strong></p>\n<ol>\n<li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li>\n<li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li>\n<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>\n</ol>\n<p>The update rule is, for $l = 1, …, L$: </p>\n<p><img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\"></p>\n<p>where:</p>\n<ul>\n<li>t counts the number of steps taken of Adam </li>\n<li>L is the number of layers</li>\n<li>$\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li>\n<li>$\\alpha$ is the learning rate</li>\n<li>$\\varepsilon$ is a very small number to avoid dividing by zero</li>\n</ul>\n<p>As usual, we will store all parameters in the <code>parameters</code> dictionary</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_adam</span><span class=\"params\">(parameters)</span> :</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes v and s as two python dictionaries with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"W\" + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"b\" + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns: </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;</span><br><span class=\"line\">    s = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> v, s</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_adam</span><span class=\"params\">(parameters, grads, v, s, t, learning_rate = <span class=\"number\">0.01</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                                beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Adam</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                 <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing first moment estimate, python dictionary</span></span><br><span class=\"line\">    s_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing second moment estimate, python dictionary    </span></span><br><span class=\"line\">    <span class=\"comment\"># Perform Adam update on all parameters</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the gradients.</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected first moment estimate.</span></span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the squared gradients.</span></span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected second raw moment estimate.</span></span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters. </span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v, s</span><br></pre></td></tr></table></figure>\n<h2 id=\"Model-with-different-optimization-algorithms\"><a href=\"#Model-with-different-optimization-algorithms\" class=\"headerlink\" title=\"Model with different optimization algorithms\"></a>Model with different optimization algorithms</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(X, Y, layers_dims, optimizer, learning_rate = <span class=\"number\">0.0007</span>, mini_batch_size = <span class=\"number\">64</span>, beta = <span class=\"number\">0.9</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">          beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>, num_epochs = <span class=\"number\">10000</span>, print_cost = True)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (2, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- python list, containing the size of each layer</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- the size of a mini batch</span></span><br><span class=\"line\"><span class=\"string\">    beta -- Momentum hyperparameter</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\">    num_epochs -- number of epochs</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    L = len(layers_dims)             <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    costs = []                       <span class=\"comment\"># to keep track of the cost</span></span><br><span class=\"line\">    t = <span class=\"number\">0</span>                            <span class=\"comment\"># initializing the counter required for Adam update</span></span><br><span class=\"line\">    seed = <span class=\"number\">10</span>                        <span class=\"comment\"># For grading purposes, so that your \"random\" minibatches are the same as ours</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize parameters</span></span><br><span class=\"line\">    parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Initialize the optimizer</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">        <span class=\"keyword\">pass</span> <span class=\"comment\"># no initialization required for gradient descent</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">        v = initialize_velocity(parameters)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">        v, s = initialize_adam(parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Optimization loop</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class=\"line\">        seed = seed + <span class=\"number\">1</span></span><br><span class=\"line\">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> minibatch <span class=\"keyword\">in</span> minibatches:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Select a minibatch</span></span><br><span class=\"line\">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">            cost = compute_cost(a3, minibatch_Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Update parameters</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">                t = t + <span class=\"number\">1</span> <span class=\"comment\"># Adam counter</span></span><br><span class=\"line\">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 1000 epoch</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">print</span> (<span class=\"string\">\"Cost after epoch %i: %f\"</span> %(i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)  </span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(costs)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'epochs (per 100)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate = \"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br><span class=\"line\"></span><br><span class=\"line\">train_X, train_Y = load_dataset()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># train 3-layer model</span></span><br><span class=\"line\">layers_dims = [train_X.shape[<span class=\"number\">0</span>], <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class=\"string\">\"gd\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Predict</span></span><br><span class=\"line\">predictions = predict(train_X, train_Y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Plot decision boundary</span></span><br><span class=\"line\">plt.title(<span class=\"string\">\"Model with Gradient Descent optimization\"</span>)</span><br><span class=\"line\">axes = plt.gca()</span><br><span class=\"line\">axes.set_xlim([<span class=\"number\">-1.5</span>,<span class=\"number\">2.5</span>])</span><br><span class=\"line\">axes.set_ylim([<span class=\"number\">-1</span>,<span class=\"number\">1.5</span>])</span><br><span class=\"line\">plot_decision_boundary(<span class=\"keyword\">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p><img src=\"/images/sgd.png\" alt=\"\"><br><img src=\"/images/minibatch.png\" alt=\"\"></p>\n<ul>\n<li><strong>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.</strong></li>\n<li><strong>You have to tune a learning rate hyperparameter $\\alpha$.</strong></li>\n<li><strong>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</strong></li>\n<li><p><strong>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</strong></p>\n</li>\n<li><p><strong>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</strong></p>\n</li>\n<li><p><strong>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</strong></p>\n</li>\n</ul>\n<p><strong>Some advantages of Adam include:</strong></p>\n<ul>\n<li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) </li>\n<li>Usually works well even with little tuning of hyperparameters (except $\\alpha$)</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Batch-Gradient-Descent\"><a href=\"#Batch-Gradient-Descent\" class=\"headerlink\" title=\"(Batch) Gradient Descent\"></a>(Batch) Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    a, caches = forward_propagation(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">    cost = compute_cost(a, Y)</span><br><span class=\"line\">    <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">    grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Stochastic-Gradient-Descent\"><a href=\"#Stochastic-Gradient-Descent\" class=\"headerlink\" title=\"Stochastic Gradient Descent\"></a>Stochastic Gradient Descent</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = data_input</span><br><span class=\"line\">Y = labels</span><br><span class=\"line\">parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, m):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">        cost = compute_cost(a, Y[:,j])</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">        grads = backward_propagation(a, caches, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Mini-Batch-Gradient-descent\"><a href=\"#Mini-Batch-Gradient-descent\" class=\"headerlink\" title=\"Mini-Batch Gradient descent\"></a>Mini-Batch Gradient descent</h2><ul>\n<li><strong>Shuffle</strong>:</li>\n</ul>\n<p><img src=\"/images/shuffle.png\" style=\"width:550px;height:300px;\"></p>\n<ul>\n<li><strong>Partition</strong>:</li>\n</ul>\n<p><img src=\"/images/partition.png\" style=\"width:550px;height:300px;\"></p>\n<p>Note that the last mini-batch might end up smaller than <code>mini_batch_size=64</code>. Let $\\lfloor s \\rfloor$ represents $s$ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be $\\lfloor \\frac{m}{mini_batch_size}\\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be ($m-mini_batch_size \\times \\lfloor \\frac{m}{mini_batch_size}\\rfloor$). </p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">random_mini_batches</span><span class=\"params\">(X, Y, mini_batch_size = <span class=\"number\">64</span>, seed = <span class=\"number\">0</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Creates a list of random minibatches from (X, Y)</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    np.random.seed(seed)            <span class=\"comment\"># To make your \"random\" minibatches the same as ours</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]                  <span class=\"comment\"># number of training examples</span></span><br><span class=\"line\">    mini_batches = []</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"comment\"># Step 1: Shuffle (X, Y)</span></span><br><span class=\"line\">    permutation = list(np.random.permutation(m))</span><br><span class=\"line\">    shuffled_X = X[:, permutation]</span><br><span class=\"line\">    shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class=\"line\">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class=\"comment\"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_complete_minibatches):</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+<span class=\"number\">1</span>) * mini_batch_size]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> m % mini_batch_size != <span class=\"number\">0</span>:</span><br><span class=\"line\">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]</span><br><span class=\"line\">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class=\"line\">        mini_batches.append(mini_batch)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> mini_batches</span><br></pre></td></tr></table></figure>\n<h2 id=\"Momentum\"><a href=\"#Momentum\" class=\"headerlink\" title=\"Momentum\"></a>Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations. </p>\n<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. </p>\n<p><img src=\"/images/momentum.png\" style=\"width:400px;height:250px;\"></p>\n<p><caption><center> <u><font color=\"purple\"><strong>Figure 3</strong></font></u><font color=\"purple\">: The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.<br> <font color=\"black\"> </font></font></center></caption></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_velocity</span><span class=\"params\">(parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes the velocity as a python dictionary with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity.</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize velocity</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'W'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">'b'</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<p>$$\\begin{cases}<br>v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\<br>W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}<br>\\end{cases}\\tag{3}$$</p>\n<p>$$\\begin{cases}<br>v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\<br>b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}}<br>\\end{cases}\\tag{4}$$</p>\n<p>where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary.  Note that the iterator <code>l</code> starts at 0 in the <code>for</code> loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift <code>l</code> to <code>l+1</code> when coding.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_momentum</span><span class=\"params\">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Momentum</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing the current velocity:</span></span><br><span class=\"line\"><span class=\"string\">                    v['dW' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v['db' + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    beta -- the momentum hyperparameter, scalar</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary containing your updated velocities</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks    </span></span><br><span class=\"line\">    <span class=\"comment\"># Momentum update for each parameter</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):        </span><br><span class=\"line\">        <span class=\"comment\"># compute velocities</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta * v[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># update parameters</span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v</span><br></pre></td></tr></table></figure>\n<p><strong>How do you choose $\\beta$?</strong></p>\n<ul>\n<li>The larger the momentum $\\beta$ is, the smoother the update because the more we take the past gradients into account. But if $\\beta$ is too big, it could also smooth out the updates too much. </li>\n<li>Common values for $\\beta$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. </li>\n<li>Tuning the optimal $\\beta$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$. </li>\n</ul>\n<h2 id=\"Adam\"><a href=\"#Adam\" class=\"headerlink\" title=\"Adam\"></a>Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. </p>\n<p><strong>How does Adam work?</strong></p>\n<ol>\n<li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li>\n<li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li>\n<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>\n</ol>\n<p>The update rule is, for $l = 1, …, L$: </p>\n<p><img src=\"/images/adam.PNG\" style=\"width:550px;height:300px;\"></p>\n<p>where:</p>\n<ul>\n<li>t counts the number of steps taken of Adam </li>\n<li>L is the number of layers</li>\n<li>$\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li>\n<li>$\\alpha$ is the learning rate</li>\n<li>$\\varepsilon$ is a very small number to avoid dividing by zero</li>\n</ul>\n<p>As usual, we will store all parameters in the <code>parameters</code> dictionary</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_adam</span><span class=\"params\">(parameters)</span> :</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Initializes v and s as two python dictionaries with:</span></span><br><span class=\"line\"><span class=\"string\">                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" </span></span><br><span class=\"line\"><span class=\"string\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters.</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"W\" + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters[\"b\" + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Returns: </span></span><br><span class=\"line\"><span class=\"string\">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    v[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                    s[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span> <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v = &#123;&#125;</span><br><span class=\"line\">    s = &#123;&#125;    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = np.zeros((parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">0</span>], parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)].shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> v, s</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters_with_adam</span><span class=\"params\">(parameters, grads, v, s, t, learning_rate = <span class=\"number\">0.01</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                                beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using Adam</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['W' + str(l)] = Wl</span></span><br><span class=\"line\"><span class=\"string\">                    parameters['b' + str(l)] = bl</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class=\"line\"><span class=\"string\">                    grads['dW' + str(l)] = dWl</span></span><br><span class=\"line\"><span class=\"string\">                    grads['db' + str(l)] = dbl</span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                 <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    v_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing first moment estimate, python dictionary</span></span><br><span class=\"line\">    s_corrected = &#123;&#125;                         <span class=\"comment\"># Initializing second moment estimate, python dictionary    </span></span><br><span class=\"line\">    <span class=\"comment\"># Perform Adam update on all parameters</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the gradients.</span></span><br><span class=\"line\">        v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta1 * v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta1) * grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)]</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected first moment estimate.</span></span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = v[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta1 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Moving average of the squared gradients.</span></span><br><span class=\"line\">        s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'dW'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = beta2 * s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] + (<span class=\"number\">1</span> - beta2) * (grads[<span class=\"string\">'db'</span> + str(l+<span class=\"number\">1</span>)] ** <span class=\"number\">2</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Compute bias-corrected second raw moment estimate.</span></span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] = s[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (<span class=\"number\">1</span> - beta2 ** t)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters. </span></span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"dW\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l+<span class=\"number\">1</span>)] - learning_rate * v_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)] / (np.sqrt(s_corrected[<span class=\"string\">\"db\"</span> + str(l+<span class=\"number\">1</span>)]) + epsilon)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters, v, s</span><br></pre></td></tr></table></figure>\n<h2 id=\"Model-with-different-optimization-algorithms\"><a href=\"#Model-with-different-optimization-algorithms\" class=\"headerlink\" title=\"Model with different optimization algorithms\"></a>Model with different optimization algorithms</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">model</span><span class=\"params\">(X, Y, layers_dims, optimizer, learning_rate = <span class=\"number\">0.0007</span>, mini_batch_size = <span class=\"number\">64</span>, beta = <span class=\"number\">0.9</span>,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">          beta1 = <span class=\"number\">0.9</span>, beta2 = <span class=\"number\">0.999</span>,  epsilon = <span class=\"number\">1e-8</span>, num_epochs = <span class=\"number\">10000</span>, print_cost = True)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- input data, of shape (2, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- python list, containing the size of each layer</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- the learning rate, scalar.</span></span><br><span class=\"line\"><span class=\"string\">    mini_batch_size -- the size of a mini batch</span></span><br><span class=\"line\"><span class=\"string\">    beta -- Momentum hyperparameter</span></span><br><span class=\"line\"><span class=\"string\">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class=\"line\"><span class=\"string\">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class=\"line\"><span class=\"string\">    num_epochs -- number of epochs</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters </span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\"></span><br><span class=\"line\">    L = len(layers_dims)             <span class=\"comment\"># number of layers in the neural networks</span></span><br><span class=\"line\">    costs = []                       <span class=\"comment\"># to keep track of the cost</span></span><br><span class=\"line\">    t = <span class=\"number\">0</span>                            <span class=\"comment\"># initializing the counter required for Adam update</span></span><br><span class=\"line\">    seed = <span class=\"number\">10</span>                        <span class=\"comment\"># For grading purposes, so that your \"random\" minibatches are the same as ours</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Initialize parameters</span></span><br><span class=\"line\">    parameters = initialize_parameters(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Initialize the optimizer</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">        <span class=\"keyword\">pass</span> <span class=\"comment\"># no initialization required for gradient descent</span></span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">        v = initialize_velocity(parameters)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">        v, s = initialize_adam(parameters)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Optimization loop</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class=\"line\">        seed = seed + <span class=\"number\">1</span></span><br><span class=\"line\">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> minibatch <span class=\"keyword\">in</span> minibatches:</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Select a minibatch</span></span><br><span class=\"line\">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Compute cost</span></span><br><span class=\"line\">            cost = compute_cost(a3, minibatch_Y)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Backward propagation</span></span><br><span class=\"line\">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># Update parameters</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> optimizer == <span class=\"string\">\"gd\"</span>:</span><br><span class=\"line\">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"momentum\"</span>:</span><br><span class=\"line\">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> optimizer == <span class=\"string\">\"adam\"</span>:</span><br><span class=\"line\">                t = t + <span class=\"number\">1</span> <span class=\"comment\"># Adam counter</span></span><br><span class=\"line\">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 1000 epoch</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">1000</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"keyword\">print</span> (<span class=\"string\">\"Cost after epoch %i: %f\"</span> %(i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)  </span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(costs)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'epochs (per 100)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate = \"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br><span class=\"line\"></span><br><span class=\"line\">train_X, train_Y = load_dataset()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># train 3-layer model</span></span><br><span class=\"line\">layers_dims = [train_X.shape[<span class=\"number\">0</span>], <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>]</span><br><span class=\"line\">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class=\"string\">\"gd\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Predict</span></span><br><span class=\"line\">predictions = predict(train_X, train_Y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Plot decision boundary</span></span><br><span class=\"line\">plt.title(<span class=\"string\">\"Model with Gradient Descent optimization\"</span>)</span><br><span class=\"line\">axes = plt.gca()</span><br><span class=\"line\">axes.set_xlim([<span class=\"number\">-1.5</span>,<span class=\"number\">2.5</span>])</span><br><span class=\"line\">axes.set_ylim([<span class=\"number\">-1</span>,<span class=\"number\">1.5</span>])</span><br><span class=\"line\">plot_decision_boundary(<span class=\"keyword\">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p><img src=\"/images/sgd.png\" alt=\"\"><br><img src=\"/images/minibatch.png\" alt=\"\"></p>\n<ul>\n<li><strong>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.</strong></li>\n<li><strong>You have to tune a learning rate hyperparameter $\\alpha$.</strong></li>\n<li><strong>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</strong></li>\n<li><p><strong>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</strong></p>\n</li>\n<li><p><strong>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</strong></p>\n</li>\n<li><p><strong>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</strong></p>\n</li>\n</ul>\n<p><strong>Some advantages of Adam include:</strong></p>\n<ul>\n<li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum) </li>\n<li>Usually works well even with little tuning of hyperparameters (except $\\alpha$)</li>\n</ul>\n"},{"title":"How to set up a blog with hexo on github.io","date":"2018-07-18T10:17:31.000Z","_content":"### Install Git (https://git-scm.com/)\n\n* If you have a git, you can check it by `git -v`\n\n### Install node.js envornment \n\n* Download from [https://nodejs.org/en/](https://nodejs.org/en/)\n* Install as the default(make sure the envorment path is collected)\n* When you finsh, you can check it by `node -v`\n\n###  Make a new repository named \"<your_username>.github.io\"\n\n### Install Hexo\n\n* `npm install hexo-cli -g`\n* `hexo init blog`(that folder you wanted to store your webpage)\n* `cd blog`\n* `npm install`\n* `hexo server`\n\n### Connect hexo with github\n\n* `cd blog`\n* `git config --global user.name \"<your_username>\"`\n* `git config --global user.email \"<your_email>\"`\n\nCheck if you have a ssh keygen. If not you can do as following\n\n* `cd ~/.ssh`\n* generate key: `ssh-kengen -t rsa -C \"<your_email>\"` (choice default setting)\n* add key to ssh-agent: `eval \"$(ssh-agent -s)\"`\n* `ssh-add ~/.ssh/id_rsa`\n\n### Sign in the github, in settings, add a new ssh key, copy the `id_rsa.pub` to key options. check if it's ok by `ssh -T git@github.com`. If you get a hi message, it is ok.\n\n### In your blog folder, edit the _config.yml file like this.\n\n```\n(in the end)\ndeploy:\n    type: git\n    repository: git@github.com:<your_username>/<your_username>.github.io.git\n    branch: master\n```\n\n### Before deploy the blog website, you should install a plug\n\n* `cd blog`\n* `npm install hexo-deployer-git --save`\n\n### Run it online.\n\n* `hexo clean`\n* `hexo g`\n* `hexo d`\n\n","source":"_posts/How-to-set-up-a-blog-with-hexo-on-github-io.md","raw":"---\ntitle: How to set up a blog with hexo on github.io\ndate: 2018-07-18 18:17:31\ntags: hexo\ncategories: web\n---\n### Install Git (https://git-scm.com/)\n\n* If you have a git, you can check it by `git -v`\n\n### Install node.js envornment \n\n* Download from [https://nodejs.org/en/](https://nodejs.org/en/)\n* Install as the default(make sure the envorment path is collected)\n* When you finsh, you can check it by `node -v`\n\n###  Make a new repository named \"<your_username>.github.io\"\n\n### Install Hexo\n\n* `npm install hexo-cli -g`\n* `hexo init blog`(that folder you wanted to store your webpage)\n* `cd blog`\n* `npm install`\n* `hexo server`\n\n### Connect hexo with github\n\n* `cd blog`\n* `git config --global user.name \"<your_username>\"`\n* `git config --global user.email \"<your_email>\"`\n\nCheck if you have a ssh keygen. If not you can do as following\n\n* `cd ~/.ssh`\n* generate key: `ssh-kengen -t rsa -C \"<your_email>\"` (choice default setting)\n* add key to ssh-agent: `eval \"$(ssh-agent -s)\"`\n* `ssh-add ~/.ssh/id_rsa`\n\n### Sign in the github, in settings, add a new ssh key, copy the `id_rsa.pub` to key options. check if it's ok by `ssh -T git@github.com`. If you get a hi message, it is ok.\n\n### In your blog folder, edit the _config.yml file like this.\n\n```\n(in the end)\ndeploy:\n    type: git\n    repository: git@github.com:<your_username>/<your_username>.github.io.git\n    branch: master\n```\n\n### Before deploy the blog website, you should install a plug\n\n* `cd blog`\n* `npm install hexo-deployer-git --save`\n\n### Run it online.\n\n* `hexo clean`\n* `hexo g`\n* `hexo d`\n\n","slug":"How-to-set-up-a-blog-with-hexo-on-github-io","published":1,"updated":"2018-08-19T01:59:35.790Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh1y000czkvo3sbh0wnu","content":"<h3 id=\"Install-Git-https-git-scm-com\"><a href=\"#Install-Git-https-git-scm-com\" class=\"headerlink\" title=\"Install Git (https://git-scm.com/)\"></a>Install Git (<a href=\"https://git-scm.com/\" target=\"_blank\" rel=\"noopener\">https://git-scm.com/</a>)</h3><ul>\n<li>If you have a git, you can check it by <code>git -v</code></li>\n</ul>\n<h3 id=\"Install-node-js-envornment\"><a href=\"#Install-node-js-envornment\" class=\"headerlink\" title=\"Install node.js envornment\"></a>Install node.js envornment</h3><ul>\n<li>Download from <a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">https://nodejs.org/en/</a></li>\n<li>Install as the default(make sure the envorment path is collected)</li>\n<li>When you finsh, you can check it by <code>node -v</code></li>\n</ul>\n<h3 id=\"Make-a-new-repository-named-“-lt-your-username-gt-github-io”\"><a href=\"#Make-a-new-repository-named-“-lt-your-username-gt-github-io”\" class=\"headerlink\" title=\"Make a new repository named “&lt;your_username&gt;.github.io”\"></a>Make a new repository named “&lt;your_username&gt;.github.io”</h3><h3 id=\"Install-Hexo\"><a href=\"#Install-Hexo\" class=\"headerlink\" title=\"Install Hexo\"></a>Install Hexo</h3><ul>\n<li><code>npm install hexo-cli -g</code></li>\n<li><code>hexo init blog</code>(that folder you wanted to store your webpage)</li>\n<li><code>cd blog</code></li>\n<li><code>npm install</code></li>\n<li><code>hexo server</code></li>\n</ul>\n<h3 id=\"Connect-hexo-with-github\"><a href=\"#Connect-hexo-with-github\" class=\"headerlink\" title=\"Connect hexo with github\"></a>Connect hexo with github</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>git config --global user.name &quot;&lt;your_username&gt;&quot;</code></li>\n<li><code>git config --global user.email &quot;&lt;your_email&gt;&quot;</code></li>\n</ul>\n<p>Check if you have a ssh keygen. If not you can do as following</p>\n<ul>\n<li><code>cd ~/.ssh</code></li>\n<li>generate key: <code>ssh-kengen -t rsa -C &quot;&lt;your_email&gt;&quot;</code> (choice default setting)</li>\n<li>add key to ssh-agent: <code>eval &quot;$(ssh-agent -s)&quot;</code></li>\n<li><code>ssh-add ~/.ssh/id_rsa</code></li>\n</ul>\n<h3 id=\"Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\"><a href=\"#Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\" class=\"headerlink\" title=\"Sign in the github, in settings, add a new ssh key, copy the id_rsa.pub to key options. check if it’s ok by ssh -T git@github.com. If you get a hi message, it is ok.\"></a>Sign in the github, in settings, add a new ssh key, copy the <code>id_rsa.pub</code> to key options. check if it’s ok by <code>ssh -T git@github.com</code>. If you get a hi message, it is ok.</h3><h3 id=\"In-your-blog-folder-edit-the-config-yml-file-like-this\"><a href=\"#In-your-blog-folder-edit-the-config-yml-file-like-this\" class=\"headerlink\" title=\"In your blog folder, edit the _config.yml file like this.\"></a>In your blog folder, edit the _config.yml file like this.</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(in the end)</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">    type: git</span><br><span class=\"line\">    repository: git@github.com:&lt;your_username&gt;/&lt;your_username&gt;.github.io.git</span><br><span class=\"line\">    branch: master</span><br></pre></td></tr></table></figure>\n<h3 id=\"Before-deploy-the-blog-website-you-should-install-a-plug\"><a href=\"#Before-deploy-the-blog-website-you-should-install-a-plug\" class=\"headerlink\" title=\"Before deploy the blog website, you should install a plug\"></a>Before deploy the blog website, you should install a plug</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>npm install hexo-deployer-git --save</code></li>\n</ul>\n<h3 id=\"Run-it-online\"><a href=\"#Run-it-online\" class=\"headerlink\" title=\"Run it online.\"></a>Run it online.</h3><ul>\n<li><code>hexo clean</code></li>\n<li><code>hexo g</code></li>\n<li><code>hexo d</code></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Install-Git-https-git-scm-com\"><a href=\"#Install-Git-https-git-scm-com\" class=\"headerlink\" title=\"Install Git (https://git-scm.com/)\"></a>Install Git (<a href=\"https://git-scm.com/\" target=\"_blank\" rel=\"noopener\">https://git-scm.com/</a>)</h3><ul>\n<li>If you have a git, you can check it by <code>git -v</code></li>\n</ul>\n<h3 id=\"Install-node-js-envornment\"><a href=\"#Install-node-js-envornment\" class=\"headerlink\" title=\"Install node.js envornment\"></a>Install node.js envornment</h3><ul>\n<li>Download from <a href=\"https://nodejs.org/en/\" target=\"_blank\" rel=\"noopener\">https://nodejs.org/en/</a></li>\n<li>Install as the default(make sure the envorment path is collected)</li>\n<li>When you finsh, you can check it by <code>node -v</code></li>\n</ul>\n<h3 id=\"Make-a-new-repository-named-“-lt-your-username-gt-github-io”\"><a href=\"#Make-a-new-repository-named-“-lt-your-username-gt-github-io”\" class=\"headerlink\" title=\"Make a new repository named “&lt;your_username&gt;.github.io”\"></a>Make a new repository named “&lt;your_username&gt;.github.io”</h3><h3 id=\"Install-Hexo\"><a href=\"#Install-Hexo\" class=\"headerlink\" title=\"Install Hexo\"></a>Install Hexo</h3><ul>\n<li><code>npm install hexo-cli -g</code></li>\n<li><code>hexo init blog</code>(that folder you wanted to store your webpage)</li>\n<li><code>cd blog</code></li>\n<li><code>npm install</code></li>\n<li><code>hexo server</code></li>\n</ul>\n<h3 id=\"Connect-hexo-with-github\"><a href=\"#Connect-hexo-with-github\" class=\"headerlink\" title=\"Connect hexo with github\"></a>Connect hexo with github</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>git config --global user.name &quot;&lt;your_username&gt;&quot;</code></li>\n<li><code>git config --global user.email &quot;&lt;your_email&gt;&quot;</code></li>\n</ul>\n<p>Check if you have a ssh keygen. If not you can do as following</p>\n<ul>\n<li><code>cd ~/.ssh</code></li>\n<li>generate key: <code>ssh-kengen -t rsa -C &quot;&lt;your_email&gt;&quot;</code> (choice default setting)</li>\n<li>add key to ssh-agent: <code>eval &quot;$(ssh-agent -s)&quot;</code></li>\n<li><code>ssh-add ~/.ssh/id_rsa</code></li>\n</ul>\n<h3 id=\"Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\"><a href=\"#Sign-in-the-github-in-settings-add-a-new-ssh-key-copy-the-id-rsa-pub-to-key-options-check-if-it’s-ok-by-ssh-T-git-github-com-If-you-get-a-hi-message-it-is-ok\" class=\"headerlink\" title=\"Sign in the github, in settings, add a new ssh key, copy the id_rsa.pub to key options. check if it’s ok by ssh -T git@github.com. If you get a hi message, it is ok.\"></a>Sign in the github, in settings, add a new ssh key, copy the <code>id_rsa.pub</code> to key options. check if it’s ok by <code>ssh -T git@github.com</code>. If you get a hi message, it is ok.</h3><h3 id=\"In-your-blog-folder-edit-the-config-yml-file-like-this\"><a href=\"#In-your-blog-folder-edit-the-config-yml-file-like-this\" class=\"headerlink\" title=\"In your blog folder, edit the _config.yml file like this.\"></a>In your blog folder, edit the _config.yml file like this.</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(in the end)</span><br><span class=\"line\">deploy:</span><br><span class=\"line\">    type: git</span><br><span class=\"line\">    repository: git@github.com:&lt;your_username&gt;/&lt;your_username&gt;.github.io.git</span><br><span class=\"line\">    branch: master</span><br></pre></td></tr></table></figure>\n<h3 id=\"Before-deploy-the-blog-website-you-should-install-a-plug\"><a href=\"#Before-deploy-the-blog-website-you-should-install-a-plug\" class=\"headerlink\" title=\"Before deploy the blog website, you should install a plug\"></a>Before deploy the blog website, you should install a plug</h3><ul>\n<li><code>cd blog</code></li>\n<li><code>npm install hexo-deployer-git --save</code></li>\n</ul>\n<h3 id=\"Run-it-online\"><a href=\"#Run-it-online\" class=\"headerlink\" title=\"Run it online.\"></a>Run it online.</h3><ul>\n<li><code>hexo clean</code></li>\n<li><code>hexo g</code></li>\n<li><code>hexo d</code></li>\n</ul>\n"},{"title":"ImageNet Classification wih Deep Convolutional Neural Network","date":"2018-08-28T07:07:25.000Z","mathjax":true,"_content":"这是一篇由Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton发表在NIPS上的[Paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)。该论文中提出了一种新型网络架构即 **AlexNet**.\n\n## AlexNet\n\nAlexNet首次在大规模图像数据集实现了深层卷积神经网络结构，点燃了深度学习应用在计算机视觉领域的这把火。其在 *ImageNet LSVRC-2012* $^{[1]}$ 目标识别竞赛的 *top-5 error* $^{[2]}$ 为15.3%，同期第二名仅为26.2%，碾压其他传统的hand-craft 特征方法，使得计算机视觉从业者从繁重的特征工程中解脱出来，转向思考能够从数据中自动提取需要的特征，做到数据驱动。\n\n### 数据集\n\n对原始高清图像进行下采样得到固定的256\\*256的图像。具体方法是，给一个矩形图像，先调整短边长度为256然后裁剪出中间部分256\\*256的图片。最后按RGB像素值减去训练集中所有图像的均值图像像素。\n\n### 架构\n\n![](/images/alexnet_architecture.PNG)\n\n第二、第四和第五卷积层的内核只连接到位于同一GPU上的前一层内核映射。第三个卷积层的内核连接到第二层的所有内核映射。完全连接层的神经元与前一层的所有神经元连接。局部响应归一化接在第一，第二个卷积层后面。最大池化层跟随着局部响应层和第五个卷积层。\n\n#### ReLU Nonlinearity\n\n标准的神经元模型输出是sigmoid, tanh这些的函数。用梯度下降训练时在训练时间上，这些饱和非线性函数比非饱和非线性函数需要的时间要多得多。相比于tanh单元，深度卷积网络使用ReLU训练得更快。\n\n#### 在多个GPU上训练\n\n当前的gpu特别适合于cross-gpu并行化，因为它们能够直接从彼此的内存中读取和写入，而无需通过主机内存。并行化方案实际上是将一半的内核(或神经元)放在每个GPU上，还有一个额外的技巧:GPU只在特定层进行通信。这个方案使top-1和top-5的错误率分别降低了1.7%和1.2%。\n\n#### 局部响应归一化\n\n$$b^{i}\\_{x, y} = a^{i}\\_{x, y} / (k + \\alpha \\sum_{j = max(0, i - n/2)}^{min(N - 1, i + n/2)} (a^{i}_{x, y})^2)^{\\beta}$$\n\n$a^{i}\\_{x, y}$ 是应用卷积(包括非线性单元)操作后第i个通道位于(x, y)的值, $b^{i}\\_{x, y}$ 是应用局部响应归一化后的同一位置上值。常数$k, n, \\alpha, \\beta$ 是超参数。这里设置为$k = 2, n = 5, \\alpha=10^{-4}, \\beta=0.75$\n\n来源于生物学上的概念: **侧抑制**, 指被激化的神经元抑制相邻神经元的现象。这使得响应比较大的值相对更大，提高了模型的泛化能力。这个方案使top-1和top-5的错误率分别降低了1.4%和1.2%。\n\n#### 重叠池化\n\nCNNs中的池化层汇总了同一核映射中相邻神经元群的输出。传统的池化，相邻池单元不会重叠，即步长$s=z$ 池化单元大小。如果$s < z$则得到重叠的池化。在整个网络中使用$s = 2, z=3$。这个方案使top-1和top-5的错误率分别降低了0.4%和0.3%。\n\n### 降低过拟合\n\n#### 数据增强\n\n关于图像数据最简单也最常用的降低过拟合的方法是通过保留标签对图像进行简单变换的方法人为地扩大数据集。\n\n第一种形式是：将图像进行水平翻转。通过从256\\*256的图像中随机抽取227\\*227的块，还有它们的水平翻转，用这些图像去训练网络。在测试时，通过抽取测试图像的四个角落和中间227\\*227的块，以及它们的水平翻转，一共10个块来输入进网络，最后以它们的平均值作为预测结果。\n\n第二种形式是：改变RGB通道的强度。具体来说，在整个训练集中对RGB像素值集执行PCA。对于每个训练图像，添加已找到的主成分的倍数。与对应的特征值成正比的大小乘以均值为0和标准差为0.1的高斯随机变量。\n\n对每个像素值 $I\\_{xy} = [I^{R}\\_{xy}, I^{G}\\_{xy}, I^{B}\\_{xy}]^T$ 加上以下数量：\n$$[p_1, p_2, p_3][\\alpha_1 \\lambda_1, \\alpha_2 \\lambda_2, \\alpha_3 \\lambda_3]^T$$\n$p_i, \\lambda_i$ 分别是RGB像素协方差矩阵的第i个特征向量和特征值, $\\alpha_i$ 是随机变量，每个 $\\alpha_i$ 只一个训练图像的一个像素。这个方案使top-1降低了1%。\n\n#### Dropout\n\n以一定概率使神经元的输出置为0。这种技术减少了神经元复杂的协同适应，因为神经元不能依赖于特定的其他神经元的存在。因此，它不得不学习更健壮的特性，这些特性与其他神经元的许多不同随机子集一起使用。\n\n### 注解\n\n[1]: ImageNet数据集大约包含2.2万种不同种类的1500万张高清图像。年度的图像识别竞赛**the ImageNet Large-Scale Visual Recognition Challenge\n(ILSVRC)** 从2010开始举行。 ILSVRC 使用大约1000种类每类1000张图片的数据集。\n\n[2]: the top-5 错误率是：测试图像的预测结果前五位不包含真实标签的比例。\n\n[3]: The model result in ILSVRC-2010\n\n![](/images/alexnet_ilsvrc.PNG)\n","source":"_posts/ImageNet-Classification-wih-Deep-Convolutional-Neural-Network.md","raw":"---\ntitle: ImageNet Classification wih Deep Convolutional Neural Network\ndate: 2018-08-28 15:07:25\ntags: CNN\ncategories: 深度学习\nmathjax: true\n---\n这是一篇由Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton发表在NIPS上的[Paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)。该论文中提出了一种新型网络架构即 **AlexNet**.\n\n## AlexNet\n\nAlexNet首次在大规模图像数据集实现了深层卷积神经网络结构，点燃了深度学习应用在计算机视觉领域的这把火。其在 *ImageNet LSVRC-2012* $^{[1]}$ 目标识别竞赛的 *top-5 error* $^{[2]}$ 为15.3%，同期第二名仅为26.2%，碾压其他传统的hand-craft 特征方法，使得计算机视觉从业者从繁重的特征工程中解脱出来，转向思考能够从数据中自动提取需要的特征，做到数据驱动。\n\n### 数据集\n\n对原始高清图像进行下采样得到固定的256\\*256的图像。具体方法是，给一个矩形图像，先调整短边长度为256然后裁剪出中间部分256\\*256的图片。最后按RGB像素值减去训练集中所有图像的均值图像像素。\n\n### 架构\n\n![](/images/alexnet_architecture.PNG)\n\n第二、第四和第五卷积层的内核只连接到位于同一GPU上的前一层内核映射。第三个卷积层的内核连接到第二层的所有内核映射。完全连接层的神经元与前一层的所有神经元连接。局部响应归一化接在第一，第二个卷积层后面。最大池化层跟随着局部响应层和第五个卷积层。\n\n#### ReLU Nonlinearity\n\n标准的神经元模型输出是sigmoid, tanh这些的函数。用梯度下降训练时在训练时间上，这些饱和非线性函数比非饱和非线性函数需要的时间要多得多。相比于tanh单元，深度卷积网络使用ReLU训练得更快。\n\n#### 在多个GPU上训练\n\n当前的gpu特别适合于cross-gpu并行化，因为它们能够直接从彼此的内存中读取和写入，而无需通过主机内存。并行化方案实际上是将一半的内核(或神经元)放在每个GPU上，还有一个额外的技巧:GPU只在特定层进行通信。这个方案使top-1和top-5的错误率分别降低了1.7%和1.2%。\n\n#### 局部响应归一化\n\n$$b^{i}\\_{x, y} = a^{i}\\_{x, y} / (k + \\alpha \\sum_{j = max(0, i - n/2)}^{min(N - 1, i + n/2)} (a^{i}_{x, y})^2)^{\\beta}$$\n\n$a^{i}\\_{x, y}$ 是应用卷积(包括非线性单元)操作后第i个通道位于(x, y)的值, $b^{i}\\_{x, y}$ 是应用局部响应归一化后的同一位置上值。常数$k, n, \\alpha, \\beta$ 是超参数。这里设置为$k = 2, n = 5, \\alpha=10^{-4}, \\beta=0.75$\n\n来源于生物学上的概念: **侧抑制**, 指被激化的神经元抑制相邻神经元的现象。这使得响应比较大的值相对更大，提高了模型的泛化能力。这个方案使top-1和top-5的错误率分别降低了1.4%和1.2%。\n\n#### 重叠池化\n\nCNNs中的池化层汇总了同一核映射中相邻神经元群的输出。传统的池化，相邻池单元不会重叠，即步长$s=z$ 池化单元大小。如果$s < z$则得到重叠的池化。在整个网络中使用$s = 2, z=3$。这个方案使top-1和top-5的错误率分别降低了0.4%和0.3%。\n\n### 降低过拟合\n\n#### 数据增强\n\n关于图像数据最简单也最常用的降低过拟合的方法是通过保留标签对图像进行简单变换的方法人为地扩大数据集。\n\n第一种形式是：将图像进行水平翻转。通过从256\\*256的图像中随机抽取227\\*227的块，还有它们的水平翻转，用这些图像去训练网络。在测试时，通过抽取测试图像的四个角落和中间227\\*227的块，以及它们的水平翻转，一共10个块来输入进网络，最后以它们的平均值作为预测结果。\n\n第二种形式是：改变RGB通道的强度。具体来说，在整个训练集中对RGB像素值集执行PCA。对于每个训练图像，添加已找到的主成分的倍数。与对应的特征值成正比的大小乘以均值为0和标准差为0.1的高斯随机变量。\n\n对每个像素值 $I\\_{xy} = [I^{R}\\_{xy}, I^{G}\\_{xy}, I^{B}\\_{xy}]^T$ 加上以下数量：\n$$[p_1, p_2, p_3][\\alpha_1 \\lambda_1, \\alpha_2 \\lambda_2, \\alpha_3 \\lambda_3]^T$$\n$p_i, \\lambda_i$ 分别是RGB像素协方差矩阵的第i个特征向量和特征值, $\\alpha_i$ 是随机变量，每个 $\\alpha_i$ 只一个训练图像的一个像素。这个方案使top-1降低了1%。\n\n#### Dropout\n\n以一定概率使神经元的输出置为0。这种技术减少了神经元复杂的协同适应，因为神经元不能依赖于特定的其他神经元的存在。因此，它不得不学习更健壮的特性，这些特性与其他神经元的许多不同随机子集一起使用。\n\n### 注解\n\n[1]: ImageNet数据集大约包含2.2万种不同种类的1500万张高清图像。年度的图像识别竞赛**the ImageNet Large-Scale Visual Recognition Challenge\n(ILSVRC)** 从2010开始举行。 ILSVRC 使用大约1000种类每类1000张图片的数据集。\n\n[2]: the top-5 错误率是：测试图像的预测结果前五位不包含真实标签的比例。\n\n[3]: The model result in ILSVRC-2010\n\n![](/images/alexnet_ilsvrc.PNG)\n","slug":"ImageNet-Classification-wih-Deep-Convolutional-Neural-Network","published":1,"updated":"2018-08-28T07:25:42.186Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh22000dzkvojp09hnqb","content":"<p>这是一篇由Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton发表在NIPS上的<a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Paper</a>。该论文中提出了一种新型网络架构即 <strong>AlexNet</strong>.</p>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><p>AlexNet首次在大规模图像数据集实现了深层卷积神经网络结构，点燃了深度学习应用在计算机视觉领域的这把火。其在 <em>ImageNet LSVRC-2012</em> $^{[1]}$ 目标识别竞赛的 <em>top-5 error</em> $^{[2]}$ 为15.3%，同期第二名仅为26.2%，碾压其他传统的hand-craft 特征方法，使得计算机视觉从业者从繁重的特征工程中解脱出来，转向思考能够从数据中自动提取需要的特征，做到数据驱动。</p>\n<h3 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h3><p>对原始高清图像进行下采样得到固定的256*256的图像。具体方法是，给一个矩形图像，先调整短边长度为256然后裁剪出中间部分256*256的图片。最后按RGB像素值减去训练集中所有图像的均值图像像素。</p>\n<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p><img src=\"/images/alexnet_architecture.PNG\" alt=\"\"></p>\n<p>第二、第四和第五卷积层的内核只连接到位于同一GPU上的前一层内核映射。第三个卷积层的内核连接到第二层的所有内核映射。完全连接层的神经元与前一层的所有神经元连接。局部响应归一化接在第一，第二个卷积层后面。最大池化层跟随着局部响应层和第五个卷积层。</p>\n<h4 id=\"ReLU-Nonlinearity\"><a href=\"#ReLU-Nonlinearity\" class=\"headerlink\" title=\"ReLU Nonlinearity\"></a>ReLU Nonlinearity</h4><p>标准的神经元模型输出是sigmoid, tanh这些的函数。用梯度下降训练时在训练时间上，这些饱和非线性函数比非饱和非线性函数需要的时间要多得多。相比于tanh单元，深度卷积网络使用ReLU训练得更快。</p>\n<h4 id=\"在多个GPU上训练\"><a href=\"#在多个GPU上训练\" class=\"headerlink\" title=\"在多个GPU上训练\"></a>在多个GPU上训练</h4><p>当前的gpu特别适合于cross-gpu并行化，因为它们能够直接从彼此的内存中读取和写入，而无需通过主机内存。并行化方案实际上是将一半的内核(或神经元)放在每个GPU上，还有一个额外的技巧:GPU只在特定层进行通信。这个方案使top-1和top-5的错误率分别降低了1.7%和1.2%。</p>\n<h4 id=\"局部响应归一化\"><a href=\"#局部响应归一化\" class=\"headerlink\" title=\"局部响应归一化\"></a>局部响应归一化</h4><p>$$b^{i}_{x, y} = a^{i}_{x, y} / (k + \\alpha \\sum_{j = max(0, i - n/2)}^{min(N - 1, i + n/2)} (a^{i}_{x, y})^2)^{\\beta}$$</p>\n<p>$a^{i}_{x, y}$ 是应用卷积(包括非线性单元)操作后第i个通道位于(x, y)的值, $b^{i}_{x, y}$ 是应用局部响应归一化后的同一位置上值。常数$k, n, \\alpha, \\beta$ 是超参数。这里设置为$k = 2, n = 5, \\alpha=10^{-4}, \\beta=0.75$</p>\n<p>来源于生物学上的概念: <strong>侧抑制</strong>, 指被激化的神经元抑制相邻神经元的现象。这使得响应比较大的值相对更大，提高了模型的泛化能力。这个方案使top-1和top-5的错误率分别降低了1.4%和1.2%。</p>\n<h4 id=\"重叠池化\"><a href=\"#重叠池化\" class=\"headerlink\" title=\"重叠池化\"></a>重叠池化</h4><p>CNNs中的池化层汇总了同一核映射中相邻神经元群的输出。传统的池化，相邻池单元不会重叠，即步长$s=z$ 池化单元大小。如果$s &lt; z$则得到重叠的池化。在整个网络中使用$s = 2, z=3$。这个方案使top-1和top-5的错误率分别降低了0.4%和0.3%。</p>\n<h3 id=\"降低过拟合\"><a href=\"#降低过拟合\" class=\"headerlink\" title=\"降低过拟合\"></a>降低过拟合</h3><h4 id=\"数据增强\"><a href=\"#数据增强\" class=\"headerlink\" title=\"数据增强\"></a>数据增强</h4><p>关于图像数据最简单也最常用的降低过拟合的方法是通过保留标签对图像进行简单变换的方法人为地扩大数据集。</p>\n<p>第一种形式是：将图像进行水平翻转。通过从256*256的图像中随机抽取227*227的块，还有它们的水平翻转，用这些图像去训练网络。在测试时，通过抽取测试图像的四个角落和中间227*227的块，以及它们的水平翻转，一共10个块来输入进网络，最后以它们的平均值作为预测结果。</p>\n<p>第二种形式是：改变RGB通道的强度。具体来说，在整个训练集中对RGB像素值集执行PCA。对于每个训练图像，添加已找到的主成分的倍数。与对应的特征值成正比的大小乘以均值为0和标准差为0.1的高斯随机变量。</p>\n<p>对每个像素值 $I_{xy} = [I^{R}_{xy}, I^{G}_{xy}, I^{B}_{xy}]^T$ 加上以下数量：<br>$$[p_1, p_2, p_3][\\alpha_1 \\lambda_1, \\alpha_2 \\lambda_2, \\alpha_3 \\lambda_3]^T$$<br>$p_i, \\lambda_i$ 分别是RGB像素协方差矩阵的第i个特征向量和特征值, $\\alpha_i$ 是随机变量，每个 $\\alpha_i$ 只一个训练图像的一个像素。这个方案使top-1降低了1%。</p>\n<h4 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h4><p>以一定概率使神经元的输出置为0。这种技术减少了神经元复杂的协同适应，因为神经元不能依赖于特定的其他神经元的存在。因此，它不得不学习更健壮的特性，这些特性与其他神经元的许多不同随机子集一起使用。</p>\n<h3 id=\"注解\"><a href=\"#注解\" class=\"headerlink\" title=\"注解\"></a>注解</h3><p>[1]: ImageNet数据集大约包含2.2万种不同种类的1500万张高清图像。年度的图像识别竞赛<strong>the ImageNet Large-Scale Visual Recognition Challenge<br>(ILSVRC)</strong> 从2010开始举行。 ILSVRC 使用大约1000种类每类1000张图片的数据集。</p>\n<p>[2]: the top-5 错误率是：测试图像的预测结果前五位不包含真实标签的比例。</p>\n<p>[3]: The model result in ILSVRC-2010</p>\n<p><img src=\"/images/alexnet_ilsvrc.PNG\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>这是一篇由Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton发表在NIPS上的<a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Paper</a>。该论文中提出了一种新型网络架构即 <strong>AlexNet</strong>.</p>\n<h2 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h2><p>AlexNet首次在大规模图像数据集实现了深层卷积神经网络结构，点燃了深度学习应用在计算机视觉领域的这把火。其在 <em>ImageNet LSVRC-2012</em> $^{[1]}$ 目标识别竞赛的 <em>top-5 error</em> $^{[2]}$ 为15.3%，同期第二名仅为26.2%，碾压其他传统的hand-craft 特征方法，使得计算机视觉从业者从繁重的特征工程中解脱出来，转向思考能够从数据中自动提取需要的特征，做到数据驱动。</p>\n<h3 id=\"数据集\"><a href=\"#数据集\" class=\"headerlink\" title=\"数据集\"></a>数据集</h3><p>对原始高清图像进行下采样得到固定的256*256的图像。具体方法是，给一个矩形图像，先调整短边长度为256然后裁剪出中间部分256*256的图片。最后按RGB像素值减去训练集中所有图像的均值图像像素。</p>\n<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p><img src=\"/images/alexnet_architecture.PNG\" alt=\"\"></p>\n<p>第二、第四和第五卷积层的内核只连接到位于同一GPU上的前一层内核映射。第三个卷积层的内核连接到第二层的所有内核映射。完全连接层的神经元与前一层的所有神经元连接。局部响应归一化接在第一，第二个卷积层后面。最大池化层跟随着局部响应层和第五个卷积层。</p>\n<h4 id=\"ReLU-Nonlinearity\"><a href=\"#ReLU-Nonlinearity\" class=\"headerlink\" title=\"ReLU Nonlinearity\"></a>ReLU Nonlinearity</h4><p>标准的神经元模型输出是sigmoid, tanh这些的函数。用梯度下降训练时在训练时间上，这些饱和非线性函数比非饱和非线性函数需要的时间要多得多。相比于tanh单元，深度卷积网络使用ReLU训练得更快。</p>\n<h4 id=\"在多个GPU上训练\"><a href=\"#在多个GPU上训练\" class=\"headerlink\" title=\"在多个GPU上训练\"></a>在多个GPU上训练</h4><p>当前的gpu特别适合于cross-gpu并行化，因为它们能够直接从彼此的内存中读取和写入，而无需通过主机内存。并行化方案实际上是将一半的内核(或神经元)放在每个GPU上，还有一个额外的技巧:GPU只在特定层进行通信。这个方案使top-1和top-5的错误率分别降低了1.7%和1.2%。</p>\n<h4 id=\"局部响应归一化\"><a href=\"#局部响应归一化\" class=\"headerlink\" title=\"局部响应归一化\"></a>局部响应归一化</h4><p>$$b^{i}_{x, y} = a^{i}_{x, y} / (k + \\alpha \\sum_{j = max(0, i - n/2)}^{min(N - 1, i + n/2)} (a^{i}_{x, y})^2)^{\\beta}$$</p>\n<p>$a^{i}_{x, y}$ 是应用卷积(包括非线性单元)操作后第i个通道位于(x, y)的值, $b^{i}_{x, y}$ 是应用局部响应归一化后的同一位置上值。常数$k, n, \\alpha, \\beta$ 是超参数。这里设置为$k = 2, n = 5, \\alpha=10^{-4}, \\beta=0.75$</p>\n<p>来源于生物学上的概念: <strong>侧抑制</strong>, 指被激化的神经元抑制相邻神经元的现象。这使得响应比较大的值相对更大，提高了模型的泛化能力。这个方案使top-1和top-5的错误率分别降低了1.4%和1.2%。</p>\n<h4 id=\"重叠池化\"><a href=\"#重叠池化\" class=\"headerlink\" title=\"重叠池化\"></a>重叠池化</h4><p>CNNs中的池化层汇总了同一核映射中相邻神经元群的输出。传统的池化，相邻池单元不会重叠，即步长$s=z$ 池化单元大小。如果$s &lt; z$则得到重叠的池化。在整个网络中使用$s = 2, z=3$。这个方案使top-1和top-5的错误率分别降低了0.4%和0.3%。</p>\n<h3 id=\"降低过拟合\"><a href=\"#降低过拟合\" class=\"headerlink\" title=\"降低过拟合\"></a>降低过拟合</h3><h4 id=\"数据增强\"><a href=\"#数据增强\" class=\"headerlink\" title=\"数据增强\"></a>数据增强</h4><p>关于图像数据最简单也最常用的降低过拟合的方法是通过保留标签对图像进行简单变换的方法人为地扩大数据集。</p>\n<p>第一种形式是：将图像进行水平翻转。通过从256*256的图像中随机抽取227*227的块，还有它们的水平翻转，用这些图像去训练网络。在测试时，通过抽取测试图像的四个角落和中间227*227的块，以及它们的水平翻转，一共10个块来输入进网络，最后以它们的平均值作为预测结果。</p>\n<p>第二种形式是：改变RGB通道的强度。具体来说，在整个训练集中对RGB像素值集执行PCA。对于每个训练图像，添加已找到的主成分的倍数。与对应的特征值成正比的大小乘以均值为0和标准差为0.1的高斯随机变量。</p>\n<p>对每个像素值 $I_{xy} = [I^{R}_{xy}, I^{G}_{xy}, I^{B}_{xy}]^T$ 加上以下数量：<br>$$[p_1, p_2, p_3][\\alpha_1 \\lambda_1, \\alpha_2 \\lambda_2, \\alpha_3 \\lambda_3]^T$$<br>$p_i, \\lambda_i$ 分别是RGB像素协方差矩阵的第i个特征向量和特征值, $\\alpha_i$ 是随机变量，每个 $\\alpha_i$ 只一个训练图像的一个像素。这个方案使top-1降低了1%。</p>\n<h4 id=\"Dropout\"><a href=\"#Dropout\" class=\"headerlink\" title=\"Dropout\"></a>Dropout</h4><p>以一定概率使神经元的输出置为0。这种技术减少了神经元复杂的协同适应，因为神经元不能依赖于特定的其他神经元的存在。因此，它不得不学习更健壮的特性，这些特性与其他神经元的许多不同随机子集一起使用。</p>\n<h3 id=\"注解\"><a href=\"#注解\" class=\"headerlink\" title=\"注解\"></a>注解</h3><p>[1]: ImageNet数据集大约包含2.2万种不同种类的1500万张高清图像。年度的图像识别竞赛<strong>the ImageNet Large-Scale Visual Recognition Challenge<br>(ILSVRC)</strong> 从2010开始举行。 ILSVRC 使用大约1000种类每类1000张图片的数据集。</p>\n<p>[2]: the top-5 错误率是：测试图像的预测结果前五位不包含真实标签的比例。</p>\n<p>[3]: The model result in ILSVRC-2010</p>\n<p><img src=\"/images/alexnet_ilsvrc.PNG\" alt=\"\"></p>\n"},{"title":"Very Deep Convolutional Networks for Large-Scale Image Recongnition","date":"2018-08-31T02:08:37.000Z","mathjax":true,"_content":"## 概述\n\n这是一篇由牛津大学视觉几何组(VGG)2015年发表在ICLR上的论文。该论文中，他们主要研究了卷积网络深度对大尺度图像识别精度的影响。主要的贡献是，对使用同一小卷积核但深度不同的网络性能的完整评估，当把深度加到16-19层时，它相对于先前技术在性能上有了很大改善。文中的网络架构被称为 **VGG**, 它也在ImageNet Challenge 2014上取得了定位第一，分类第二的好成绩。\n\n## 网络配置\n\n所有的网络使用相同的卷积池化操作只是深度不同。使用'SAME'卷积, 卷积核大小均为 $3 \\times 3$ , 步长为1. 使用最大池化, 池化单元大小均为 $2 \\times 2$, 步长为2. 卷积核个数(通道数)从64开始以2倍增长直到512. 每层卷积层的非线性函数为ReLU. 最后三个全连接层的大小分别为4096, 4096, 1000.\n\n![](/images/architecture.PNG)\n\n亮点：**使用3个堆叠的3\\*3卷积而不是采样像alexnet中的7\\*7卷积.**\n\n理由：在3个堆叠的卷积层中都包含了ReLU非线性单元，这使决策函数更具有分辨力；其次减少了参数，相比于7\\*7的卷积需要参数( $7 \\times7 C^2$ ), 3个3\\*3卷积层需要更少的参数( $3\\times 3 \\times 3 C^2$ )\n\n## 分类任务的训练和测试\n\n### 训练阶段\n\n利用带动量的小批量梯度下降法优化多项逻辑回归目标。$batch size=256, momentum=0.9, L2=5*10^{-4}, dropout=0.5, learning\\_rate=0.01$\n先训练小网络A, 再将训练后A的权重给其他网络初始化, 其他还未初始化的层，权重使用均值为0方差为0.01的正态分布初始化，偏差初始化为0.\n在每个SGD迭代过程，对每幅输入图像进行随机裁剪到固定大小224\\*224.\n\n训练图像的大小(S: 是经过各向同性调整后的最小边长)：\n\n方法一：单尺度训练，固定S=256/S=384. 先用S=256训练，将参数保留然后再用S=384进行微调(学习率下降到0.001)。\n\n方法二：多尺度训练，S从[256, 512]中随机取值，先用S=384训练再将其参数保留用多尺度方法训练。\n\n### 测试阶段\n\n给一个测试图像，先将其进行各向同性调整到预定义的最小尺寸Q. 将全连接层转换为卷积层(第一个全连接层变为7\\*7的卷积层，第二，三个全连接层变为1\\*1的卷积层，核的大小均与之前全连接层的单元数相等)。将整个未裁剪的图像应用在整个卷积网络上。结果是一个类的得分映射，通道数等于类数，以及一个可变的空间分辨率，取决于输入的图像大小。最后，为了获得图像的类分数的固定大小向量，类分数被进行空间平均（一个类的得分为该通道上像素的平均值）。还能通过水平翻转的方式来增强测试集。\n\n亮点：**测试时将全连接层转换为卷积层。**\n\n理由：在测试时不需要生成多个裁剪的图像重复进行计算。对输入图像的大小没有限制。\n\n## 分类任务的评估\n\n该数据集包含1000个类的图像，并分为三组:训练(130万张图像)、验证(50K图像)和测试(100K带有提示标签的图像)。\n\n### 单尺度评估(Q不变)\n\n$对于固定的S, Q=S; 对于 S \\in [S_{min}, S_{max}], Q = 0.5(S_{min} + S_{max})$\n![](/images/vgg_performance_1.PNG)\n\n### 多尺度评估(Q变化)\n\n$对于固定的S, Q=\\{S-32, S, S+32\\}; 对于 S \\in [S_{min}, S_{max}], Q = \\{S_{min}, 0.5(S_{min} + S_{max}), S_{max}\\}$\n![](/images/vgg_performance_2.PNG)\n\n### Multi-crop 评估(S变化)\n\n![](/images/vgg_performance_3.PNG)\n","source":"_posts/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recongnition.md","raw":"---\ntitle: Very Deep Convolutional Networks for Large-Scale Image Recongnition\ndate: 2018-08-31 10:08:37\ntags: CNN, VGG\ncategories: 深度学习\nmathjax: true\n---\n## 概述\n\n这是一篇由牛津大学视觉几何组(VGG)2015年发表在ICLR上的论文。该论文中，他们主要研究了卷积网络深度对大尺度图像识别精度的影响。主要的贡献是，对使用同一小卷积核但深度不同的网络性能的完整评估，当把深度加到16-19层时，它相对于先前技术在性能上有了很大改善。文中的网络架构被称为 **VGG**, 它也在ImageNet Challenge 2014上取得了定位第一，分类第二的好成绩。\n\n## 网络配置\n\n所有的网络使用相同的卷积池化操作只是深度不同。使用'SAME'卷积, 卷积核大小均为 $3 \\times 3$ , 步长为1. 使用最大池化, 池化单元大小均为 $2 \\times 2$, 步长为2. 卷积核个数(通道数)从64开始以2倍增长直到512. 每层卷积层的非线性函数为ReLU. 最后三个全连接层的大小分别为4096, 4096, 1000.\n\n![](/images/architecture.PNG)\n\n亮点：**使用3个堆叠的3\\*3卷积而不是采样像alexnet中的7\\*7卷积.**\n\n理由：在3个堆叠的卷积层中都包含了ReLU非线性单元，这使决策函数更具有分辨力；其次减少了参数，相比于7\\*7的卷积需要参数( $7 \\times7 C^2$ ), 3个3\\*3卷积层需要更少的参数( $3\\times 3 \\times 3 C^2$ )\n\n## 分类任务的训练和测试\n\n### 训练阶段\n\n利用带动量的小批量梯度下降法优化多项逻辑回归目标。$batch size=256, momentum=0.9, L2=5*10^{-4}, dropout=0.5, learning\\_rate=0.01$\n先训练小网络A, 再将训练后A的权重给其他网络初始化, 其他还未初始化的层，权重使用均值为0方差为0.01的正态分布初始化，偏差初始化为0.\n在每个SGD迭代过程，对每幅输入图像进行随机裁剪到固定大小224\\*224.\n\n训练图像的大小(S: 是经过各向同性调整后的最小边长)：\n\n方法一：单尺度训练，固定S=256/S=384. 先用S=256训练，将参数保留然后再用S=384进行微调(学习率下降到0.001)。\n\n方法二：多尺度训练，S从[256, 512]中随机取值，先用S=384训练再将其参数保留用多尺度方法训练。\n\n### 测试阶段\n\n给一个测试图像，先将其进行各向同性调整到预定义的最小尺寸Q. 将全连接层转换为卷积层(第一个全连接层变为7\\*7的卷积层，第二，三个全连接层变为1\\*1的卷积层，核的大小均与之前全连接层的单元数相等)。将整个未裁剪的图像应用在整个卷积网络上。结果是一个类的得分映射，通道数等于类数，以及一个可变的空间分辨率，取决于输入的图像大小。最后，为了获得图像的类分数的固定大小向量，类分数被进行空间平均（一个类的得分为该通道上像素的平均值）。还能通过水平翻转的方式来增强测试集。\n\n亮点：**测试时将全连接层转换为卷积层。**\n\n理由：在测试时不需要生成多个裁剪的图像重复进行计算。对输入图像的大小没有限制。\n\n## 分类任务的评估\n\n该数据集包含1000个类的图像，并分为三组:训练(130万张图像)、验证(50K图像)和测试(100K带有提示标签的图像)。\n\n### 单尺度评估(Q不变)\n\n$对于固定的S, Q=S; 对于 S \\in [S_{min}, S_{max}], Q = 0.5(S_{min} + S_{max})$\n![](/images/vgg_performance_1.PNG)\n\n### 多尺度评估(Q变化)\n\n$对于固定的S, Q=\\{S-32, S, S+32\\}; 对于 S \\in [S_{min}, S_{max}], Q = \\{S_{min}, 0.5(S_{min} + S_{max}), S_{max}\\}$\n![](/images/vgg_performance_2.PNG)\n\n### Multi-crop 评估(S变化)\n\n![](/images/vgg_performance_3.PNG)\n","slug":"Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recongnition","published":1,"updated":"2018-08-31T03:52:46.414Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh2c000hzkvo2nqvzqlu","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>这是一篇由牛津大学视觉几何组(VGG)2015年发表在ICLR上的论文。该论文中，他们主要研究了卷积网络深度对大尺度图像识别精度的影响。主要的贡献是，对使用同一小卷积核但深度不同的网络性能的完整评估，当把深度加到16-19层时，它相对于先前技术在性能上有了很大改善。文中的网络架构被称为 <strong>VGG</strong>, 它也在ImageNet Challenge 2014上取得了定位第一，分类第二的好成绩。</p>\n<h2 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a>网络配置</h2><p>所有的网络使用相同的卷积池化操作只是深度不同。使用’SAME’卷积, 卷积核大小均为 $3 \\times 3$ , 步长为1. 使用最大池化, 池化单元大小均为 $2 \\times 2$, 步长为2. 卷积核个数(通道数)从64开始以2倍增长直到512. 每层卷积层的非线性函数为ReLU. 最后三个全连接层的大小分别为4096, 4096, 1000.</p>\n<p><img src=\"/images/architecture.PNG\" alt=\"\"></p>\n<p>亮点：<strong>使用3个堆叠的3*3卷积而不是采样像alexnet中的7*7卷积.</strong></p>\n<p>理由：在3个堆叠的卷积层中都包含了ReLU非线性单元，这使决策函数更具有分辨力；其次减少了参数，相比于7*7的卷积需要参数( $7 \\times7 C^2$ ), 3个3*3卷积层需要更少的参数( $3\\times 3 \\times 3 C^2$ )</p>\n<h2 id=\"分类任务的训练和测试\"><a href=\"#分类任务的训练和测试\" class=\"headerlink\" title=\"分类任务的训练和测试\"></a>分类任务的训练和测试</h2><h3 id=\"训练阶段\"><a href=\"#训练阶段\" class=\"headerlink\" title=\"训练阶段\"></a>训练阶段</h3><p>利用带动量的小批量梯度下降法优化多项逻辑回归目标。$batch size=256, momentum=0.9, L2=5<em>10^{-4}, dropout=0.5, learning_rate=0.01$<br>先训练小网络A, 再将训练后A的权重给其他网络初始化, 其他还未初始化的层，权重使用均值为0方差为0.01的正态分布初始化，偏差初始化为0.<br>在每个SGD迭代过程，对每幅输入图像进行随机裁剪到固定大小224\\</em>224.</p>\n<p>训练图像的大小(S: 是经过各向同性调整后的最小边长)：</p>\n<p>方法一：单尺度训练，固定S=256/S=384. 先用S=256训练，将参数保留然后再用S=384进行微调(学习率下降到0.001)。</p>\n<p>方法二：多尺度训练，S从[256, 512]中随机取值，先用S=384训练再将其参数保留用多尺度方法训练。</p>\n<h3 id=\"测试阶段\"><a href=\"#测试阶段\" class=\"headerlink\" title=\"测试阶段\"></a>测试阶段</h3><p>给一个测试图像，先将其进行各向同性调整到预定义的最小尺寸Q. 将全连接层转换为卷积层(第一个全连接层变为7*7的卷积层，第二，三个全连接层变为1*1的卷积层，核的大小均与之前全连接层的单元数相等)。将整个未裁剪的图像应用在整个卷积网络上。结果是一个类的得分映射，通道数等于类数，以及一个可变的空间分辨率，取决于输入的图像大小。最后，为了获得图像的类分数的固定大小向量，类分数被进行空间平均（一个类的得分为该通道上像素的平均值）。还能通过水平翻转的方式来增强测试集。</p>\n<p>亮点：<strong>测试时将全连接层转换为卷积层。</strong></p>\n<p>理由：在测试时不需要生成多个裁剪的图像重复进行计算。对输入图像的大小没有限制。</p>\n<h2 id=\"分类任务的评估\"><a href=\"#分类任务的评估\" class=\"headerlink\" title=\"分类任务的评估\"></a>分类任务的评估</h2><p>该数据集包含1000个类的图像，并分为三组:训练(130万张图像)、验证(50K图像)和测试(100K带有提示标签的图像)。</p>\n<h3 id=\"单尺度评估-Q不变\"><a href=\"#单尺度评估-Q不变\" class=\"headerlink\" title=\"单尺度评估(Q不变)\"></a>单尺度评估(Q不变)</h3><p>$对于固定的S, Q=S; 对于 S \\in [S_{min}, S_{max}], Q = 0.5(S_{min} + S_{max})$<br><img src=\"/images/vgg_performance_1.PNG\" alt=\"\"></p>\n<h3 id=\"多尺度评估-Q变化\"><a href=\"#多尺度评估-Q变化\" class=\"headerlink\" title=\"多尺度评估(Q变化)\"></a>多尺度评估(Q变化)</h3><p>$对于固定的S, Q={S-32, S, S+32}; 对于 S \\in [S_{min}, S_{max}], Q = {S_{min}, 0.5(S_{min} + S_{max}), S_{max}}$<br><img src=\"/images/vgg_performance_2.PNG\" alt=\"\"></p>\n<h3 id=\"Multi-crop-评估-S变化\"><a href=\"#Multi-crop-评估-S变化\" class=\"headerlink\" title=\"Multi-crop 评估(S变化)\"></a>Multi-crop 评估(S变化)</h3><p><img src=\"/images/vgg_performance_3.PNG\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>这是一篇由牛津大学视觉几何组(VGG)2015年发表在ICLR上的论文。该论文中，他们主要研究了卷积网络深度对大尺度图像识别精度的影响。主要的贡献是，对使用同一小卷积核但深度不同的网络性能的完整评估，当把深度加到16-19层时，它相对于先前技术在性能上有了很大改善。文中的网络架构被称为 <strong>VGG</strong>, 它也在ImageNet Challenge 2014上取得了定位第一，分类第二的好成绩。</p>\n<h2 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a>网络配置</h2><p>所有的网络使用相同的卷积池化操作只是深度不同。使用’SAME’卷积, 卷积核大小均为 $3 \\times 3$ , 步长为1. 使用最大池化, 池化单元大小均为 $2 \\times 2$, 步长为2. 卷积核个数(通道数)从64开始以2倍增长直到512. 每层卷积层的非线性函数为ReLU. 最后三个全连接层的大小分别为4096, 4096, 1000.</p>\n<p><img src=\"/images/architecture.PNG\" alt=\"\"></p>\n<p>亮点：<strong>使用3个堆叠的3*3卷积而不是采样像alexnet中的7*7卷积.</strong></p>\n<p>理由：在3个堆叠的卷积层中都包含了ReLU非线性单元，这使决策函数更具有分辨力；其次减少了参数，相比于7*7的卷积需要参数( $7 \\times7 C^2$ ), 3个3*3卷积层需要更少的参数( $3\\times 3 \\times 3 C^2$ )</p>\n<h2 id=\"分类任务的训练和测试\"><a href=\"#分类任务的训练和测试\" class=\"headerlink\" title=\"分类任务的训练和测试\"></a>分类任务的训练和测试</h2><h3 id=\"训练阶段\"><a href=\"#训练阶段\" class=\"headerlink\" title=\"训练阶段\"></a>训练阶段</h3><p>利用带动量的小批量梯度下降法优化多项逻辑回归目标。$batch size=256, momentum=0.9, L2=5<em>10^{-4}, dropout=0.5, learning_rate=0.01$<br>先训练小网络A, 再将训练后A的权重给其他网络初始化, 其他还未初始化的层，权重使用均值为0方差为0.01的正态分布初始化，偏差初始化为0.<br>在每个SGD迭代过程，对每幅输入图像进行随机裁剪到固定大小224\\</em>224.</p>\n<p>训练图像的大小(S: 是经过各向同性调整后的最小边长)：</p>\n<p>方法一：单尺度训练，固定S=256/S=384. 先用S=256训练，将参数保留然后再用S=384进行微调(学习率下降到0.001)。</p>\n<p>方法二：多尺度训练，S从[256, 512]中随机取值，先用S=384训练再将其参数保留用多尺度方法训练。</p>\n<h3 id=\"测试阶段\"><a href=\"#测试阶段\" class=\"headerlink\" title=\"测试阶段\"></a>测试阶段</h3><p>给一个测试图像，先将其进行各向同性调整到预定义的最小尺寸Q. 将全连接层转换为卷积层(第一个全连接层变为7*7的卷积层，第二，三个全连接层变为1*1的卷积层，核的大小均与之前全连接层的单元数相等)。将整个未裁剪的图像应用在整个卷积网络上。结果是一个类的得分映射，通道数等于类数，以及一个可变的空间分辨率，取决于输入的图像大小。最后，为了获得图像的类分数的固定大小向量，类分数被进行空间平均（一个类的得分为该通道上像素的平均值）。还能通过水平翻转的方式来增强测试集。</p>\n<p>亮点：<strong>测试时将全连接层转换为卷积层。</strong></p>\n<p>理由：在测试时不需要生成多个裁剪的图像重复进行计算。对输入图像的大小没有限制。</p>\n<h2 id=\"分类任务的评估\"><a href=\"#分类任务的评估\" class=\"headerlink\" title=\"分类任务的评估\"></a>分类任务的评估</h2><p>该数据集包含1000个类的图像，并分为三组:训练(130万张图像)、验证(50K图像)和测试(100K带有提示标签的图像)。</p>\n<h3 id=\"单尺度评估-Q不变\"><a href=\"#单尺度评估-Q不变\" class=\"headerlink\" title=\"单尺度评估(Q不变)\"></a>单尺度评估(Q不变)</h3><p>$对于固定的S, Q=S; 对于 S \\in [S_{min}, S_{max}], Q = 0.5(S_{min} + S_{max})$<br><img src=\"/images/vgg_performance_1.PNG\" alt=\"\"></p>\n<h3 id=\"多尺度评估-Q变化\"><a href=\"#多尺度评估-Q变化\" class=\"headerlink\" title=\"多尺度评估(Q变化)\"></a>多尺度评估(Q变化)</h3><p>$对于固定的S, Q={S-32, S, S+32}; 对于 S \\in [S_{min}, S_{max}], Q = {S_{min}, 0.5(S_{min} + S_{max}), S_{max}}$<br><img src=\"/images/vgg_performance_2.PNG\" alt=\"\"></p>\n<h3 id=\"Multi-crop-评估-S变化\"><a href=\"#Multi-crop-评估-S变化\" class=\"headerlink\" title=\"Multi-crop 评估(S变化)\"></a>Multi-crop 评估(S变化)</h3><p><img src=\"/images/vgg_performance_3.PNG\" alt=\"\"></p>\n"},{"title":"dropout 正则化","date":"2018-07-20T08:18:24.000Z","mathjax":true,"_content":"## dropout 正则化\n\n**dropout（随机失活）**是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在**计算机视觉（Computer Vision）**领域。\n\n![dropout_regularization](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png)\n\n### 反向随机失活（Inverted dropout）\n\n反向随机失活是实现 dropout 的方法。对第`l`层进行 dropout：\n\n```python\nkeep_prob = 0.8    # 设置神经元保留概率\ndl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob\nal = np.multiply(al, dl)\nal /= keep_prob\n\n# 反向传播过程为\ndal = dal * dl\ndal /= keep_prob\n```\n\n最后一步`al /= keep_prob`是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个`keep_prob`。\n\n**注意**，在**测试阶段不要使用 dropout**，因为那样会使得预测结果变得随机。\n\n### 理解 dropout\n\n对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。\n\n因此，通过传播过程，dropout 将产生和 L2 正则化相同的**收缩权重**的效果。\n\n对于不同的层，设置的`keep_prob`也不同。一般来说，神经元较少的层，会设`keep_prob`为 1.0，而神经元多的层则会设置比较小的`keep_prob`。\n\ndropout 的一大**缺点**是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将`keep_prob`全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。\n\n## 其他正则化方法\n\n* 数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。\n* 早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。\n","source":"_posts/dropout.md","raw":"---\ntitle: dropout 正则化\ndate: 2018-07-20 16:18:24\ntags: dropout\ncategories: 深度学习\nmathjax: true\n---\n## dropout 正则化\n\n**dropout（随机失活）**是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在**计算机视觉（Computer Vision）**领域。\n\n![dropout_regularization](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png)\n\n### 反向随机失活（Inverted dropout）\n\n反向随机失活是实现 dropout 的方法。对第`l`层进行 dropout：\n\n```python\nkeep_prob = 0.8    # 设置神经元保留概率\ndl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob\nal = np.multiply(al, dl)\nal /= keep_prob\n\n# 反向传播过程为\ndal = dal * dl\ndal /= keep_prob\n```\n\n最后一步`al /= keep_prob`是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个`keep_prob`。\n\n**注意**，在**测试阶段不要使用 dropout**，因为那样会使得预测结果变得随机。\n\n### 理解 dropout\n\n对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。\n\n因此，通过传播过程，dropout 将产生和 L2 正则化相同的**收缩权重**的效果。\n\n对于不同的层，设置的`keep_prob`也不同。一般来说，神经元较少的层，会设`keep_prob`为 1.0，而神经元多的层则会设置比较小的`keep_prob`。\n\ndropout 的一大**缺点**是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将`keep_prob`全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。\n\n## 其他正则化方法\n\n* 数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。\n* 早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。\n","slug":"dropout","published":1,"updated":"2018-08-31T03:49:05.231Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh2h000jzkvoj92ty2ho","content":"<h2 id=\"dropout-正则化\"><a href=\"#dropout-正则化\" class=\"headerlink\" title=\"dropout 正则化\"></a>dropout 正则化</h2><p><strong>dropout（随机失活）</strong>是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在<strong>计算机视觉（Computer Vision）</strong>领域。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png\" alt=\"dropout_regularization\"></p>\n<h3 id=\"反向随机失活（Inverted-dropout）\"><a href=\"#反向随机失活（Inverted-dropout）\" class=\"headerlink\" title=\"反向随机失活（Inverted dropout）\"></a>反向随机失活（Inverted dropout）</h3><p>反向随机失活是实现 dropout 的方法。对第<code>l</code>层进行 dropout：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keep_prob = <span class=\"number\">0.8</span>    <span class=\"comment\"># 设置神经元保留概率</span></span><br><span class=\"line\">dl = np.random.rand(al.shape[<span class=\"number\">0</span>], al.shape[<span class=\"number\">1</span>]) &lt; keep_prob</span><br><span class=\"line\">al = np.multiply(al, dl)</span><br><span class=\"line\">al /= keep_prob</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 反向传播过程为</span></span><br><span class=\"line\">dal = dal * dl</span><br><span class=\"line\">dal /= keep_prob</span><br></pre></td></tr></table></figure>\n<p>最后一步<code>al /= keep_prob</code>是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个<code>keep_prob</code>。</p>\n<p><strong>注意</strong>，在<strong>测试阶段不要使用 dropout</strong>，因为那样会使得预测结果变得随机。</p>\n<h3 id=\"理解-dropout\"><a href=\"#理解-dropout\" class=\"headerlink\" title=\"理解 dropout\"></a>理解 dropout</h3><p>对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。</p>\n<p>因此，通过传播过程，dropout 将产生和 L2 正则化相同的<strong>收缩权重</strong>的效果。</p>\n<p>对于不同的层，设置的<code>keep_prob</code>也不同。一般来说，神经元较少的层，会设<code>keep_prob</code>为 1.0，而神经元多的层则会设置比较小的<code>keep_prob</code>。</p>\n<p>dropout 的一大<strong>缺点</strong>是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将<code>keep_prob</code>全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。</p>\n<h2 id=\"其他正则化方法\"><a href=\"#其他正则化方法\" class=\"headerlink\" title=\"其他正则化方法\"></a>其他正则化方法</h2><ul>\n<li>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</li>\n<li>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"dropout-正则化\"><a href=\"#dropout-正则化\" class=\"headerlink\" title=\"dropout 正则化\"></a>dropout 正则化</h2><p><strong>dropout（随机失活）</strong>是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在<strong>计算机视觉（Computer Vision）</strong>领域。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dropout_regularization.png\" alt=\"dropout_regularization\"></p>\n<h3 id=\"反向随机失活（Inverted-dropout）\"><a href=\"#反向随机失活（Inverted-dropout）\" class=\"headerlink\" title=\"反向随机失活（Inverted dropout）\"></a>反向随机失活（Inverted dropout）</h3><p>反向随机失活是实现 dropout 的方法。对第<code>l</code>层进行 dropout：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">keep_prob = <span class=\"number\">0.8</span>    <span class=\"comment\"># 设置神经元保留概率</span></span><br><span class=\"line\">dl = np.random.rand(al.shape[<span class=\"number\">0</span>], al.shape[<span class=\"number\">1</span>]) &lt; keep_prob</span><br><span class=\"line\">al = np.multiply(al, dl)</span><br><span class=\"line\">al /= keep_prob</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 反向传播过程为</span></span><br><span class=\"line\">dal = dal * dl</span><br><span class=\"line\">dal /= keep_prob</span><br></pre></td></tr></table></figure>\n<p>最后一步<code>al /= keep_prob</code>是因为 $a^{[l]}$中的一部分元素失活（相当于被归零），为了在下一层计算时不影响 $Z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$的期望值，因此除以一个<code>keep_prob</code>。</p>\n<p><strong>注意</strong>，在<strong>测试阶段不要使用 dropout</strong>，因为那样会使得预测结果变得随机。</p>\n<h3 id=\"理解-dropout\"><a href=\"#理解-dropout\" class=\"headerlink\" title=\"理解 dropout\"></a>理解 dropout</h3><p>对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。</p>\n<p>因此，通过传播过程，dropout 将产生和 L2 正则化相同的<strong>收缩权重</strong>的效果。</p>\n<p>对于不同的层，设置的<code>keep_prob</code>也不同。一般来说，神经元较少的层，会设<code>keep_prob</code>为 1.0，而神经元多的层则会设置比较小的<code>keep_prob</code>。</p>\n<p>dropout 的一大<strong>缺点</strong>是成本函数无法被明确定义。因为每次迭代都会随机消除一些神经元结点的影响，因此无法确保成本函数单调递减。因此，使用 dropout 时，先将<code>keep_prob</code>全部设置为 1.0 后运行代码，确保 $J(w, b)$函数单调递减，再打开 dropout。</p>\n<h2 id=\"其他正则化方法\"><a href=\"#其他正则化方法\" class=\"headerlink\" title=\"其他正则化方法\"></a>其他正则化方法</h2><ul>\n<li>数据扩增（Data Augmentation）：通过图片的一些变换（翻转，局部放大后切割等），得到更多的训练集和验证集。</li>\n<li>早停止法（Early Stopping）：将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。</li>\n</ul>\n"},{"title":"github使用手册","date":"2018-08-05T02:45:20.000Z","_content":"## git clone\n\n### clone地址https和SSH的区别\n\n前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。\n\nhttps url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。\n\n### 在github上添加ssh key的方法\n\n1. \t首先需要检查你电脑是否已经有 SSH key \n\n`cd ~/.ssh/ | ls` 检查是否有文件id_rsa.pub, 若存在则跳过第二步\n\n2. 创建一个ssh key\n\n`ssh-keygen -t rsa -C \"your_email@example.com\"` 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub\n\n3. 添加ssh key到github\n\n拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。\n\n4. 测试ssh key\n\n`ssh -T git@github.com`\n\n### clone指定分支\n\n`git clone -b <分支名> <address.git>`\n\n## 添加新的分支\n\n1. 先将仓库克隆到本地\n2. `git branch`查看分支。`git branch <分支名>` 新建分支\n3. `git checkout <分支名>` 切换到新分支\n4. `git push -u origin <分支名>` 同步分支到github\n","source":"_posts/github使用手册.md","raw":"---\ntitle: github使用手册\ndate: 2018-08-05 10:45:20\ntags: git\ncategories: 程序员实用工具\n---\n## git clone\n\n### clone地址https和SSH的区别\n\n前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。\n\nhttps url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。\n\n### 在github上添加ssh key的方法\n\n1. \t首先需要检查你电脑是否已经有 SSH key \n\n`cd ~/.ssh/ | ls` 检查是否有文件id_rsa.pub, 若存在则跳过第二步\n\n2. 创建一个ssh key\n\n`ssh-keygen -t rsa -C \"your_email@example.com\"` 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub\n\n3. 添加ssh key到github\n\n拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。\n\n4. 测试ssh key\n\n`ssh -T git@github.com`\n\n### clone指定分支\n\n`git clone -b <分支名> <address.git>`\n\n## 添加新的分支\n\n1. 先将仓库克隆到本地\n2. `git branch`查看分支。`git branch <分支名>` 新建分支\n3. `git checkout <分支名>` 切换到新分支\n4. `git push -u origin <分支名>` 同步分支到github\n","slug":"github使用手册","published":1,"updated":"2018-08-19T01:59:35.791Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh2o000nzkvo48z7y4wk","content":"<h2 id=\"git-clone\"><a href=\"#git-clone\" class=\"headerlink\" title=\"git clone\"></a>git clone</h2><h3 id=\"clone地址https和SSH的区别\"><a href=\"#clone地址https和SSH的区别\" class=\"headerlink\" title=\"clone地址https和SSH的区别\"></a>clone地址https和SSH的区别</h3><p>前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。</p>\n<p>https url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。</p>\n<h3 id=\"在github上添加ssh-key的方法\"><a href=\"#在github上添加ssh-key的方法\" class=\"headerlink\" title=\"在github上添加ssh key的方法\"></a>在github上添加ssh key的方法</h3><ol>\n<li>首先需要检查你电脑是否已经有 SSH key </li>\n</ol>\n<p><code>cd ~/.ssh/ | ls</code> 检查是否有文件id_rsa.pub, 若存在则跳过第二步</p>\n<ol start=\"2\">\n<li>创建一个ssh key</li>\n</ol>\n<p><code>ssh-keygen -t rsa -C &quot;your_email@example.com&quot;</code> 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub</p>\n<ol start=\"3\">\n<li>添加ssh key到github</li>\n</ol>\n<p>拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。</p>\n<ol start=\"4\">\n<li>测试ssh key</li>\n</ol>\n<p><code>ssh -T git@github.com</code></p>\n<h3 id=\"clone指定分支\"><a href=\"#clone指定分支\" class=\"headerlink\" title=\"clone指定分支\"></a>clone指定分支</h3><p><code>git clone -b &lt;分支名&gt; &lt;address.git&gt;</code></p>\n<h2 id=\"添加新的分支\"><a href=\"#添加新的分支\" class=\"headerlink\" title=\"添加新的分支\"></a>添加新的分支</h2><ol>\n<li>先将仓库克隆到本地</li>\n<li><code>git branch</code>查看分支。<code>git branch &lt;分支名&gt;</code> 新建分支</li>\n<li><code>git checkout &lt;分支名&gt;</code> 切换到新分支</li>\n<li><code>git push -u origin &lt;分支名&gt;</code> 同步分支到github</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"git-clone\"><a href=\"#git-clone\" class=\"headerlink\" title=\"git clone\"></a>git clone</h2><h3 id=\"clone地址https和SSH的区别\"><a href=\"#clone地址https和SSH的区别\" class=\"headerlink\" title=\"clone地址https和SSH的区别\"></a>clone地址https和SSH的区别</h3><p>前者可以随意克隆github上的项目，而不管是谁的；而后者则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。</p>\n<p>https url 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。</p>\n<h3 id=\"在github上添加ssh-key的方法\"><a href=\"#在github上添加ssh-key的方法\" class=\"headerlink\" title=\"在github上添加ssh key的方法\"></a>在github上添加ssh key的方法</h3><ol>\n<li>首先需要检查你电脑是否已经有 SSH key </li>\n</ol>\n<p><code>cd ~/.ssh/ | ls</code> 检查是否有文件id_rsa.pub, 若存在则跳过第二步</p>\n<ol start=\"2\">\n<li>创建一个ssh key</li>\n</ol>\n<p><code>ssh-keygen -t rsa -C &quot;your_email@example.com&quot;</code> 使用默认设置，可设置密码用于push操作。完成后将得到两个文件，放在./ssh目录下，分别为id_rsa和id_rsa.pub</p>\n<ol start=\"3\">\n<li>添加ssh key到github</li>\n</ol>\n<p>拷贝id_rsa.pub文件的内容，复制到github账户的sshkey设置页面处。</p>\n<ol start=\"4\">\n<li>测试ssh key</li>\n</ol>\n<p><code>ssh -T git@github.com</code></p>\n<h3 id=\"clone指定分支\"><a href=\"#clone指定分支\" class=\"headerlink\" title=\"clone指定分支\"></a>clone指定分支</h3><p><code>git clone -b &lt;分支名&gt; &lt;address.git&gt;</code></p>\n<h2 id=\"添加新的分支\"><a href=\"#添加新的分支\" class=\"headerlink\" title=\"添加新的分支\"></a>添加新的分支</h2><ol>\n<li>先将仓库克隆到本地</li>\n<li><code>git branch</code>查看分支。<code>git branch &lt;分支名&gt;</code> 新建分支</li>\n<li><code>git checkout &lt;分支名&gt;</code> 切换到新分支</li>\n<li><code>git push -u origin &lt;分支名&gt;</code> 同步分支到github</li>\n</ol>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncategories: web\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2018-08-19T01:59:35.792Z","updated":"2018-08-19T01:59:35.792Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh2s000qzkvoulr3be2r","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"},{"title":"matplotlib","date":"2018-08-15T23:51:58.000Z","_content":"\n## 快速绘图\n\n### 使用pyplot模块绘图\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.figure(figsize=(8,4))\nplt.plot(x, y, label=\"$sin(x)$\", color=\"red\", linewidth=2)\nplt.xlabel(\"Time(s)\")\nplt.ylabel(\"Volt\")\nplt.title(\"pyplot first example\")\nplt.ylim(-1.2, 1.2)\nplt.legend()\nplt.show()\n```\n\n保存图片`plt.savefig('test.png', dpi=120)`的像素值由参数`matplotlib.rcParams[\"savefig.dpi\"]`决定，默认为100.\n保存对象不一定是文件，还可是和文件对象有相同调用接口的对象.\n\n```python\nfrom StringIO import StringIO\nbuf = StringIO()\nplt.savefig(buf, fmt='png')\nbuf.getvalue()[:20]\n```\n\n### 以面向对象方式绘图\n\n```python\nfig = plt.gcf()  # get current figure\naxes = plt.gca()  # get current axes\n```\n\n在pyplot模块中，许多函数都是对当前的Figure和Axes对象进行处理.\n\n### 配置属性\n\n使用matplotlib绘制的图表的每个组成部分都和一个对象对应，可以通过调用这些对象的属性设置方法`set_*()`或pyplot模块的属性设置函数`setp()`来设它们的属性值.\n\n```\nx = np.arange(0, 5, 0.1)\nline = plt.plot(x, x*x)[0]\nline.set_antialiased(False)\n\nlines = plt.plot(x, np.sin(x), x, np.cos(x))\nplt.setp(lines, color=\"r\", linewidth=2.0)\n```\n\n同样可以调用Line2D对象的`get_*()`或`plt.getp()`来获取对象的属性值.\n\n```python\nline.get_linewidth()\n\n# getp()只能对一个对象操作\nplt.getp(lines[0], \"color\")\nplt.getp(lines[1])  # 输出全部属性\n\nf = plt.gcf()\nplt.getp(f)\n\nallines = plt.getp(plt.gca(), \"lines\")\nallines = f.axes[0].lines\n```\n\n### 绘制多个子图\n\n一个Figure对象可以包含多个子图Axes.\n\n`subplot(numRows, numCols, plotNum)`\n\n`subplot(323), subplot(3, 2, 3)`\n\n```python\n# 绘制6个子图并设置不同的背景颜色\nfor idx, color in enumerate(\"rgbyck\"):\n    plt.subplot(321 + idx, axisbg=color)\nplt.show()\n```\n\n`plt.subplot(212)  # 占据第二整行`\n\n```python\n同时在多幅图表、多个子图中进行绘制\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(1)  # 创建图表1\nplt.figure(2)\nax1 = plt.subplot(211)  # 在图表2中创建子图1\nax2 = plt.subplot(212)\n\nx = np.linspace(0, 3, 100)\nfor i in xrange(5):\n    plt.figure(1)  # 选择图表1\n    plt.plot(x, np.exp(i * x / 3)\n    plt.sca(ax1)  # 选择图表2的子图1\n    plt.plot(x, np.sin(x * i))\n    plt.sca(ax2)  # 选择图表2的子图2\n    plt.plot(x, np.cos(i * x))\nplt.show()\n```\n\n### 配置文件\n\n绘制一幅图表要对许多对象的属性进行配置。我们通常采用了默认配置，matplotlib将这些默认配置保存在一个名为“matplotlibrc”的配置文件中。\n\n```python\nmatplotlib.get_configdir()  # 获取用户配置路径\nmatplotlib.matplotlib_fname()  # 获得目前使用的配置文件的路径\nmatplotlib.rc_params()  # 配置文件的读入，返回字典\nmatplotlib.rc(\"lines\", marker='x', linewidth=2, color=\"red\")  # 对配置字典进行设置\nmatplotlib.rcdefaults()  # 回复默认配置\n``````\n\n### 在图表中显示中文\n\n```pythno\nfrom matplotlib.font_manager import fontManager\n# 获得所有可用的字体列表\nfontManager.ttflist\n\n# 获得字体文件的全路径和字体名\nfontManager.ttflist[0].name\nfontManager.ttflist[0].fname\n\n\n```python\n# 显示所有的中文字体\nfrom matplotlib.font_manager import fontManager\nimport matplotlib.pyplot as plt\nimport os\n\nfig = plt.figure(figsize=(12, 6))\nax = fig.add_subplot(111)\nplt.subplot_adjust(0, 0, 1, 1, 0, 0)\nplt.xticks([])\nplt.yticks([])\nx, y = 0.05, 0.08\nfonts = [font.name for font in fontManager.ttflist if os.path.exists(font.fname) and os.stat(font.fname).st_size>1e6]\nfont = set(fonts)\ndy = (1.0 - y) / (len(fonts) / 4 + (len(fonts) % 4 != 0))\nfor font in fonts:\n    t = ax.text(x, y, u\"中文字体\", {'fontname': font, 'fontsize': 14}, transform=ax.transAxes)\n    ax.test(x, y - dy / 2, font, transform=ax.transAxes)\n    x += 0.25\n    if x >= 1.0:\n        y += dy\n        x = 0.05\nplt.show()\n```\n\n```python\n# 使用ttc字体文件\nfrom matplotlib.font_Manager import FontProperties\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfont = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)\nt = np.linspace(0, 10, 100)\ny = np.sin(t)\nplt.plot(t, y)\nplt.title(u\"正弦波\", fontproperties=font)\nplt.show()\n```\n\n直接修改配置文件，设置默认字体。\n\n`plt.rcParams[\"font.family\"] = \"SimHei\"`\n\n## Artist对象\n","source":"_posts/matplotlib.md","raw":"---\ntitle: matplotlib\ndate: 2018-08-16 07:51:58\ntags: python\ncategories: python包和模块\n---\n\n## 快速绘图\n\n### 使用pyplot模块绘图\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.figure(figsize=(8,4))\nplt.plot(x, y, label=\"$sin(x)$\", color=\"red\", linewidth=2)\nplt.xlabel(\"Time(s)\")\nplt.ylabel(\"Volt\")\nplt.title(\"pyplot first example\")\nplt.ylim(-1.2, 1.2)\nplt.legend()\nplt.show()\n```\n\n保存图片`plt.savefig('test.png', dpi=120)`的像素值由参数`matplotlib.rcParams[\"savefig.dpi\"]`决定，默认为100.\n保存对象不一定是文件，还可是和文件对象有相同调用接口的对象.\n\n```python\nfrom StringIO import StringIO\nbuf = StringIO()\nplt.savefig(buf, fmt='png')\nbuf.getvalue()[:20]\n```\n\n### 以面向对象方式绘图\n\n```python\nfig = plt.gcf()  # get current figure\naxes = plt.gca()  # get current axes\n```\n\n在pyplot模块中，许多函数都是对当前的Figure和Axes对象进行处理.\n\n### 配置属性\n\n使用matplotlib绘制的图表的每个组成部分都和一个对象对应，可以通过调用这些对象的属性设置方法`set_*()`或pyplot模块的属性设置函数`setp()`来设它们的属性值.\n\n```\nx = np.arange(0, 5, 0.1)\nline = plt.plot(x, x*x)[0]\nline.set_antialiased(False)\n\nlines = plt.plot(x, np.sin(x), x, np.cos(x))\nplt.setp(lines, color=\"r\", linewidth=2.0)\n```\n\n同样可以调用Line2D对象的`get_*()`或`plt.getp()`来获取对象的属性值.\n\n```python\nline.get_linewidth()\n\n# getp()只能对一个对象操作\nplt.getp(lines[0], \"color\")\nplt.getp(lines[1])  # 输出全部属性\n\nf = plt.gcf()\nplt.getp(f)\n\nallines = plt.getp(plt.gca(), \"lines\")\nallines = f.axes[0].lines\n```\n\n### 绘制多个子图\n\n一个Figure对象可以包含多个子图Axes.\n\n`subplot(numRows, numCols, plotNum)`\n\n`subplot(323), subplot(3, 2, 3)`\n\n```python\n# 绘制6个子图并设置不同的背景颜色\nfor idx, color in enumerate(\"rgbyck\"):\n    plt.subplot(321 + idx, axisbg=color)\nplt.show()\n```\n\n`plt.subplot(212)  # 占据第二整行`\n\n```python\n同时在多幅图表、多个子图中进行绘制\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(1)  # 创建图表1\nplt.figure(2)\nax1 = plt.subplot(211)  # 在图表2中创建子图1\nax2 = plt.subplot(212)\n\nx = np.linspace(0, 3, 100)\nfor i in xrange(5):\n    plt.figure(1)  # 选择图表1\n    plt.plot(x, np.exp(i * x / 3)\n    plt.sca(ax1)  # 选择图表2的子图1\n    plt.plot(x, np.sin(x * i))\n    plt.sca(ax2)  # 选择图表2的子图2\n    plt.plot(x, np.cos(i * x))\nplt.show()\n```\n\n### 配置文件\n\n绘制一幅图表要对许多对象的属性进行配置。我们通常采用了默认配置，matplotlib将这些默认配置保存在一个名为“matplotlibrc”的配置文件中。\n\n```python\nmatplotlib.get_configdir()  # 获取用户配置路径\nmatplotlib.matplotlib_fname()  # 获得目前使用的配置文件的路径\nmatplotlib.rc_params()  # 配置文件的读入，返回字典\nmatplotlib.rc(\"lines\", marker='x', linewidth=2, color=\"red\")  # 对配置字典进行设置\nmatplotlib.rcdefaults()  # 回复默认配置\n``````\n\n### 在图表中显示中文\n\n```pythno\nfrom matplotlib.font_manager import fontManager\n# 获得所有可用的字体列表\nfontManager.ttflist\n\n# 获得字体文件的全路径和字体名\nfontManager.ttflist[0].name\nfontManager.ttflist[0].fname\n\n\n```python\n# 显示所有的中文字体\nfrom matplotlib.font_manager import fontManager\nimport matplotlib.pyplot as plt\nimport os\n\nfig = plt.figure(figsize=(12, 6))\nax = fig.add_subplot(111)\nplt.subplot_adjust(0, 0, 1, 1, 0, 0)\nplt.xticks([])\nplt.yticks([])\nx, y = 0.05, 0.08\nfonts = [font.name for font in fontManager.ttflist if os.path.exists(font.fname) and os.stat(font.fname).st_size>1e6]\nfont = set(fonts)\ndy = (1.0 - y) / (len(fonts) / 4 + (len(fonts) % 4 != 0))\nfor font in fonts:\n    t = ax.text(x, y, u\"中文字体\", {'fontname': font, 'fontsize': 14}, transform=ax.transAxes)\n    ax.test(x, y - dy / 2, font, transform=ax.transAxes)\n    x += 0.25\n    if x >= 1.0:\n        y += dy\n        x = 0.05\nplt.show()\n```\n\n```python\n# 使用ttc字体文件\nfrom matplotlib.font_Manager import FontProperties\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfont = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=14)\nt = np.linspace(0, 10, 100)\ny = np.sin(t)\nplt.plot(t, y)\nplt.title(u\"正弦波\", fontproperties=font)\nplt.show()\n```\n\n直接修改配置文件，设置默认字体。\n\n`plt.rcParams[\"font.family\"] = \"SimHei\"`\n\n## Artist对象\n","slug":"matplotlib","published":1,"updated":"2018-08-31T03:51:45.318Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh2y000uzkvopzwrhw7y","content":"<h2 id=\"快速绘图\"><a href=\"#快速绘图\" class=\"headerlink\" title=\"快速绘图\"></a>快速绘图</h2><h3 id=\"使用pyplot模块绘图\"><a href=\"#使用pyplot模块绘图\" class=\"headerlink\" title=\"使用pyplot模块绘图\"></a>使用pyplot模块绘图</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">y = np.sin(x)</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">plt.plot(x, y, label=<span class=\"string\">\"$sin(x)$\"</span>, color=<span class=\"string\">\"red\"</span>, linewidth=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Time(s)\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Volt\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"pyplot first example\"</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-1.2</span>, <span class=\"number\">1.2</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>保存图片<code>plt.savefig(&#39;test.png&#39;, dpi=120)</code>的像素值由参数<code>matplotlib.rcParams[&quot;savefig.dpi&quot;]</code>决定，默认为100.<br>保存对象不一定是文件，还可是和文件对象有相同调用接口的对象.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> StringIO <span class=\"keyword\">import</span> StringIO</span><br><span class=\"line\">buf = StringIO()</span><br><span class=\"line\">plt.savefig(buf, fmt=<span class=\"string\">'png'</span>)</span><br><span class=\"line\">buf.getvalue()[:<span class=\"number\">20</span>]</span><br></pre></td></tr></table></figure>\n<h3 id=\"以面向对象方式绘图\"><a href=\"#以面向对象方式绘图\" class=\"headerlink\" title=\"以面向对象方式绘图\"></a>以面向对象方式绘图</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig = plt.gcf()  <span class=\"comment\"># get current figure</span></span><br><span class=\"line\">axes = plt.gca()  <span class=\"comment\"># get current axes</span></span><br></pre></td></tr></table></figure>\n<p>在pyplot模块中，许多函数都是对当前的Figure和Axes对象进行处理.</p>\n<h3 id=\"配置属性\"><a href=\"#配置属性\" class=\"headerlink\" title=\"配置属性\"></a>配置属性</h3><p>使用matplotlib绘制的图表的每个组成部分都和一个对象对应，可以通过调用这些对象的属性设置方法<code>set_*()</code>或pyplot模块的属性设置函数<code>setp()</code>来设它们的属性值.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(0, 5, 0.1)</span><br><span class=\"line\">line = plt.plot(x, x*x)[0]</span><br><span class=\"line\">line.set_antialiased(False)</span><br><span class=\"line\"></span><br><span class=\"line\">lines = plt.plot(x, np.sin(x), x, np.cos(x))</span><br><span class=\"line\">plt.setp(lines, color=&quot;r&quot;, linewidth=2.0)</span><br></pre></td></tr></table></figure>\n<p>同样可以调用Line2D对象的<code>get_*()</code>或<code>plt.getp()</code>来获取对象的属性值.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">line.get_linewidth()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># getp()只能对一个对象操作</span></span><br><span class=\"line\">plt.getp(lines[<span class=\"number\">0</span>], <span class=\"string\">\"color\"</span>)</span><br><span class=\"line\">plt.getp(lines[<span class=\"number\">1</span>])  <span class=\"comment\"># 输出全部属性</span></span><br><span class=\"line\"></span><br><span class=\"line\">f = plt.gcf()</span><br><span class=\"line\">plt.getp(f)</span><br><span class=\"line\"></span><br><span class=\"line\">allines = plt.getp(plt.gca(), <span class=\"string\">\"lines\"</span>)</span><br><span class=\"line\">allines = f.axes[<span class=\"number\">0</span>].lines</span><br></pre></td></tr></table></figure>\n<h3 id=\"绘制多个子图\"><a href=\"#绘制多个子图\" class=\"headerlink\" title=\"绘制多个子图\"></a>绘制多个子图</h3><p>一个Figure对象可以包含多个子图Axes.</p>\n<p><code>subplot(numRows, numCols, plotNum)</code></p>\n<p><code>subplot(323), subplot(3, 2, 3)</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 绘制6个子图并设置不同的背景颜色</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> idx, color <span class=\"keyword\">in</span> enumerate(<span class=\"string\">\"rgbyck\"</span>):</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">321</span> + idx, axisbg=color)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><code>plt.subplot(212)  # 占据第二整行</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">同时在多幅图表、多个子图中进行绘制</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(<span class=\"number\">1</span>)  <span class=\"comment\"># 创建图表1</span></span><br><span class=\"line\">plt.figure(<span class=\"number\">2</span>)</span><br><span class=\"line\">ax1 = plt.subplot(<span class=\"number\">211</span>)  <span class=\"comment\"># 在图表2中创建子图1</span></span><br><span class=\"line\">ax2 = plt.subplot(<span class=\"number\">212</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(<span class=\"number\">5</span>):</span><br><span class=\"line\">    plt.figure(<span class=\"number\">1</span>)  <span class=\"comment\"># 选择图表1</span></span><br><span class=\"line\">    plt.plot(x, np.exp(i * x / <span class=\"number\">3</span>)</span><br><span class=\"line\">    plt.sca(ax1)  <span class=\"comment\"># 选择图表2的子图1</span></span><br><span class=\"line\">    plt.plot(x, np.sin(x * i))</span><br><span class=\"line\">    plt.sca(ax2)  <span class=\"comment\"># 选择图表2的子图2</span></span><br><span class=\"line\">    plt.plot(x, np.cos(i * x))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>绘制一幅图表要对许多对象的属性进行配置。我们通常采用了默认配置，matplotlib将这些默认配置保存在一个名为“matplotlibrc”的配置文件中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">matplotlib.get_configdir()  <span class=\"comment\"># 获取用户配置路径</span></span><br><span class=\"line\">matplotlib.matplotlib_fname()  <span class=\"comment\"># 获得目前使用的配置文件的路径</span></span><br><span class=\"line\">matplotlib.rc_params()  <span class=\"comment\"># 配置文件的读入，返回字典</span></span><br><span class=\"line\">matplotlib.rc(<span class=\"string\">\"lines\"</span>, marker=<span class=\"string\">'x'</span>, linewidth=<span class=\"number\">2</span>, color=<span class=\"string\">\"red\"</span>)  <span class=\"comment\"># 对配置字典进行设置</span></span><br><span class=\"line\">matplotlib.rcdefaults()  <span class=\"comment\"># 回复默认配置</span></span><br><span class=\"line\">```</span><br></pre></td></tr></table></figure>\n<h3 id=\"在图表中显示中文\"><a href=\"#在图表中显示中文\" class=\"headerlink\" title=\"在图表中显示中文\"></a>在图表中显示中文</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from matplotlib.font_manager import fontManager</span><br><span class=\"line\"># 获得所有可用的字体列表</span><br><span class=\"line\">fontManager.ttflist</span><br><span class=\"line\"></span><br><span class=\"line\"># 获得字体文件的全路径和字体名</span><br><span class=\"line\">fontManager.ttflist[0].name</span><br><span class=\"line\">fontManager.ttflist[0].fname</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">```python</span><br><span class=\"line\"># 显示所有的中文字体</span><br><span class=\"line\">from matplotlib.font_manager import fontManager</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">fig = plt.figure(figsize=(12, 6))</span><br><span class=\"line\">ax = fig.add_subplot(111)</span><br><span class=\"line\">plt.subplot_adjust(0, 0, 1, 1, 0, 0)</span><br><span class=\"line\">plt.xticks([])</span><br><span class=\"line\">plt.yticks([])</span><br><span class=\"line\">x, y = 0.05, 0.08</span><br><span class=\"line\">fonts = [font.name for font in fontManager.ttflist if os.path.exists(font.fname) and os.stat(font.fname).st_size&gt;1e6]</span><br><span class=\"line\">font = set(fonts)</span><br><span class=\"line\">dy = (1.0 - y) / (len(fonts) / 4 + (len(fonts) % 4 != 0))</span><br><span class=\"line\">for font in fonts:</span><br><span class=\"line\">    t = ax.text(x, y, u&quot;中文字体&quot;, &#123;&apos;fontname&apos;: font, &apos;fontsize&apos;: 14&#125;, transform=ax.transAxes)</span><br><span class=\"line\">    ax.test(x, y - dy / 2, font, transform=ax.transAxes)</span><br><span class=\"line\">    x += 0.25</span><br><span class=\"line\">    if x &gt;= 1.0:</span><br><span class=\"line\">        y += dy</span><br><span class=\"line\">        x = 0.05</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用ttc字体文件</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.font_Manager <span class=\"keyword\">import</span> FontProperties</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">font = FontProperties(fname=<span class=\"string\">r\"c:\\windows\\fonts\\simsun.ttc\"</span>, size=<span class=\"number\">14</span>)</span><br><span class=\"line\">t = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">y = np.sin(t)</span><br><span class=\"line\">plt.plot(t, y)</span><br><span class=\"line\">plt.title(<span class=\"string\">u\"正弦波\"</span>, fontproperties=font)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>直接修改配置文件，设置默认字体。</p>\n<p><code>plt.rcParams[&quot;font.family&quot;] = &quot;SimHei&quot;</code></p>\n<h2 id=\"Artist对象\"><a href=\"#Artist对象\" class=\"headerlink\" title=\"Artist对象\"></a>Artist对象</h2>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"快速绘图\"><a href=\"#快速绘图\" class=\"headerlink\" title=\"快速绘图\"></a>快速绘图</h2><h3 id=\"使用pyplot模块绘图\"><a href=\"#使用pyplot模块绘图\" class=\"headerlink\" title=\"使用pyplot模块绘图\"></a>使用pyplot模块绘图</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">y = np.sin(x)</span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>,<span class=\"number\">4</span>))</span><br><span class=\"line\">plt.plot(x, y, label=<span class=\"string\">\"$sin(x)$\"</span>, color=<span class=\"string\">\"red\"</span>, linewidth=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">\"Time(s)\"</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">\"Volt\"</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"pyplot first example\"</span>)</span><br><span class=\"line\">plt.ylim(<span class=\"number\">-1.2</span>, <span class=\"number\">1.2</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>保存图片<code>plt.savefig(&#39;test.png&#39;, dpi=120)</code>的像素值由参数<code>matplotlib.rcParams[&quot;savefig.dpi&quot;]</code>决定，默认为100.<br>保存对象不一定是文件，还可是和文件对象有相同调用接口的对象.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> StringIO <span class=\"keyword\">import</span> StringIO</span><br><span class=\"line\">buf = StringIO()</span><br><span class=\"line\">plt.savefig(buf, fmt=<span class=\"string\">'png'</span>)</span><br><span class=\"line\">buf.getvalue()[:<span class=\"number\">20</span>]</span><br></pre></td></tr></table></figure>\n<h3 id=\"以面向对象方式绘图\"><a href=\"#以面向对象方式绘图\" class=\"headerlink\" title=\"以面向对象方式绘图\"></a>以面向对象方式绘图</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig = plt.gcf()  <span class=\"comment\"># get current figure</span></span><br><span class=\"line\">axes = plt.gca()  <span class=\"comment\"># get current axes</span></span><br></pre></td></tr></table></figure>\n<p>在pyplot模块中，许多函数都是对当前的Figure和Axes对象进行处理.</p>\n<h3 id=\"配置属性\"><a href=\"#配置属性\" class=\"headerlink\" title=\"配置属性\"></a>配置属性</h3><p>使用matplotlib绘制的图表的每个组成部分都和一个对象对应，可以通过调用这些对象的属性设置方法<code>set_*()</code>或pyplot模块的属性设置函数<code>setp()</code>来设它们的属性值.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = np.arange(0, 5, 0.1)</span><br><span class=\"line\">line = plt.plot(x, x*x)[0]</span><br><span class=\"line\">line.set_antialiased(False)</span><br><span class=\"line\"></span><br><span class=\"line\">lines = plt.plot(x, np.sin(x), x, np.cos(x))</span><br><span class=\"line\">plt.setp(lines, color=&quot;r&quot;, linewidth=2.0)</span><br></pre></td></tr></table></figure>\n<p>同样可以调用Line2D对象的<code>get_*()</code>或<code>plt.getp()</code>来获取对象的属性值.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">line.get_linewidth()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># getp()只能对一个对象操作</span></span><br><span class=\"line\">plt.getp(lines[<span class=\"number\">0</span>], <span class=\"string\">\"color\"</span>)</span><br><span class=\"line\">plt.getp(lines[<span class=\"number\">1</span>])  <span class=\"comment\"># 输出全部属性</span></span><br><span class=\"line\"></span><br><span class=\"line\">f = plt.gcf()</span><br><span class=\"line\">plt.getp(f)</span><br><span class=\"line\"></span><br><span class=\"line\">allines = plt.getp(plt.gca(), <span class=\"string\">\"lines\"</span>)</span><br><span class=\"line\">allines = f.axes[<span class=\"number\">0</span>].lines</span><br></pre></td></tr></table></figure>\n<h3 id=\"绘制多个子图\"><a href=\"#绘制多个子图\" class=\"headerlink\" title=\"绘制多个子图\"></a>绘制多个子图</h3><p>一个Figure对象可以包含多个子图Axes.</p>\n<p><code>subplot(numRows, numCols, plotNum)</code></p>\n<p><code>subplot(323), subplot(3, 2, 3)</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 绘制6个子图并设置不同的背景颜色</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> idx, color <span class=\"keyword\">in</span> enumerate(<span class=\"string\">\"rgbyck\"</span>):</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">321</span> + idx, axisbg=color)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><code>plt.subplot(212)  # 占据第二整行</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">同时在多幅图表、多个子图中进行绘制</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(<span class=\"number\">1</span>)  <span class=\"comment\"># 创建图表1</span></span><br><span class=\"line\">plt.figure(<span class=\"number\">2</span>)</span><br><span class=\"line\">ax1 = plt.subplot(<span class=\"number\">211</span>)  <span class=\"comment\"># 在图表2中创建子图1</span></span><br><span class=\"line\">ax2 = plt.subplot(<span class=\"number\">212</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">x = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(<span class=\"number\">5</span>):</span><br><span class=\"line\">    plt.figure(<span class=\"number\">1</span>)  <span class=\"comment\"># 选择图表1</span></span><br><span class=\"line\">    plt.plot(x, np.exp(i * x / <span class=\"number\">3</span>)</span><br><span class=\"line\">    plt.sca(ax1)  <span class=\"comment\"># 选择图表2的子图1</span></span><br><span class=\"line\">    plt.plot(x, np.sin(x * i))</span><br><span class=\"line\">    plt.sca(ax2)  <span class=\"comment\"># 选择图表2的子图2</span></span><br><span class=\"line\">    plt.plot(x, np.cos(i * x))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"配置文件\"><a href=\"#配置文件\" class=\"headerlink\" title=\"配置文件\"></a>配置文件</h3><p>绘制一幅图表要对许多对象的属性进行配置。我们通常采用了默认配置，matplotlib将这些默认配置保存在一个名为“matplotlibrc”的配置文件中。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">matplotlib.get_configdir()  <span class=\"comment\"># 获取用户配置路径</span></span><br><span class=\"line\">matplotlib.matplotlib_fname()  <span class=\"comment\"># 获得目前使用的配置文件的路径</span></span><br><span class=\"line\">matplotlib.rc_params()  <span class=\"comment\"># 配置文件的读入，返回字典</span></span><br><span class=\"line\">matplotlib.rc(<span class=\"string\">\"lines\"</span>, marker=<span class=\"string\">'x'</span>, linewidth=<span class=\"number\">2</span>, color=<span class=\"string\">\"red\"</span>)  <span class=\"comment\"># 对配置字典进行设置</span></span><br><span class=\"line\">matplotlib.rcdefaults()  <span class=\"comment\"># 回复默认配置</span></span><br><span class=\"line\">```</span><br></pre></td></tr></table></figure>\n<h3 id=\"在图表中显示中文\"><a href=\"#在图表中显示中文\" class=\"headerlink\" title=\"在图表中显示中文\"></a>在图表中显示中文</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from matplotlib.font_manager import fontManager</span><br><span class=\"line\"># 获得所有可用的字体列表</span><br><span class=\"line\">fontManager.ttflist</span><br><span class=\"line\"></span><br><span class=\"line\"># 获得字体文件的全路径和字体名</span><br><span class=\"line\">fontManager.ttflist[0].name</span><br><span class=\"line\">fontManager.ttflist[0].fname</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">```python</span><br><span class=\"line\"># 显示所有的中文字体</span><br><span class=\"line\">from matplotlib.font_manager import fontManager</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">import os</span><br><span class=\"line\"></span><br><span class=\"line\">fig = plt.figure(figsize=(12, 6))</span><br><span class=\"line\">ax = fig.add_subplot(111)</span><br><span class=\"line\">plt.subplot_adjust(0, 0, 1, 1, 0, 0)</span><br><span class=\"line\">plt.xticks([])</span><br><span class=\"line\">plt.yticks([])</span><br><span class=\"line\">x, y = 0.05, 0.08</span><br><span class=\"line\">fonts = [font.name for font in fontManager.ttflist if os.path.exists(font.fname) and os.stat(font.fname).st_size&gt;1e6]</span><br><span class=\"line\">font = set(fonts)</span><br><span class=\"line\">dy = (1.0 - y) / (len(fonts) / 4 + (len(fonts) % 4 != 0))</span><br><span class=\"line\">for font in fonts:</span><br><span class=\"line\">    t = ax.text(x, y, u&quot;中文字体&quot;, &#123;&apos;fontname&apos;: font, &apos;fontsize&apos;: 14&#125;, transform=ax.transAxes)</span><br><span class=\"line\">    ax.test(x, y - dy / 2, font, transform=ax.transAxes)</span><br><span class=\"line\">    x += 0.25</span><br><span class=\"line\">    if x &gt;= 1.0:</span><br><span class=\"line\">        y += dy</span><br><span class=\"line\">        x = 0.05</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用ttc字体文件</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib.font_Manager <span class=\"keyword\">import</span> FontProperties</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"></span><br><span class=\"line\">font = FontProperties(fname=<span class=\"string\">r\"c:\\windows\\fonts\\simsun.ttc\"</span>, size=<span class=\"number\">14</span>)</span><br><span class=\"line\">t = np.linspace(<span class=\"number\">0</span>, <span class=\"number\">10</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\">y = np.sin(t)</span><br><span class=\"line\">plt.plot(t, y)</span><br><span class=\"line\">plt.title(<span class=\"string\">u\"正弦波\"</span>, fontproperties=font)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p>直接修改配置文件，设置默认字体。</p>\n<p><code>plt.rcParams[&quot;font.family&quot;] = &quot;SimHei&quot;</code></p>\n<h2 id=\"Artist对象\"><a href=\"#Artist对象\" class=\"headerlink\" title=\"Artist对象\"></a>Artist对象</h2>"},{"title":"python内置小工具","date":"2018-08-04T23:24:47.000Z","_content":"\n## 极简文件下载（Web）服务器\n\n### 作用\n\n快速共享文件\n\n### 实用方法\n\nIn python2：\n\n`python -m SimpleHttpServer`\n\nIn python3:\n\n`python -m http.server`\n\n执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。**若当前目录存在一个名为`index.html`的文件，则默认会显示该文件的内容**\n\n## 使用python解压zip压缩包\n\n`$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources\n`\n","source":"_posts/python内置小工具.md","raw":"---\ntitle: python内置小工具\ndate: 2018-08-05 07:24:47\ntags: python\ncategories: 程序员实用工具\n---\n\n## 极简文件下载（Web）服务器\n\n### 作用\n\n快速共享文件\n\n### 实用方法\n\nIn python2：\n\n`python -m SimpleHttpServer`\n\nIn python3:\n\n`python -m http.server`\n\n执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。**若当前目录存在一个名为`index.html`的文件，则默认会显示该文件的内容**\n\n## 使用python解压zip压缩包\n\n`$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources\n`\n","slug":"python内置小工具","published":1,"updated":"2018-08-31T03:51:42.853Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh31000xzkvoxs8df41n","content":"<h2 id=\"极简文件下载（Web）服务器\"><a href=\"#极简文件下载（Web）服务器\" class=\"headerlink\" title=\"极简文件下载（Web）服务器\"></a>极简文件下载（Web）服务器</h2><h3 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h3><p>快速共享文件</p>\n<h3 id=\"实用方法\"><a href=\"#实用方法\" class=\"headerlink\" title=\"实用方法\"></a>实用方法</h3><p>In python2：</p>\n<p><code>python -m SimpleHttpServer</code></p>\n<p>In python3:</p>\n<p><code>python -m http.server</code></p>\n<p>执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。<strong>若当前目录存在一个名为<code>index.html</code>的文件，则默认会显示该文件的内容</strong></p>\n<h2 id=\"使用python解压zip压缩包\"><a href=\"#使用python解压zip压缩包\" class=\"headerlink\" title=\"使用python解压zip压缩包\"></a>使用python解压zip压缩包</h2><p><code>$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources</code></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"极简文件下载（Web）服务器\"><a href=\"#极简文件下载（Web）服务器\" class=\"headerlink\" title=\"极简文件下载（Web）服务器\"></a>极简文件下载（Web）服务器</h2><h3 id=\"作用\"><a href=\"#作用\" class=\"headerlink\" title=\"作用\"></a>作用</h3><p>快速共享文件</p>\n<h3 id=\"实用方法\"><a href=\"#实用方法\" class=\"headerlink\" title=\"实用方法\"></a>实用方法</h3><p>In python2：</p>\n<p><code>python -m SimpleHttpServer</code></p>\n<p>In python3:</p>\n<p><code>python -m http.server</code></p>\n<p>执行上述命令会在当前目录启动一个文件下载服务器，默认端口8000。<strong>若当前目录存在一个名为<code>index.html</code>的文件，则默认会显示该文件的内容</strong></p>\n<h2 id=\"使用python解压zip压缩包\"><a href=\"#使用python解压zip压缩包\" class=\"headerlink\" title=\"使用python解压zip压缩包\"></a>使用python解压zip压缩包</h2><p><code>$ python -m zipfile\nUsage:\n    zipfile.py -l zipfile.zip        # Show listing of a zipfile\n    zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n    zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n    zipfile.py -c zipfile.zip src ... # Create zipfile from sources</code></p>\n"},{"title":"Requests","date":"2018-08-14T00:15:18.000Z","_content":"\n## [Requests](cn.python-requests.org)\n\nRequests is an elegant and simple HTTP library for python, built for human beings.\n\n## Beloved Features\n\n* Keep-Alive & Connection Pooling\n* International Domains and URLs\n* Session with Cookie Persistence\n* Browser-style SSL Verification\n* Automatic Content Decoding\n* Basic/Digest Authentication\n* Elegant Key/Value Cookie\n* Automatic Decompression\n* Unicode Response Bodies\n* HTTP(S) Proxy Support\n* Multipart File Uploads\n* Streaming Downloads\n* Connection Timeouts\n* Chunked Requests\n* `.netrc` Support\n\n## 快速上手\n\n### 发送请求\n\n```python\nimport requests\nr = requests.get(url)\n```\n`r`为Response对象，requests的方法还有`put, delete, head, options`\n\n### 传递URL参数\n\n如url为`http://httpbin.org/get?key2=value2&key1=value1`\n```python\npayload = {'key1':'value1', 'key2':'value2'}\nr = requests.get(\"http://httpbin.org/get\", params=payload)\n```\n\n### 响应内容\n\n`r.text` 响应文本\n`r.encoding` 编码格式\n\n### 二进制响应内容\n\n`r.content` 以字节的方式访问响应体\n\n```python\nfrom PIL import Image\nfrom io import BytesIO\ni = Image.open(BytesIO(r.content))\n```\n\n### JSON响应内容\n\n`r.josn()`\n\n### 原始响应内容\n\n```python\nr = requests.get(url, stream=True)\nr.raw\nr.raw.read(10)\n\nwith open(filename, 'wb') as fd:\n    for chunk in r.iter_content(chunk_size):\n        fd.write(chunk)\n```\n\n### 定制请求头\n\n`headers = {'user-agent': 'my_-app/0.0.1'}`\n`r = requests.get(url, headers=headers)`\n\n### 更复杂的POST请求\n\n```python\npayload = {'key1':'value1', 'key2':'value2'}\nr = requests.post(\"http://httpbin.org/get\", data=payload)\n\npayload = (('key1', 'value1'), ('key2', 'value2'))\nr = requests.post(\"http://httpbin.org/get\", data=payload)\n\nr = requests.post(url, json=payload)\n```\n\n### POST一个多部分编码的文件\n\n```python\nfiles = {'file': open('report.xls', 'rb')}\nr = requests.post(url, files=files)\n\n# 你还可以显式地设置文件名，文件类型和请求头\nfiles = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})}\n```\n\n### 响应状态码\n\n`r.status_code` 检查响应状态码\n`r.status_code == requests.codes.ok` 内置的状态码查询对象\n`r.raise_for_status()` 抛出异常\n\n### 响应头\n\n`r.headers`\n```json\n{\n    'content-encoding': 'gzip',\n    'transfer-encoding': 'chunked',\n    'connection': 'close',\n    'server': 'nginx/1.0.4',\n    'x-runtime': '148ms',\n    'etag': '\"e1ca502697e5c9317743dc078f67693f\"',\n    'content-type': 'application/json'\n}\n```\n\n### Cookie\n\n`r.cookies`\n```python\ncookies = dict(cookies_are='working')\nr = requests.get(url, cookies=cookies)\n```\n\nCookie 的返回对象为 RequestsCookieJar，它的行为和字典类似，但接口更为完整，适合跨域名跨路径使用。你还可以把 Cookie Jar 传到 Requests 中：\n\n```python\njar = requests.cookies.RequestsCookieJar()\njar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')\nr = requests.get(url, cookies=jar)\n```\n\n### 重定向和请求历史\n\nResponse.history 是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。\n\n```python\nr = requests.get(url, allow_redirects=False)\nr.history\n```\n\n### 超时\n\n`r = requests.get(url, timeouts=0.01)`\n\n### 错误与异常\n\n遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常\n\n如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常\n\n若请求超时，则抛出一个 Timeout 异常。\n\n若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。\n\n所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。\n","source":"_posts/requests.md","raw":"---\ntitle: Requests\ndate: 2018-08-14 08:15:18\ntags: python\ncategories: python包和模块\n---\n\n## [Requests](cn.python-requests.org)\n\nRequests is an elegant and simple HTTP library for python, built for human beings.\n\n## Beloved Features\n\n* Keep-Alive & Connection Pooling\n* International Domains and URLs\n* Session with Cookie Persistence\n* Browser-style SSL Verification\n* Automatic Content Decoding\n* Basic/Digest Authentication\n* Elegant Key/Value Cookie\n* Automatic Decompression\n* Unicode Response Bodies\n* HTTP(S) Proxy Support\n* Multipart File Uploads\n* Streaming Downloads\n* Connection Timeouts\n* Chunked Requests\n* `.netrc` Support\n\n## 快速上手\n\n### 发送请求\n\n```python\nimport requests\nr = requests.get(url)\n```\n`r`为Response对象，requests的方法还有`put, delete, head, options`\n\n### 传递URL参数\n\n如url为`http://httpbin.org/get?key2=value2&key1=value1`\n```python\npayload = {'key1':'value1', 'key2':'value2'}\nr = requests.get(\"http://httpbin.org/get\", params=payload)\n```\n\n### 响应内容\n\n`r.text` 响应文本\n`r.encoding` 编码格式\n\n### 二进制响应内容\n\n`r.content` 以字节的方式访问响应体\n\n```python\nfrom PIL import Image\nfrom io import BytesIO\ni = Image.open(BytesIO(r.content))\n```\n\n### JSON响应内容\n\n`r.josn()`\n\n### 原始响应内容\n\n```python\nr = requests.get(url, stream=True)\nr.raw\nr.raw.read(10)\n\nwith open(filename, 'wb') as fd:\n    for chunk in r.iter_content(chunk_size):\n        fd.write(chunk)\n```\n\n### 定制请求头\n\n`headers = {'user-agent': 'my_-app/0.0.1'}`\n`r = requests.get(url, headers=headers)`\n\n### 更复杂的POST请求\n\n```python\npayload = {'key1':'value1', 'key2':'value2'}\nr = requests.post(\"http://httpbin.org/get\", data=payload)\n\npayload = (('key1', 'value1'), ('key2', 'value2'))\nr = requests.post(\"http://httpbin.org/get\", data=payload)\n\nr = requests.post(url, json=payload)\n```\n\n### POST一个多部分编码的文件\n\n```python\nfiles = {'file': open('report.xls', 'rb')}\nr = requests.post(url, files=files)\n\n# 你还可以显式地设置文件名，文件类型和请求头\nfiles = {'file': ('report.xls', open('report.xls', 'rb'), 'application/vnd.ms-excel', {'Expires': '0'})}\n```\n\n### 响应状态码\n\n`r.status_code` 检查响应状态码\n`r.status_code == requests.codes.ok` 内置的状态码查询对象\n`r.raise_for_status()` 抛出异常\n\n### 响应头\n\n`r.headers`\n```json\n{\n    'content-encoding': 'gzip',\n    'transfer-encoding': 'chunked',\n    'connection': 'close',\n    'server': 'nginx/1.0.4',\n    'x-runtime': '148ms',\n    'etag': '\"e1ca502697e5c9317743dc078f67693f\"',\n    'content-type': 'application/json'\n}\n```\n\n### Cookie\n\n`r.cookies`\n```python\ncookies = dict(cookies_are='working')\nr = requests.get(url, cookies=cookies)\n```\n\nCookie 的返回对象为 RequestsCookieJar，它的行为和字典类似，但接口更为完整，适合跨域名跨路径使用。你还可以把 Cookie Jar 传到 Requests 中：\n\n```python\njar = requests.cookies.RequestsCookieJar()\njar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')\nr = requests.get(url, cookies=jar)\n```\n\n### 重定向和请求历史\n\nResponse.history 是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。\n\n```python\nr = requests.get(url, allow_redirects=False)\nr.history\n```\n\n### 超时\n\n`r = requests.get(url, timeouts=0.01)`\n\n### 错误与异常\n\n遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常\n\n如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常\n\n若请求超时，则抛出一个 Timeout 异常。\n\n若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。\n\n所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。\n","slug":"requests","published":1,"updated":"2018-08-31T03:51:44.175Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh3m0011zkvomzak4ys6","content":"<h2 id=\"Requests\"><a href=\"#Requests\" class=\"headerlink\" title=\"Requests\"></a><a href=\"cn.python-requests.org\">Requests</a></h2><p>Requests is an elegant and simple HTTP library for python, built for human beings.</p>\n<h2 id=\"Beloved-Features\"><a href=\"#Beloved-Features\" class=\"headerlink\" title=\"Beloved Features\"></a>Beloved Features</h2><ul>\n<li>Keep-Alive &amp; Connection Pooling</li>\n<li>International Domains and URLs</li>\n<li>Session with Cookie Persistence</li>\n<li>Browser-style SSL Verification</li>\n<li>Automatic Content Decoding</li>\n<li>Basic/Digest Authentication</li>\n<li>Elegant Key/Value Cookie</li>\n<li>Automatic Decompression</li>\n<li>Unicode Response Bodies</li>\n<li>HTTP(S) Proxy Support</li>\n<li>Multipart File Uploads</li>\n<li>Streaming Downloads</li>\n<li>Connection Timeouts</li>\n<li>Chunked Requests</li>\n<li><code>.netrc</code> Support</li>\n</ul>\n<h2 id=\"快速上手\"><a href=\"#快速上手\" class=\"headerlink\" title=\"快速上手\"></a>快速上手</h2><h3 id=\"发送请求\"><a href=\"#发送请求\" class=\"headerlink\" title=\"发送请求\"></a>发送请求</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\">r = requests.get(url)</span><br></pre></td></tr></table></figure>\n<p><code>r</code>为Response对象，requests的方法还有<code>put, delete, head, options</code></p>\n<h3 id=\"传递URL参数\"><a href=\"#传递URL参数\" class=\"headerlink\" title=\"传递URL参数\"></a>传递URL参数</h3><p>如url为<code>http://httpbin.org/get?key2=value2&amp;key1=value1</code><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload = &#123;<span class=\"string\">'key1'</span>:<span class=\"string\">'value1'</span>, <span class=\"string\">'key2'</span>:<span class=\"string\">'value2'</span>&#125;</span><br><span class=\"line\">r = requests.get(<span class=\"string\">\"http://httpbin.org/get\"</span>, params=payload)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"响应内容\"><a href=\"#响应内容\" class=\"headerlink\" title=\"响应内容\"></a>响应内容</h3><p><code>r.text</code> 响应文本<br><code>r.encoding</code> 编码格式</p>\n<h3 id=\"二进制响应内容\"><a href=\"#二进制响应内容\" class=\"headerlink\" title=\"二进制响应内容\"></a>二进制响应内容</h3><p><code>r.content</code> 以字节的方式访问响应体</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> io <span class=\"keyword\">import</span> BytesIO</span><br><span class=\"line\">i = Image.open(BytesIO(r.content))</span><br></pre></td></tr></table></figure>\n<h3 id=\"JSON响应内容\"><a href=\"#JSON响应内容\" class=\"headerlink\" title=\"JSON响应内容\"></a>JSON响应内容</h3><p><code>r.josn()</code></p>\n<h3 id=\"原始响应内容\"><a href=\"#原始响应内容\" class=\"headerlink\" title=\"原始响应内容\"></a>原始响应内容</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = requests.get(url, stream=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">r.raw</span><br><span class=\"line\">r.raw.read(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> open(filename, <span class=\"string\">'wb'</span>) <span class=\"keyword\">as</span> fd:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> chunk <span class=\"keyword\">in</span> r.iter_content(chunk_size):</span><br><span class=\"line\">        fd.write(chunk)</span><br></pre></td></tr></table></figure>\n<h3 id=\"定制请求头\"><a href=\"#定制请求头\" class=\"headerlink\" title=\"定制请求头\"></a>定制请求头</h3><p><code>headers = {&#39;user-agent&#39;: &#39;my_-app/0.0.1&#39;}</code><br><code>r = requests.get(url, headers=headers)</code></p>\n<h3 id=\"更复杂的POST请求\"><a href=\"#更复杂的POST请求\" class=\"headerlink\" title=\"更复杂的POST请求\"></a>更复杂的POST请求</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload = &#123;<span class=\"string\">'key1'</span>:<span class=\"string\">'value1'</span>, <span class=\"string\">'key2'</span>:<span class=\"string\">'value2'</span>&#125;</span><br><span class=\"line\">r = requests.post(<span class=\"string\">\"http://httpbin.org/get\"</span>, data=payload)</span><br><span class=\"line\"></span><br><span class=\"line\">payload = ((<span class=\"string\">'key1'</span>, <span class=\"string\">'value1'</span>), (<span class=\"string\">'key2'</span>, <span class=\"string\">'value2'</span>))</span><br><span class=\"line\">r = requests.post(<span class=\"string\">\"http://httpbin.org/get\"</span>, data=payload)</span><br><span class=\"line\"></span><br><span class=\"line\">r = requests.post(url, json=payload)</span><br></pre></td></tr></table></figure>\n<h3 id=\"POST一个多部分编码的文件\"><a href=\"#POST一个多部分编码的文件\" class=\"headerlink\" title=\"POST一个多部分编码的文件\"></a>POST一个多部分编码的文件</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">files = &#123;<span class=\"string\">'file'</span>: open(<span class=\"string\">'report.xls'</span>, <span class=\"string\">'rb'</span>)&#125;</span><br><span class=\"line\">r = requests.post(url, files=files)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 你还可以显式地设置文件名，文件类型和请求头</span></span><br><span class=\"line\">files = &#123;<span class=\"string\">'file'</span>: (<span class=\"string\">'report.xls'</span>, open(<span class=\"string\">'report.xls'</span>, <span class=\"string\">'rb'</span>), <span class=\"string\">'application/vnd.ms-excel'</span>, &#123;<span class=\"string\">'Expires'</span>: <span class=\"string\">'0'</span>&#125;)&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"响应状态码\"><a href=\"#响应状态码\" class=\"headerlink\" title=\"响应状态码\"></a>响应状态码</h3><p><code>r.status_code</code> 检查响应状态码<br><code>r.status_code == requests.codes.ok</code> 内置的状态码查询对象<br><code>r.raise_for_status()</code> 抛出异常</p>\n<h3 id=\"响应头\"><a href=\"#响应头\" class=\"headerlink\" title=\"响应头\"></a>响应头</h3><p><code>r.headers</code><br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    'content-encoding': 'gzip',</span><br><span class=\"line\">    'transfer-encoding': 'chunked',</span><br><span class=\"line\">    'connection': 'close',</span><br><span class=\"line\">    'server': 'nginx/1.0.4',</span><br><span class=\"line\">    'x-runtime': '148ms',</span><br><span class=\"line\">    'etag': '\"e1ca502697e5c9317743dc078f67693f\"',</span><br><span class=\"line\">    'content-type': 'application/json'</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Cookie\"><a href=\"#Cookie\" class=\"headerlink\" title=\"Cookie\"></a>Cookie</h3><p><code>r.cookies</code><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cookies = dict(cookies_are=<span class=\"string\">'working'</span>)</span><br><span class=\"line\">r = requests.get(url, cookies=cookies)</span><br></pre></td></tr></table></figure></p>\n<p>Cookie 的返回对象为 RequestsCookieJar，它的行为和字典类似，但接口更为完整，适合跨域名跨路径使用。你还可以把 Cookie Jar 传到 Requests 中：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jar = requests.cookies.RequestsCookieJar()</span><br><span class=\"line\">jar.set(<span class=\"string\">'tasty_cookie'</span>, <span class=\"string\">'yum'</span>, domain=<span class=\"string\">'httpbin.org'</span>, path=<span class=\"string\">'/cookies'</span>)</span><br><span class=\"line\">r = requests.get(url, cookies=jar)</span><br></pre></td></tr></table></figure>\n<h3 id=\"重定向和请求历史\"><a href=\"#重定向和请求历史\" class=\"headerlink\" title=\"重定向和请求历史\"></a>重定向和请求历史</h3><p>Response.history 是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = requests.get(url, allow_redirects=<span class=\"keyword\">False</span>)</span><br><span class=\"line\">r.history</span><br></pre></td></tr></table></figure>\n<h3 id=\"超时\"><a href=\"#超时\" class=\"headerlink\" title=\"超时\"></a>超时</h3><p><code>r = requests.get(url, timeouts=0.01)</code></p>\n<h3 id=\"错误与异常\"><a href=\"#错误与异常\" class=\"headerlink\" title=\"错误与异常\"></a>错误与异常</h3><p>遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常</p>\n<p>如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常</p>\n<p>若请求超时，则抛出一个 Timeout 异常。</p>\n<p>若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。</p>\n<p>所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Requests\"><a href=\"#Requests\" class=\"headerlink\" title=\"Requests\"></a><a href=\"cn.python-requests.org\">Requests</a></h2><p>Requests is an elegant and simple HTTP library for python, built for human beings.</p>\n<h2 id=\"Beloved-Features\"><a href=\"#Beloved-Features\" class=\"headerlink\" title=\"Beloved Features\"></a>Beloved Features</h2><ul>\n<li>Keep-Alive &amp; Connection Pooling</li>\n<li>International Domains and URLs</li>\n<li>Session with Cookie Persistence</li>\n<li>Browser-style SSL Verification</li>\n<li>Automatic Content Decoding</li>\n<li>Basic/Digest Authentication</li>\n<li>Elegant Key/Value Cookie</li>\n<li>Automatic Decompression</li>\n<li>Unicode Response Bodies</li>\n<li>HTTP(S) Proxy Support</li>\n<li>Multipart File Uploads</li>\n<li>Streaming Downloads</li>\n<li>Connection Timeouts</li>\n<li>Chunked Requests</li>\n<li><code>.netrc</code> Support</li>\n</ul>\n<h2 id=\"快速上手\"><a href=\"#快速上手\" class=\"headerlink\" title=\"快速上手\"></a>快速上手</h2><h3 id=\"发送请求\"><a href=\"#发送请求\" class=\"headerlink\" title=\"发送请求\"></a>发送请求</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\">r = requests.get(url)</span><br></pre></td></tr></table></figure>\n<p><code>r</code>为Response对象，requests的方法还有<code>put, delete, head, options</code></p>\n<h3 id=\"传递URL参数\"><a href=\"#传递URL参数\" class=\"headerlink\" title=\"传递URL参数\"></a>传递URL参数</h3><p>如url为<code>http://httpbin.org/get?key2=value2&amp;key1=value1</code><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload = &#123;<span class=\"string\">'key1'</span>:<span class=\"string\">'value1'</span>, <span class=\"string\">'key2'</span>:<span class=\"string\">'value2'</span>&#125;</span><br><span class=\"line\">r = requests.get(<span class=\"string\">\"http://httpbin.org/get\"</span>, params=payload)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"响应内容\"><a href=\"#响应内容\" class=\"headerlink\" title=\"响应内容\"></a>响应内容</h3><p><code>r.text</code> 响应文本<br><code>r.encoding</code> 编码格式</p>\n<h3 id=\"二进制响应内容\"><a href=\"#二进制响应内容\" class=\"headerlink\" title=\"二进制响应内容\"></a>二进制响应内容</h3><p><code>r.content</code> 以字节的方式访问响应体</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">from</span> io <span class=\"keyword\">import</span> BytesIO</span><br><span class=\"line\">i = Image.open(BytesIO(r.content))</span><br></pre></td></tr></table></figure>\n<h3 id=\"JSON响应内容\"><a href=\"#JSON响应内容\" class=\"headerlink\" title=\"JSON响应内容\"></a>JSON响应内容</h3><p><code>r.josn()</code></p>\n<h3 id=\"原始响应内容\"><a href=\"#原始响应内容\" class=\"headerlink\" title=\"原始响应内容\"></a>原始响应内容</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = requests.get(url, stream=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">r.raw</span><br><span class=\"line\">r.raw.read(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> open(filename, <span class=\"string\">'wb'</span>) <span class=\"keyword\">as</span> fd:</span><br><span class=\"line\">    <span class=\"keyword\">for</span> chunk <span class=\"keyword\">in</span> r.iter_content(chunk_size):</span><br><span class=\"line\">        fd.write(chunk)</span><br></pre></td></tr></table></figure>\n<h3 id=\"定制请求头\"><a href=\"#定制请求头\" class=\"headerlink\" title=\"定制请求头\"></a>定制请求头</h3><p><code>headers = {&#39;user-agent&#39;: &#39;my_-app/0.0.1&#39;}</code><br><code>r = requests.get(url, headers=headers)</code></p>\n<h3 id=\"更复杂的POST请求\"><a href=\"#更复杂的POST请求\" class=\"headerlink\" title=\"更复杂的POST请求\"></a>更复杂的POST请求</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">payload = &#123;<span class=\"string\">'key1'</span>:<span class=\"string\">'value1'</span>, <span class=\"string\">'key2'</span>:<span class=\"string\">'value2'</span>&#125;</span><br><span class=\"line\">r = requests.post(<span class=\"string\">\"http://httpbin.org/get\"</span>, data=payload)</span><br><span class=\"line\"></span><br><span class=\"line\">payload = ((<span class=\"string\">'key1'</span>, <span class=\"string\">'value1'</span>), (<span class=\"string\">'key2'</span>, <span class=\"string\">'value2'</span>))</span><br><span class=\"line\">r = requests.post(<span class=\"string\">\"http://httpbin.org/get\"</span>, data=payload)</span><br><span class=\"line\"></span><br><span class=\"line\">r = requests.post(url, json=payload)</span><br></pre></td></tr></table></figure>\n<h3 id=\"POST一个多部分编码的文件\"><a href=\"#POST一个多部分编码的文件\" class=\"headerlink\" title=\"POST一个多部分编码的文件\"></a>POST一个多部分编码的文件</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">files = &#123;<span class=\"string\">'file'</span>: open(<span class=\"string\">'report.xls'</span>, <span class=\"string\">'rb'</span>)&#125;</span><br><span class=\"line\">r = requests.post(url, files=files)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 你还可以显式地设置文件名，文件类型和请求头</span></span><br><span class=\"line\">files = &#123;<span class=\"string\">'file'</span>: (<span class=\"string\">'report.xls'</span>, open(<span class=\"string\">'report.xls'</span>, <span class=\"string\">'rb'</span>), <span class=\"string\">'application/vnd.ms-excel'</span>, &#123;<span class=\"string\">'Expires'</span>: <span class=\"string\">'0'</span>&#125;)&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"响应状态码\"><a href=\"#响应状态码\" class=\"headerlink\" title=\"响应状态码\"></a>响应状态码</h3><p><code>r.status_code</code> 检查响应状态码<br><code>r.status_code == requests.codes.ok</code> 内置的状态码查询对象<br><code>r.raise_for_status()</code> 抛出异常</p>\n<h3 id=\"响应头\"><a href=\"#响应头\" class=\"headerlink\" title=\"响应头\"></a>响应头</h3><p><code>r.headers</code><br><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">    'content-encoding': 'gzip',</span><br><span class=\"line\">    'transfer-encoding': 'chunked',</span><br><span class=\"line\">    'connection': 'close',</span><br><span class=\"line\">    'server': 'nginx/1.0.4',</span><br><span class=\"line\">    'x-runtime': '148ms',</span><br><span class=\"line\">    'etag': '\"e1ca502697e5c9317743dc078f67693f\"',</span><br><span class=\"line\">    'content-type': 'application/json'</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Cookie\"><a href=\"#Cookie\" class=\"headerlink\" title=\"Cookie\"></a>Cookie</h3><p><code>r.cookies</code><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cookies = dict(cookies_are=<span class=\"string\">'working'</span>)</span><br><span class=\"line\">r = requests.get(url, cookies=cookies)</span><br></pre></td></tr></table></figure></p>\n<p>Cookie 的返回对象为 RequestsCookieJar，它的行为和字典类似，但接口更为完整，适合跨域名跨路径使用。你还可以把 Cookie Jar 传到 Requests 中：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jar = requests.cookies.RequestsCookieJar()</span><br><span class=\"line\">jar.set(<span class=\"string\">'tasty_cookie'</span>, <span class=\"string\">'yum'</span>, domain=<span class=\"string\">'httpbin.org'</span>, path=<span class=\"string\">'/cookies'</span>)</span><br><span class=\"line\">r = requests.get(url, cookies=jar)</span><br></pre></td></tr></table></figure>\n<h3 id=\"重定向和请求历史\"><a href=\"#重定向和请求历史\" class=\"headerlink\" title=\"重定向和请求历史\"></a>重定向和请求历史</h3><p>Response.history 是一个 Response 对象的列表，为了完成请求而创建了这些对象。这个对象列表按照从最老到最近的请求进行排序。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">r = requests.get(url, allow_redirects=<span class=\"keyword\">False</span>)</span><br><span class=\"line\">r.history</span><br></pre></td></tr></table></figure>\n<h3 id=\"超时\"><a href=\"#超时\" class=\"headerlink\" title=\"超时\"></a>超时</h3><p><code>r = requests.get(url, timeouts=0.01)</code></p>\n<h3 id=\"错误与异常\"><a href=\"#错误与异常\" class=\"headerlink\" title=\"错误与异常\"></a>错误与异常</h3><p>遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常</p>\n<p>如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常</p>\n<p>若请求超时，则抛出一个 Timeout 异常。</p>\n<p>若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。</p>\n<p>所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。</p>\n"},{"title":"人生--路遥","date":"2018-07-19T15:20:58.000Z","_content":"\n### 故事梗概\n\n小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。\n\n### 内容简介\n\n#### 回到土地\n\n主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个\n“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。\n\n#### 离开土地\n\n城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。\n\n#### 回到土地\n\n但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦......）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。\n\n### 创作背景\n\n20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。\n\n20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。\n\n《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。\n\n当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。\n\n早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。\n\n### 人物介绍\n\n#### 高加林\n\n高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。\n\n#### 刘巧珍\n\n巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。\n\n---\n摘自《百度百科词条：人生》\n","source":"_posts/人生-路遥.md","raw":"---\ntitle: 人生--路遥\ndate: 2018-07-19 23:20:58\ntags: 路遥\ncategories: 文学\n---\n\n### 故事梗概\n\n小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。\n\n### 内容简介\n\n#### 回到土地\n\n主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个\n“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。\n\n#### 离开土地\n\n城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。\n\n#### 回到土地\n\n但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦......）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。\n\n### 创作背景\n\n20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。\n\n20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。\n\n《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。\n\n当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。\n\n早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。\n\n### 人物介绍\n\n#### 高加林\n\n高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。\n\n#### 刘巧珍\n\n巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。\n\n---\n摘自《百度百科词条：人生》\n","slug":"人生-路遥","published":1,"updated":"2018-08-31T03:48:13.715Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh3r0015zkvo68hfkm7c","content":"<h3 id=\"故事梗概\"><a href=\"#故事梗概\" class=\"headerlink\" title=\"故事梗概\"></a>故事梗概</h3><p>小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。</p>\n<h3 id=\"内容简介\"><a href=\"#内容简介\" class=\"headerlink\" title=\"内容简介\"></a>内容简介</h3><h4 id=\"回到土地\"><a href=\"#回到土地\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个<br>“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。</p>\n<h4 id=\"离开土地\"><a href=\"#离开土地\" class=\"headerlink\" title=\"离开土地\"></a>离开土地</h4><p>城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。</p>\n<h4 id=\"回到土地-1\"><a href=\"#回到土地-1\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦……）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。</p>\n<h3 id=\"创作背景\"><a href=\"#创作背景\" class=\"headerlink\" title=\"创作背景\"></a>创作背景</h3><p>20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。</p>\n<p>20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。</p>\n<p>《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。</p>\n<p>当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。</p>\n<p>早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。</p>\n<h3 id=\"人物介绍\"><a href=\"#人物介绍\" class=\"headerlink\" title=\"人物介绍\"></a>人物介绍</h3><h4 id=\"高加林\"><a href=\"#高加林\" class=\"headerlink\" title=\"高加林\"></a>高加林</h4><p>高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。</p>\n<h4 id=\"刘巧珍\"><a href=\"#刘巧珍\" class=\"headerlink\" title=\"刘巧珍\"></a>刘巧珍</h4><p>巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。</p>\n<hr>\n<p>摘自《百度百科词条：人生》</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"故事梗概\"><a href=\"#故事梗概\" class=\"headerlink\" title=\"故事梗概\"></a>故事梗概</h3><p>小说以改革时期陕北高原的城乡生活为时空背景，描写了高中毕业生高加林回到土地又离开土地，再离开土地，再回到土地这样人生的变化过程构成了其故事构架。高加林同农村姑娘刘巧珍，城市姑娘黄亚萍之间的感情纠葛构成了故事发展的矛盾，也正是体现那种艰难选择的悲剧。</p>\n<h3 id=\"内容简介\"><a href=\"#内容简介\" class=\"headerlink\" title=\"内容简介\"></a>内容简介</h3><h4 id=\"回到土地\"><a href=\"#回到土地\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>主人公是高加林，他高中毕业回到村里后当上了民办小学的教师，很满足这个既能体现他的才能而又对他充满希望的职业，但是好景不长，他就被有权有势的大队书记高明楼的儿子顶替了，他重新回到了土地。正当他失意无奈，甚至有些绝望的时候，善良美丽的农村姑娘刘巧珍闯进了他的生活，刘巧珍虽然没有文化，但是却真心真意地爱上了高加林这个<br>“文化人”，她的爱质朴纯真，她以她的那种充满激情而又实际的作法表白了她的炽烈的爱。而实际上她所得到的爱从一开始就是不平等，高加林在她的眼中是完美的，而她对于高加林来说只是在他失意时找到了精神上的慰藉。当机遇再次降临到了高加林身上，他终于抓住了这次机会，重新回到了城市。</p>\n<h4 id=\"离开土地\"><a href=\"#离开土地\" class=\"headerlink\" title=\"离开土地\"></a>离开土地</h4><p>城市生活给了高加林大显身手的机会，又让他重新遇到了他的同学黄亚萍。与巧珍相比，黄亚萍无疑是位现代女性，她开朗活泼，却又任性专横，她对高加林的爱炽烈大胆又有一种征服欲。高加林的确与她有许多相似的地方，他们有相同的知识背景，又有许多感兴趣的话题，当他们俩口若悬河、侃侃而谈时，高加林已经进入了一种艰难的选择之中。当高加林隐隐地有了这种想法时，他的念头很快便被另一种感情压下去了，他想起了巧珍那亲切可爱的脸庞，想起了巧珍那种无私而温柔的爱。当巧珍带着狗皮褥子来看他时，巧珍去县城看了好几次加林，加林都有事下乡采访了，终于有一次他俩有机会见面了，加林看到日思夜想的巧珍，心情很是激动，巧珍看他的被褥那么单薄，就说下次去给他带去她自己铺的狗皮褥子，高加林一下子不高兴了，因为城里人没有人用狗皮褥子，而且那狗皮褥子跟他生活的环境一点都不相称，他怕被别人笑话，而当巧珍给他讲的都是些家长里短的小事的时候，他一下子觉得很失落，他跟黄亚萍谈论的都是时事政治、国家大事！那才是他想要的，他的远大抱负。这种反差让高加林很是纠结。他的那种难以言说的复杂的感情一下子表现了出来。在经过反复考虑后，他接受了黄亚萍的爱，可同时意味着这种选择会无情地伤害巧珍，当他委婉地对巧珍表达了他的这种选择后，巧珍含泪接受了，但她却并没有过多地责怪高加林，反而更担心高加林以后的生活，劝他到外地多操心。但是泪水却在她脸上刷刷地淌着。</p>\n<h4 id=\"回到土地-1\"><a href=\"#回到土地-1\" class=\"headerlink\" title=\"回到土地\"></a>回到土地</h4><p>但是好梦难圆，高加林通过关系得到城内工作这件事终于被人告发了，他要面对的是重新回到生他养他的那片土地，他所有的理想和抱负如同过眼云烟难以挽留了。难以承受的是这份打击更难以面对的是生他养他的那片土地，（他本以为村里人都等着看他的笑话呢！可他万万没想到，当他灰头土脸地出现在家乡人面前的时候，家乡人给他的是各种安慰的话语，他感动的不知说什么了，只是拿出他随身带着的烟散给乡亲们。而此时他也得知巧珍已嫁作他人妇，即便如此，她依然去求她姐姐的公公、村支书——高明楼，求他给高加林安排去教学，因为据说家乡的那所学校因为学生增多要新添一个老师。德顺爷爷感慨地说道：“多好的娃娃啊！”此时的高加林已经泣不成声，趴在热情的乡土上大声痛苦……）他褪去了骄傲，认清了现实，接受了德顺爷爷的一番话，而后懊悔的扑倒在了地上。</p>\n<h3 id=\"创作背景\"><a href=\"#创作背景\" class=\"headerlink\" title=\"创作背景\"></a>创作背景</h3><p>20世纪80年代的中国，商品经济的活跃打破了农村的僵持与保守，具有现代文明的城市开始对一直困守在土地的农民产生强烈的诱惑。特别是在青年心中引起巨大的骚动，他们开始对自己的生活及周围的世界产生怀疑与不满。</p>\n<p>20世纪80年代，中国户籍制度清晰地将公民分为农业户口和非农业户口，在这种固态格式化的身份制度下，中国社会形成了独特的社会地理景观：乡村景观和城市景观；与这两种景观相对应的是两种截然不同的经济制度和生存方式、文化特征、价值观念。由此导致了中国社会最重要的社会差异；城乡差别。同时，国家还通过各种举措在主观上强化这种差异。臂如在劳动分配制度上，城市工作的工人、教师、职员每月有固定的工资收入，有相对完善的医疗制度、退休制度，同时还可以享受国家各种福利待遇。而在乡村，农民不仅要按时按量向国家交纳粮食，在很长的时期内只能有限度地支配自己的劳动产品。并且，农民还要完成国家规定的各种税费。参与无偿的劳作（例如大规模强制性的农田水利建设）。而国家采取的各种政策将农民强制性地限制在土地上。这些政策的实施直接导致了农民在整个社会发展中长时间处于相对贫困的状态中。因此，可以说在这种基本的身份差异之下，城市和乡村作为两个基本对立的概念被凸显了出来。这是一个作为卑贱农民和一个高贵知识分子的对立，普通百姓和达官显贵的对立。</p>\n<p>《人生》就是在城市的场景中展开，似乎一切都处于城市的控制下，甚至乡下人天生就应该在城里人面前低人一等。这种强烈的等级观念、城乡差异在小说中被强化。</p>\n<p>当路遥年轻时不停地奔波在城市与乡村时，他最为熟悉的生活即是“城市交叉地带”，充满生气和机遇的城市生活对于像他那样的身处封闭而又贫困的农村知识青年构成了一种双重的刺激，不论在物质还是在精神上。路遥思考并理解了这一现象，在城市化的浪潮汹涌而来的种种冲击中，他提出了农村知识青年该如何做出选择。</p>\n<p>早在大学读书时，路遥阅读了大量的经典名著，并对新中国的文学成就进行了一翻巡视。他发现以前的小说带有某种脸谱化的倾向，正如儿童眼中将电影中的人物形象简单分为“好人”和“坏蛋“，而人的思想是复杂的、多变的，绝对不能将复杂的人性这样简单的划分，这种思考体现在《人生》的主人公高加林身上。</p>\n<h3 id=\"人物介绍\"><a href=\"#人物介绍\" class=\"headerlink\" title=\"人物介绍\"></a>人物介绍</h3><h4 id=\"高加林\"><a href=\"#高加林\" class=\"headerlink\" title=\"高加林\"></a>高加林</h4><p>高加林是作者着力塑造的复杂的人物。他身上既体现了现代青年那种不断向命运挑战，自信坚毅的品质，又同时具有辛勤、朴实的传统美德。他热爱生活，心性极高，有着远大的理想和抱负。关心国际问题，爱好打篮球，并融入时代的潮流。他不像他的父亲那样忍气吞声、安守本分，而是有更高的精神追求，但是他的现实与他心中的理想总是相差极远，正是这样反差构成了他的复杂的性格特征。</p>\n<h4 id=\"刘巧珍\"><a href=\"#刘巧珍\" class=\"headerlink\" title=\"刘巧珍\"></a>刘巧珍</h4><p>巧珍美丽善良，爱情真诚。但她把自己置于高加林的附属地位，理想之光幻灭后，她以无爱的婚姻表示对命运的抗争，恰恰重陷传统道德观念的桎梏。</p>\n<hr>\n<p>摘自《百度百科词条：人生》</p>\n"},{"title":"初始化参数","date":"2018-07-22T05:22:45.000Z","_content":"\n## Initialization\n\nTraining your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  \n\nA well chosen initialization can:\n- Speed up the convergence of gradient descent\n- Increase the odds of gradient descent converging to a lower training (and generalization) error\n\n## Random initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n## He initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n\n**What you should remember from this artical**:\n- Different initializations lead to different results\n- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n- Don't intialize to values that are too large\n- He initialization works well for networks with ReLU activations.\n","source":"_posts/初始化参数.md","raw":"---\ntitle: 初始化参数\ndate: 2018-07-22 13:22:45\ntags: 优化算法\ncategories: 深度学习\n---\n\n## Initialization\n\nTraining your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  \n\nA well chosen initialization can:\n- Speed up the convergence of gradient descent\n- Increase the odds of gradient descent converging to a lower training (and generalization) error\n\n## Random initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n## He initialization\n\n```python\nparameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])\nparameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n```\n\n\n**What you should remember from this artical**:\n- Different initializations lead to different results\n- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n- Don't intialize to values that are too large\n- He initialization works well for networks with ReLU activations.\n","slug":"初始化参数","published":1,"updated":"2018-08-31T03:52:16.653Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh3w0019zkvozm965mug","content":"<h2 id=\"Initialization\"><a href=\"#Initialization\" class=\"headerlink\" title=\"Initialization\"></a>Initialization</h2><p>Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  </p>\n<p>A well chosen initialization can:</p>\n<ul>\n<li>Speed up the convergence of gradient descent</li>\n<li>Increase the odds of gradient descent converging to a lower training (and generalization) error</li>\n</ul>\n<h2 id=\"Random-initialization\"><a href=\"#Random-initialization\" class=\"headerlink\" title=\"Random initialization\"></a>Random initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * <span class=\"number\">0.01</span></span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<h2 id=\"He-initialization\"><a href=\"#He-initialization\" class=\"headerlink\" title=\"He initialization\"></a>He initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * np.sqrt(<span class=\"number\">2</span> / layers_dims[l<span class=\"number\">-1</span>])</span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><strong>What you should remember from this artical</strong>:</p>\n<ul>\n<li>Different initializations lead to different results</li>\n<li>Random initialization is used to break symmetry and make sure different hidden units can learn different things</li>\n<li>Don’t intialize to values that are too large</li>\n<li>He initialization works well for networks with ReLU activations.</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Initialization\"><a href=\"#Initialization\" class=\"headerlink\" title=\"Initialization\"></a>Initialization</h2><p>Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  </p>\n<p>A well chosen initialization can:</p>\n<ul>\n<li>Speed up the convergence of gradient descent</li>\n<li>Increase the odds of gradient descent converging to a lower training (and generalization) error</li>\n</ul>\n<h2 id=\"Random-initialization\"><a href=\"#Random-initialization\" class=\"headerlink\" title=\"Random initialization\"></a>Random initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * <span class=\"number\">0.01</span></span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<h2 id=\"He-initialization\"><a href=\"#He-initialization\" class=\"headerlink\" title=\"He initialization\"></a>He initialization</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class=\"number\">-1</span>]) * np.sqrt(<span class=\"number\">2</span> / layers_dims[l<span class=\"number\">-1</span>])</span><br><span class=\"line\">parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n<p><strong>What you should remember from this artical</strong>:</p>\n<ul>\n<li>Different initializations lead to different results</li>\n<li>Random initialization is used to break symmetry and make sure different hidden units can learn different things</li>\n<li>Don’t intialize to values that are too large</li>\n<li>He initialization works well for networks with ReLU activations.</li>\n</ul>\n"},{"title":"动态规划","date":"2018-07-21T14:47:53.000Z","mathjax":true,"_content":"## 动态规划（dynamic programming）\n\n与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。\n\n通常用来求解最优化问题。\n\n我们通常按如下4个步骤来设计一个动态规划算法：\n\n* 刻画一个最优解的结构特征\n* 递归地定义最优解的值\n* 计算最优解的值，通常采用自底向上的方法\n* 利用计算出的信息构造一个最优解\n\n## 钢条切割问题\n\n### 问题定义\n\n给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, ..., n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。\n\n### 问题分析\n\n长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为\n\n$$n = i_1 + i_2 + ... + i_k$$\n\n得到的最大收益为\n\n$$r_n = p_{i_1} + p_{i_2} + ... + p_{i_k}$$\n\n当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。\n\n则最优切割收益为\n\n$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, ..., r_{n-1} + r_1)$$\n\n除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。\n\n这样我们得到上述式子的简化版本\n\n$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$\n\n### 代码实现\n\n#### 自顶向下递归实现\n\n```python\ndef cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    if n == 0:\n        return 0\n    q = -1\n    for i in range(1, n+1):\n        q = max(q, p[i] + cut_rod(p, n-i))\n    return q\n```\n\n#### 代码分析\n\n![](/images/动态规划.jpg)\n\n令$T(n)$表示cut_rod的调用次数\n\n$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$\n\n第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$\n\n#### 使用动态规划求解\n\n朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。\n\n动态规划有两种等价的实现。**带备忘的自顶向下**、**自底向上**。这里只给出第二种的代码。\n\n```python\ndef bottom_up_cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    r = range(n + 1)  # to save subproblem's result\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            q = max(q, p[i] + r[j - i])\n            r[j] = q\n    return r[n]\n```\n\n#### 代码分析\n\n自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, ..., n$的子问题。时间复杂度为$\\Theta(n^2)$\n\n#### 扩展代码\n\n前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。\n\n```python\ndef externed_bottom_up_cut_rod(p, n):\n    r = range(n + 1)  # 长度为j的钢条的最大收益值r_j\n    s = range(n + 1)  # 最优解对应的第一条钢条的长度s_j\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            if q < p[i] + r[j - i]:\n                q = p[i] + r[j - i]\n                s[j] = i\n            r[j] = q\n    return r[n]\n\ndef print_cut_rod_solution(p, n):\n    (r, s) = externed_bottom_up_cut_rod(p, n)\n    while n > 0:\n        print(s[n])\n        n = n - s[n]\n```","source":"_posts/动态规划.md","raw":"---\ntitle: 动态规划\ndate: 2018-07-21 22:47:53\ntags: 动态规划\ncategories: 算法导论\nmathjax: true\n---\n## 动态规划（dynamic programming）\n\n与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。\n\n通常用来求解最优化问题。\n\n我们通常按如下4个步骤来设计一个动态规划算法：\n\n* 刻画一个最优解的结构特征\n* 递归地定义最优解的值\n* 计算最优解的值，通常采用自底向上的方法\n* 利用计算出的信息构造一个最优解\n\n## 钢条切割问题\n\n### 问题定义\n\n给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, ..., n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。\n\n### 问题分析\n\n长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为\n\n$$n = i_1 + i_2 + ... + i_k$$\n\n得到的最大收益为\n\n$$r_n = p_{i_1} + p_{i_2} + ... + p_{i_k}$$\n\n当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。\n\n则最优切割收益为\n\n$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, ..., r_{n-1} + r_1)$$\n\n除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。\n\n这样我们得到上述式子的简化版本\n\n$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$\n\n### 代码实现\n\n#### 自顶向下递归实现\n\n```python\ndef cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    if n == 0:\n        return 0\n    q = -1\n    for i in range(1, n+1):\n        q = max(q, p[i] + cut_rod(p, n-i))\n    return q\n```\n\n#### 代码分析\n\n![](/images/动态规划.jpg)\n\n令$T(n)$表示cut_rod的调用次数\n\n$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$\n\n第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$\n\n#### 使用动态规划求解\n\n朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。\n\n动态规划有两种等价的实现。**带备忘的自顶向下**、**自底向上**。这里只给出第二种的代码。\n\n```python\ndef bottom_up_cut_rod(p, n):\n    \"\"\"\n    Arguments:\n    p -- the table of prices.\n    n -- the total length of steel rod.\n    \"\"\"\n    r = range(n + 1)  # to save subproblem's result\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            q = max(q, p[i] + r[j - i])\n            r[j] = q\n    return r[n]\n```\n\n#### 代码分析\n\n自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, ..., n$的子问题。时间复杂度为$\\Theta(n^2)$\n\n#### 扩展代码\n\n前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。\n\n```python\ndef externed_bottom_up_cut_rod(p, n):\n    r = range(n + 1)  # 长度为j的钢条的最大收益值r_j\n    s = range(n + 1)  # 最优解对应的第一条钢条的长度s_j\n    r[0] = 0\n    for j in range(1, n + 1):\n        q = -1\n        for i in range(1, j + 1):\n            if q < p[i] + r[j - i]:\n                q = p[i] + r[j - i]\n                s[j] = i\n            r[j] = q\n    return r[n]\n\ndef print_cut_rod_solution(p, n):\n    (r, s) = externed_bottom_up_cut_rod(p, n)\n    while n > 0:\n        print(s[n])\n        n = n - s[n]\n```","slug":"动态规划","published":1,"updated":"2018-08-19T01:59:35.797Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh40001dzkvozvwz8kqf","content":"<h2 id=\"动态规划（dynamic-programming）\"><a href=\"#动态规划（dynamic-programming）\" class=\"headerlink\" title=\"动态规划（dynamic programming）\"></a>动态规划（dynamic programming）</h2><p>与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。</p>\n<p>通常用来求解最优化问题。</p>\n<p>我们通常按如下4个步骤来设计一个动态规划算法：</p>\n<ul>\n<li>刻画一个最优解的结构特征</li>\n<li>递归地定义最优解的值</li>\n<li>计算最优解的值，通常采用自底向上的方法</li>\n<li>利用计算出的信息构造一个最优解</li>\n</ul>\n<h2 id=\"钢条切割问题\"><a href=\"#钢条切割问题\" class=\"headerlink\" title=\"钢条切割问题\"></a>钢条切割问题</h2><h3 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h3><p>给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, …, n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。</p>\n<h3 id=\"问题分析\"><a href=\"#问题分析\" class=\"headerlink\" title=\"问题分析\"></a>问题分析</h3><p>长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为</p>\n<p>$$n = i_1 + i_2 + … + i_k$$</p>\n<p>得到的最大收益为</p>\n<p>$$r_n = p_{i_1} + p_{i_2} + … + p_{i_k}$$</p>\n<p>当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。</p>\n<p>则最优切割收益为</p>\n<p>$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, …, r_{n-1} + r_1)$$</p>\n<p>除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。</p>\n<p>这样我们得到上述式子的简化版本</p>\n<p>$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$</p>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"自顶向下递归实现\"><a href=\"#自顶向下递归实现\" class=\"headerlink\" title=\"自顶向下递归实现\"></a>自顶向下递归实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    q = <span class=\"number\">-1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        q = max(q, p[i] + cut_rod(p, n-i))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> q</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析\"><a href=\"#代码分析\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p><img src=\"/images/动态规划.jpg\" alt=\"\"></p>\n<p>令$T(n)$表示cut_rod的调用次数</p>\n<p>$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$</p>\n<p>第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$</p>\n<h4 id=\"使用动态规划求解\"><a href=\"#使用动态规划求解\" class=\"headerlink\" title=\"使用动态规划求解\"></a>使用动态规划求解</h4><p>朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。</p>\n<p>动态规划有两种等价的实现。<strong>带备忘的自顶向下</strong>、<strong>自底向上</strong>。这里只给出第二种的代码。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># to save subproblem's result</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            q = max(q, p[i] + r[j - i])</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析-1\"><a href=\"#代码分析-1\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p>自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, …, n$的子问题。时间复杂度为$\\Theta(n^2)$</p>\n<h4 id=\"扩展代码\"><a href=\"#扩展代码\" class=\"headerlink\" title=\"扩展代码\"></a>扩展代码</h4><p>前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">externed_bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 长度为j的钢条的最大收益值r_j</span></span><br><span class=\"line\">    s = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 最优解对应的第一条钢条的长度s_j</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> q &lt; p[i] + r[j - i]:</span><br><span class=\"line\">                q = p[i] + r[j - i]</span><br><span class=\"line\">                s[j] = i</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_cut_rod_solution</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    (r, s) = externed_bottom_up_cut_rod(p, n)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(s[n])</span><br><span class=\"line\">        n = n - s[n]</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"动态规划（dynamic-programming）\"><a href=\"#动态规划（dynamic-programming）\" class=\"headerlink\" title=\"动态规划（dynamic programming）\"></a>动态规划（dynamic programming）</h2><p>与分治法相似，都是通过组合子问题的解来求解原问题。不同的是，动态规划应用于子问题重叠的情况，即不同的子问题具有公共的子子问题。在这种情况下，动态规划算法对每个子子问题只求解一次，将其保存在一个表格中，减少了计算量。</p>\n<p>通常用来求解最优化问题。</p>\n<p>我们通常按如下4个步骤来设计一个动态规划算法：</p>\n<ul>\n<li>刻画一个最优解的结构特征</li>\n<li>递归地定义最优解的值</li>\n<li>计算最优解的值，通常采用自底向上的方法</li>\n<li>利用计算出的信息构造一个最优解</li>\n</ul>\n<h2 id=\"钢条切割问题\"><a href=\"#钢条切割问题\" class=\"headerlink\" title=\"钢条切割问题\"></a>钢条切割问题</h2><h3 id=\"问题定义\"><a href=\"#问题定义\" class=\"headerlink\" title=\"问题定义\"></a>问题定义</h3><p>给定一段长度为$n$英寸的钢条（长度均为整英寸，切割后也必须是整英寸）和一个价格表$p_i(i=1, 2, …, n)$, 求解切割钢条的方案（方案也可以是不切割），使收益$r_n$最大。</p>\n<h3 id=\"问题分析\"><a href=\"#问题分析\" class=\"headerlink\" title=\"问题分析\"></a>问题分析</h3><p>长度为$n$英寸的钢条共有$2^{n-1}$种不同的切割方案，如果一个最优解将钢条切割为$k$段，那么最优切割方案为</p>\n<p>$$n = i_1 + i_2 + … + i_k$$</p>\n<p>得到的最大收益为</p>\n<p>$$r_n = p_{i_1} + p_{i_2} + … + p_{i_k}$$</p>\n<p>当完成首次切割后，我们将两段钢条看成两个独立的钢条切割问题实例。我们通过组合两个相关子问题的最优解，并在所有可能的两段切割方案种选取组合收益最大者，构成原问题的最优解。</p>\n<p>则最优切割收益为</p>\n<p>$$r_n = max(p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, …, r_{n-1} + r_1)$$</p>\n<p>除上述求解方法外，钢条切割问题还存在一种相似的但更为简单的递归求解方法：我们将钢条从左边切割下长度为$i$的一段，只对右边剩下的长度为$n-i$的一段继续进行切割（递归求解）。</p>\n<p>这样我们得到上述式子的简化版本</p>\n<p>$$r_n = \\mathop {\\max}_{1 \\le i \\le n}(p_i + r_{n-i})$$</p>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"自顶向下递归实现\"><a href=\"#自顶向下递归实现\" class=\"headerlink\" title=\"自顶向下递归实现\"></a>自顶向下递归实现</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> n == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">    q = <span class=\"number\">-1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n+<span class=\"number\">1</span>):</span><br><span class=\"line\">        q = max(q, p[i] + cut_rod(p, n-i))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> q</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析\"><a href=\"#代码分析\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p><img src=\"/images/动态规划.jpg\" alt=\"\"></p>\n<p>令$T(n)$表示cut_rod的调用次数</p>\n<p>$$T(n) = 1 + \\sum_{j=0}^{n-1} T(j) = 2^n$$</p>\n<p>第一项“1”表示函数的额第一次调用，$T(j)$为调用cut_rod(p, n-i)所产生的所有调用$(j = n-i)$</p>\n<h4 id=\"使用动态规划求解\"><a href=\"#使用动态规划求解\" class=\"headerlink\" title=\"使用动态规划求解\"></a>使用动态规划求解</h4><p>朴素递归算法之所以效率低，是因为它反复求解相同的子问题。因此，动态规划方法仔细安排求解顺序，对每个子问题只求解一次，并将结果保存下来。如果随后再次需要此子问题的解，只需查找保存的结果。</p>\n<p>动态规划有两种等价的实现。<strong>带备忘的自顶向下</strong>、<strong>自底向上</strong>。这里只给出第二种的代码。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    p -- the table of prices.</span></span><br><span class=\"line\"><span class=\"string\">    n -- the total length of steel rod.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># to save subproblem's result</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            q = max(q, p[i] + r[j - i])</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br></pre></td></tr></table></figure>\n<h4 id=\"代码分析-1\"><a href=\"#代码分析-1\" class=\"headerlink\" title=\"代码分析\"></a>代码分析</h4><p>自底向上版本采用子问题的自然顺序，一次求解规模为$j = 0, 1, 2, …, n$的子问题。时间复杂度为$\\Theta(n^2)$</p>\n<h4 id=\"扩展代码\"><a href=\"#扩展代码\" class=\"headerlink\" title=\"扩展代码\"></a>扩展代码</h4><p>前文给出的钢条切割问题的动态规划算法返回最优解的收益值，但未返回解本身。我们可以扩展动态规划算法，使之对每个子问题不仅保存最优收益值，还保存对应的切割方案。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">externed_bottom_up_cut_rod</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    r = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 长度为j的钢条的最大收益值r_j</span></span><br><span class=\"line\">    s = range(n + <span class=\"number\">1</span>)  <span class=\"comment\"># 最优解对应的第一条钢条的长度s_j</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, n + <span class=\"number\">1</span>):</span><br><span class=\"line\">        q = <span class=\"number\">-1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, j + <span class=\"number\">1</span>):</span><br><span class=\"line\">            <span class=\"keyword\">if</span> q &lt; p[i] + r[j - i]:</span><br><span class=\"line\">                q = p[i] + r[j - i]</span><br><span class=\"line\">                s[j] = i</span><br><span class=\"line\">            r[j] = q</span><br><span class=\"line\">    <span class=\"keyword\">return</span> r[n]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">print_cut_rod_solution</span><span class=\"params\">(p, n)</span>:</span></span><br><span class=\"line\">    (r, s) = externed_bottom_up_cut_rod(p, n)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> n &gt; <span class=\"number\">0</span>:</span><br><span class=\"line\">        print(s[n])</span><br><span class=\"line\">        n = n - s[n]</span><br></pre></td></tr></table></figure>"},{"title":"十个策略故事","date":"2018-07-19T16:59:24.000Z","catrgories":"博弈论","_content":"\n### 1. 选数游戏\n\n游戏的参与者：你和一位面试官\n\n游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息\n\n游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。\n\n模拟游戏的程序\n\n```python\nimport random\n\nres = random.randint(1, 100)\n\nfor i in range(5):\n    guess = int(input(\"Epoch {}: \".format(i + 1)))\n    if guess < res:\n        print(\"your guess is lower than the key.\")\n    elif guess > res:\n        print(\"your guess is greater than the key.\")\n    else:\n        print(\"Bingo, you will get {} dollars.\".format(100 - 20 * i))\nprint(\"The key is {}\".format(res))\n```\n\n#### 总结\n\n这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。\n\n","source":"_posts/十个策略故事.md","raw":"---\ntitle: 十个策略故事\ndate: 2018-07-20 00:59:24\ntags: 策略游戏\ncatrgories: 博弈论\n---\n\n### 1. 选数游戏\n\n游戏的参与者：你和一位面试官\n\n游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息\n\n游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。\n\n模拟游戏的程序\n\n```python\nimport random\n\nres = random.randint(1, 100)\n\nfor i in range(5):\n    guess = int(input(\"Epoch {}: \".format(i + 1)))\n    if guess < res:\n        print(\"your guess is lower than the key.\")\n    elif guess > res:\n        print(\"your guess is greater than the key.\")\n    else:\n        print(\"Bingo, you will get {} dollars.\".format(100 - 20 * i))\nprint(\"The key is {}\".format(res))\n```\n\n#### 总结\n\n这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。\n\n","slug":"十个策略故事","published":1,"updated":"2018-08-19T01:59:35.798Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh43001gzkvoibct7v8u","content":"<h3 id=\"1-选数游戏\"><a href=\"#1-选数游戏\" class=\"headerlink\" title=\"1. 选数游戏\"></a>1. 选数游戏</h3><p>游戏的参与者：你和一位面试官</p>\n<p>游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息</p>\n<p>游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。</p>\n<p>模拟游戏的程序</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"></span><br><span class=\"line\">res = random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">    guess = int(input(<span class=\"string\">\"Epoch &#123;&#125;: \"</span>.format(i + <span class=\"number\">1</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> guess &lt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is lower than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> guess &gt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is greater than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        print(<span class=\"string\">\"Bingo, you will get &#123;&#125; dollars.\"</span>.format(<span class=\"number\">100</span> - <span class=\"number\">20</span> * i))</span><br><span class=\"line\">print(<span class=\"string\">\"The key is &#123;&#125;\"</span>.format(res))</span><br></pre></td></tr></table></figure>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"1-选数游戏\"><a href=\"#1-选数游戏\" class=\"headerlink\" title=\"1. 选数游戏\"></a>1. 选数游戏</h3><p>游戏的参与者：你和一位面试官</p>\n<p>游戏的内容：面试官从1到100之间随机挑选一个整数，你有5次机会猜出它。每猜一次，面试官会提供给你所猜数与结果的大小信息</p>\n<p>游戏的收益：如果你第一次就猜对，你将获得100元，之后每次收益递减20元。面试官相应地损失这么多收益。</p>\n<p>模拟游戏的程序</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> random</span><br><span class=\"line\"></span><br><span class=\"line\">res = random.randint(<span class=\"number\">1</span>, <span class=\"number\">100</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">5</span>):</span><br><span class=\"line\">    guess = int(input(<span class=\"string\">\"Epoch &#123;&#125;: \"</span>.format(i + <span class=\"number\">1</span>)))</span><br><span class=\"line\">    <span class=\"keyword\">if</span> guess &lt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is lower than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> guess &gt; res:</span><br><span class=\"line\">        print(<span class=\"string\">\"your guess is greater than the key.\"</span>)</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        print(<span class=\"string\">\"Bingo, you will get &#123;&#125; dollars.\"</span>.format(<span class=\"number\">100</span> - <span class=\"number\">20</span> * i))</span><br><span class=\"line\">print(<span class=\"string\">\"The key is &#123;&#125;\"</span>.format(res))</span><br></pre></td></tr></table></figure>\n<h4 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h4><p>这场游戏揭示了是什么使用得某些事件成为一场博弈：你必须考虑到其他与参与人得目标及策略。在猜测一个随机挑选得数字时，这个数字不会被刻意掩饰。你可以用工程师得思维将区间一分为二，尽可能做得最好。但在博弈对局中，你需要考虑其他参与人将如何行动，以及那些人的决策将如何影响你的策略。</p>\n"},{"title":"卷积神经网络","date":"2018-08-26T09:20:05.000Z","mathjax":true,"_content":"\n## Convolution neural network(CNN)\n\n是一种专门用来处理**具有类似网格结构的数据**的神经网络。例如时间序列数据(可以认为在时间轴上有规律的采样形成的一维网格)和图像数据(可以看作二维的像素网格)。\n\n## 卷积运算\n\n数学定义\n$$f(t) = f_1(t) \\ast f_2(t) = \\int_{-\\infty}^{\\infty} f_1(\\tau)f_2(t - \\tau)d\\tau$$\n\n$$y(k) = f(k) \\ast h(k) = \\sum_{i = -\\infty}^{\\infty} f(i) h(k - i)$$\n\n二维图像的卷积表示\n$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(m, n)K(i - m, j - n)$$\n\n神经网络中实现的卷积运算实际上是**互相关函数**\n$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(i + m, j + n)K(m, n)$$\n\n## 三个重要思想\n\n### 稀疏交互(sparse interactions)\n\n**在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。**\n\n![](/images/dl_pic9_2.jpg)\n\n传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中的每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而卷积网络具有稀疏交互的特征，这是使核的大小远小于输入的大小来达到的。当处理一张图像时，输入的图像可能包含成千上万个像素点，但我们可以通过只占用几十到几百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。\n\n### 参数共享(parameter sharing)\n\n特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。**即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。**\n\n在传统的神经网络中，当计算一层的输出时，权重矩阵的每个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。在卷积神经网络中，核的每一个元素都作用在输入的每一个位置上。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对每一个位置都需要学习一个单独的参数集合。\n\n![](/images/dl_pic9_5.jpg)\n\n### 等变表示(equivarient representations)\n\n等变的数学概念\n$$如果函数f(x), g(x)满足 f(g(x)) = g(f(x)) 我们就说f(x)对于变换g具有等变性 $$\n\n对于卷积来说，**如果令g是输入的任意平移函数，那么卷积函数对于g具有等变性。**\n在图像处理中，卷积产生了一个二维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。\n\n## 池化(pooling)\n\n卷积网络中一个典型层包含三级\n![](/images/dl_pic9_7.jpg)\n\n**池化层**的作用是在卷积后很好地聚合了特征，通过降维来减少运算量, 缩减模型的大小，提高计算速度，同时减小噪声提高所提取特征的稳健性。\n\n**池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。** 例如最大池化函数给出相邻区域内的最大值。\n\n**不管采用什么样的池化函数，当输入做出少量平移时，池化能够帮助输入的表示近似不变**。局部平移不变性是一个很有用的性质，尤其当我们关心某个特征是否出现而不关心它出现的具体位置时。\n\n在很多任务中，池化对于处理不同大小的输入具有重要作用。例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定大小，而这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最初的输入大小。例如最终的池化层可能会输入4组综合统计特征，每组对于着图像的一个象限。\n\n## 卷积与池化作为一种无限强的先验\n\n>先验概率分布。这是一个模型参数的概率分布，它刻画了我们在看到数据之前认为什么样的模型是合理的信念。先验被认为强或者弱取决于先验中概率密度的集中程度。一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值。\n\n我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求那些处于隐藏单元的小的空间连续的接受域内的权重以外，其余权重都为零。\n\n类似地使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。\n\n## 填充(Padding)\n\n假设输入图片的大小为 $n \\times n$，而滤波器的大小为 $f \\times f$，则卷积后的输出图片大小为 $(n-f+1) \\times (n-f+1)$。\n\n这样就有两个问题：\n\n* 每次卷积运算后，输出图片的尺寸缩小；\n* 原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。\n\n为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行 **填充（Padding）**，以增加矩阵的大小。通常将 0 作为填充值。\n\n![](/images/Padding.jpg)\n\n设每个方向扩展像素点数量为 $p$，则填充后原始图片的大小为 $(n+2p) \\times (n+2p)$，滤波器大小保持 $f \\times f$不变，则输出图片大小为 $(n+2p-f+1) \\times (n+2p-f+1)$。\n\n因此，在进行卷积运算时，我们有两种选择：\n\n* **Valid 卷积**：不填充，直接卷积。结果大小为 $(n-f+1) \\times (n-f+1)$；\n* **Same 卷积**：进行填充，并使得卷积后结果大小与输入一致，这样 $p = \\frac{f-1}{2}$。\n\n在计算机视觉领域，$f$通常为奇数。原因包括 Same 卷积中 $p = \\frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。\n\n## 卷积步长(Stride)\n\n卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置 **步长（Stride）** 来压缩一部分信息。\n\n步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：\n\n![](/images/Stride.jpg)\n\n设步长为 $s$，填充长度为 $p$，输入图片大小为 $n \\times n$，滤波器大小为 $f \\times f$，则卷积后图片的尺寸为：\n\n$$\\biggl\\lfloor \\frac{n+2p-f}{s}+1   \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor$$\n\n## 高维卷积\n\n如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。\n\n![](/images/Convolutions-on-RGB-image.png)\n\n设输入图片的尺寸为 $n \\times n \\times n_c$（$n_c$为通道数），滤波器尺寸为 $f \\times f \\times n_c$，则卷积后的输出图片尺寸为 $(n-f+1) \\times (n-f+1) \\times n^{'}_c$，$n^{'}_c$为滤波器组的个数。\n\n### 符号总结\n\n设 $l$ 层为卷积层：\n\n* $f^{[l]}$：**滤波器的高（或宽）**\n* $p^{[l]}$：**填充长度**\n* $s^{[l]}$：**步长**\n* $n^{[l]}_c$：**滤波器组的数量**\n\n* **输入维度**：$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。\n\n* **输出维度**：$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_c$ 。其中\n\n$$n^{[l]}_H = \\biggl\\lfloor \\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$\n\n$$n^{[l]}_W = \\biggl\\lfloor \\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$\n\n* **每个滤波器组的维度**：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。\n* **权重维度**：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c \\times n^{[l]}_c$\n* **偏置维度**：$1 \\times 1 \\times 1 \\times n^{[l]}_c$\n","source":"_posts/卷积神经网络.md","raw":"---\ntitle: 卷积神经网络\ndate: 2018-08-26 17:20:05\ntags: CNN\ncategories: 深度学习\nmathjax: true\n---\n\n## Convolution neural network(CNN)\n\n是一种专门用来处理**具有类似网格结构的数据**的神经网络。例如时间序列数据(可以认为在时间轴上有规律的采样形成的一维网格)和图像数据(可以看作二维的像素网格)。\n\n## 卷积运算\n\n数学定义\n$$f(t) = f_1(t) \\ast f_2(t) = \\int_{-\\infty}^{\\infty} f_1(\\tau)f_2(t - \\tau)d\\tau$$\n\n$$y(k) = f(k) \\ast h(k) = \\sum_{i = -\\infty}^{\\infty} f(i) h(k - i)$$\n\n二维图像的卷积表示\n$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(m, n)K(i - m, j - n)$$\n\n神经网络中实现的卷积运算实际上是**互相关函数**\n$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(i + m, j + n)K(m, n)$$\n\n## 三个重要思想\n\n### 稀疏交互(sparse interactions)\n\n**在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。**\n\n![](/images/dl_pic9_2.jpg)\n\n传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中的每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而卷积网络具有稀疏交互的特征，这是使核的大小远小于输入的大小来达到的。当处理一张图像时，输入的图像可能包含成千上万个像素点，但我们可以通过只占用几十到几百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。\n\n### 参数共享(parameter sharing)\n\n特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。**即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。**\n\n在传统的神经网络中，当计算一层的输出时，权重矩阵的每个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。在卷积神经网络中，核的每一个元素都作用在输入的每一个位置上。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对每一个位置都需要学习一个单独的参数集合。\n\n![](/images/dl_pic9_5.jpg)\n\n### 等变表示(equivarient representations)\n\n等变的数学概念\n$$如果函数f(x), g(x)满足 f(g(x)) = g(f(x)) 我们就说f(x)对于变换g具有等变性 $$\n\n对于卷积来说，**如果令g是输入的任意平移函数，那么卷积函数对于g具有等变性。**\n在图像处理中，卷积产生了一个二维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。\n\n## 池化(pooling)\n\n卷积网络中一个典型层包含三级\n![](/images/dl_pic9_7.jpg)\n\n**池化层**的作用是在卷积后很好地聚合了特征，通过降维来减少运算量, 缩减模型的大小，提高计算速度，同时减小噪声提高所提取特征的稳健性。\n\n**池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。** 例如最大池化函数给出相邻区域内的最大值。\n\n**不管采用什么样的池化函数，当输入做出少量平移时，池化能够帮助输入的表示近似不变**。局部平移不变性是一个很有用的性质，尤其当我们关心某个特征是否出现而不关心它出现的具体位置时。\n\n在很多任务中，池化对于处理不同大小的输入具有重要作用。例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定大小，而这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最初的输入大小。例如最终的池化层可能会输入4组综合统计特征，每组对于着图像的一个象限。\n\n## 卷积与池化作为一种无限强的先验\n\n>先验概率分布。这是一个模型参数的概率分布，它刻画了我们在看到数据之前认为什么样的模型是合理的信念。先验被认为强或者弱取决于先验中概率密度的集中程度。一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值。\n\n我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求那些处于隐藏单元的小的空间连续的接受域内的权重以外，其余权重都为零。\n\n类似地使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。\n\n## 填充(Padding)\n\n假设输入图片的大小为 $n \\times n$，而滤波器的大小为 $f \\times f$，则卷积后的输出图片大小为 $(n-f+1) \\times (n-f+1)$。\n\n这样就有两个问题：\n\n* 每次卷积运算后，输出图片的尺寸缩小；\n* 原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。\n\n为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行 **填充（Padding）**，以增加矩阵的大小。通常将 0 作为填充值。\n\n![](/images/Padding.jpg)\n\n设每个方向扩展像素点数量为 $p$，则填充后原始图片的大小为 $(n+2p) \\times (n+2p)$，滤波器大小保持 $f \\times f$不变，则输出图片大小为 $(n+2p-f+1) \\times (n+2p-f+1)$。\n\n因此，在进行卷积运算时，我们有两种选择：\n\n* **Valid 卷积**：不填充，直接卷积。结果大小为 $(n-f+1) \\times (n-f+1)$；\n* **Same 卷积**：进行填充，并使得卷积后结果大小与输入一致，这样 $p = \\frac{f-1}{2}$。\n\n在计算机视觉领域，$f$通常为奇数。原因包括 Same 卷积中 $p = \\frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。\n\n## 卷积步长(Stride)\n\n卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置 **步长（Stride）** 来压缩一部分信息。\n\n步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：\n\n![](/images/Stride.jpg)\n\n设步长为 $s$，填充长度为 $p$，输入图片大小为 $n \\times n$，滤波器大小为 $f \\times f$，则卷积后图片的尺寸为：\n\n$$\\biggl\\lfloor \\frac{n+2p-f}{s}+1   \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor$$\n\n## 高维卷积\n\n如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。\n\n![](/images/Convolutions-on-RGB-image.png)\n\n设输入图片的尺寸为 $n \\times n \\times n_c$（$n_c$为通道数），滤波器尺寸为 $f \\times f \\times n_c$，则卷积后的输出图片尺寸为 $(n-f+1) \\times (n-f+1) \\times n^{'}_c$，$n^{'}_c$为滤波器组的个数。\n\n### 符号总结\n\n设 $l$ 层为卷积层：\n\n* $f^{[l]}$：**滤波器的高（或宽）**\n* $p^{[l]}$：**填充长度**\n* $s^{[l]}$：**步长**\n* $n^{[l]}_c$：**滤波器组的数量**\n\n* **输入维度**：$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。\n\n* **输出维度**：$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_c$ 。其中\n\n$$n^{[l]}_H = \\biggl\\lfloor \\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$\n\n$$n^{[l]}_W = \\biggl\\lfloor \\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$\n\n* **每个滤波器组的维度**：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。\n* **权重维度**：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c \\times n^{[l]}_c$\n* **偏置维度**：$1 \\times 1 \\times 1 \\times n^{[l]}_c$\n","slug":"卷积神经网络","published":1,"updated":"2018-08-26T09:48:30.935Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh49001lzkvo1qz0qam9","content":"<h2 id=\"Convolution-neural-network-CNN\"><a href=\"#Convolution-neural-network-CNN\" class=\"headerlink\" title=\"Convolution neural network(CNN)\"></a>Convolution neural network(CNN)</h2><p>是一种专门用来处理<strong>具有类似网格结构的数据</strong>的神经网络。例如时间序列数据(可以认为在时间轴上有规律的采样形成的一维网格)和图像数据(可以看作二维的像素网格)。</p>\n<h2 id=\"卷积运算\"><a href=\"#卷积运算\" class=\"headerlink\" title=\"卷积运算\"></a>卷积运算</h2><p>数学定义<br>$$f(t) = f_1(t) \\ast f_2(t) = \\int_{-\\infty}^{\\infty} f_1(\\tau)f_2(t - \\tau)d\\tau$$</p>\n<p>$$y(k) = f(k) \\ast h(k) = \\sum_{i = -\\infty}^{\\infty} f(i) h(k - i)$$</p>\n<p>二维图像的卷积表示<br>$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(m, n)K(i - m, j - n)$$</p>\n<p>神经网络中实现的卷积运算实际上是<strong>互相关函数</strong><br>$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(i + m, j + n)K(m, n)$$</p>\n<h2 id=\"三个重要思想\"><a href=\"#三个重要思想\" class=\"headerlink\" title=\"三个重要思想\"></a>三个重要思想</h2><h3 id=\"稀疏交互-sparse-interactions\"><a href=\"#稀疏交互-sparse-interactions\" class=\"headerlink\" title=\"稀疏交互(sparse interactions)\"></a>稀疏交互(sparse interactions)</h3><p><strong>在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。</strong></p>\n<p><img src=\"/images/dl_pic9_2.jpg\" alt=\"\"></p>\n<p>传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中的每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而卷积网络具有稀疏交互的特征，这是使核的大小远小于输入的大小来达到的。当处理一张图像时，输入的图像可能包含成千上万个像素点，但我们可以通过只占用几十到几百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。</p>\n<h3 id=\"参数共享-parameter-sharing\"><a href=\"#参数共享-parameter-sharing\" class=\"headerlink\" title=\"参数共享(parameter sharing)\"></a>参数共享(parameter sharing)</h3><p>特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。<strong>即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。</strong></p>\n<p>在传统的神经网络中，当计算一层的输出时，权重矩阵的每个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。在卷积神经网络中，核的每一个元素都作用在输入的每一个位置上。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对每一个位置都需要学习一个单独的参数集合。</p>\n<p><img src=\"/images/dl_pic9_5.jpg\" alt=\"\"></p>\n<h3 id=\"等变表示-equivarient-representations\"><a href=\"#等变表示-equivarient-representations\" class=\"headerlink\" title=\"等变表示(equivarient representations)\"></a>等变表示(equivarient representations)</h3><p>等变的数学概念<br>$$如果函数f(x), g(x)满足 f(g(x)) = g(f(x)) 我们就说f(x)对于变换g具有等变性 $$</p>\n<p>对于卷积来说，<strong>如果令g是输入的任意平移函数，那么卷积函数对于g具有等变性。</strong><br>在图像处理中，卷积产生了一个二维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。</p>\n<h2 id=\"池化-pooling\"><a href=\"#池化-pooling\" class=\"headerlink\" title=\"池化(pooling)\"></a>池化(pooling)</h2><p>卷积网络中一个典型层包含三级<br><img src=\"/images/dl_pic9_7.jpg\" alt=\"\"></p>\n<p><strong>池化层</strong>的作用是在卷积后很好地聚合了特征，通过降维来减少运算量, 缩减模型的大小，提高计算速度，同时减小噪声提高所提取特征的稳健性。</p>\n<p><strong>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。</strong> 例如最大池化函数给出相邻区域内的最大值。</p>\n<p><strong>不管采用什么样的池化函数，当输入做出少量平移时，池化能够帮助输入的表示近似不变</strong>。局部平移不变性是一个很有用的性质，尤其当我们关心某个特征是否出现而不关心它出现的具体位置时。</p>\n<p>在很多任务中，池化对于处理不同大小的输入具有重要作用。例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定大小，而这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最初的输入大小。例如最终的池化层可能会输入4组综合统计特征，每组对于着图像的一个象限。</p>\n<h2 id=\"卷积与池化作为一种无限强的先验\"><a href=\"#卷积与池化作为一种无限强的先验\" class=\"headerlink\" title=\"卷积与池化作为一种无限强的先验\"></a>卷积与池化作为一种无限强的先验</h2><blockquote>\n<p>先验概率分布。这是一个模型参数的概率分布，它刻画了我们在看到数据之前认为什么样的模型是合理的信念。先验被认为强或者弱取决于先验中概率密度的集中程度。一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值。</p>\n</blockquote>\n<p>我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求那些处于隐藏单元的小的空间连续的接受域内的权重以外，其余权重都为零。</p>\n<p>类似地使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。</p>\n<h2 id=\"填充-Padding\"><a href=\"#填充-Padding\" class=\"headerlink\" title=\"填充(Padding)\"></a>填充(Padding)</h2><p>假设输入图片的大小为 $n \\times n$，而滤波器的大小为 $f \\times f$，则卷积后的输出图片大小为 $(n-f+1) \\times (n-f+1)$。</p>\n<p>这样就有两个问题：</p>\n<ul>\n<li>每次卷积运算后，输出图片的尺寸缩小；</li>\n<li>原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。</li>\n</ul>\n<p>为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行 <strong>填充（Padding）</strong>，以增加矩阵的大小。通常将 0 作为填充值。</p>\n<p><img src=\"/images/Padding.jpg\" alt=\"\"></p>\n<p>设每个方向扩展像素点数量为 $p$，则填充后原始图片的大小为 $(n+2p) \\times (n+2p)$，滤波器大小保持 $f \\times f$不变，则输出图片大小为 $(n+2p-f+1) \\times (n+2p-f+1)$。</p>\n<p>因此，在进行卷积运算时，我们有两种选择：</p>\n<ul>\n<li><strong>Valid 卷积</strong>：不填充，直接卷积。结果大小为 $(n-f+1) \\times (n-f+1)$；</li>\n<li><strong>Same 卷积</strong>：进行填充，并使得卷积后结果大小与输入一致，这样 $p = \\frac{f-1}{2}$。</li>\n</ul>\n<p>在计算机视觉领域，$f$通常为奇数。原因包括 Same 卷积中 $p = \\frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。</p>\n<h2 id=\"卷积步长-Stride\"><a href=\"#卷积步长-Stride\" class=\"headerlink\" title=\"卷积步长(Stride)\"></a>卷积步长(Stride)</h2><p>卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置 <strong>步长（Stride）</strong> 来压缩一部分信息。</p>\n<p>步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：</p>\n<p><img src=\"/images/Stride.jpg\" alt=\"\"></p>\n<p>设步长为 $s$，填充长度为 $p$，输入图片大小为 $n \\times n$，滤波器大小为 $f \\times f$，则卷积后图片的尺寸为：</p>\n<p>$$\\biggl\\lfloor \\frac{n+2p-f}{s}+1   \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor$$</p>\n<h2 id=\"高维卷积\"><a href=\"#高维卷积\" class=\"headerlink\" title=\"高维卷积\"></a>高维卷积</h2><p>如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。</p>\n<p><img src=\"/images/Convolutions-on-RGB-image.png\" alt=\"\"></p>\n<p>设输入图片的尺寸为 $n \\times n \\times n_c$（$n_c$为通道数），滤波器尺寸为 $f \\times f \\times n_c$，则卷积后的输出图片尺寸为 $(n-f+1) \\times (n-f+1) \\times n^{‘}_c$，$n^{‘}_c$为滤波器组的个数。</p>\n<h3 id=\"符号总结\"><a href=\"#符号总结\" class=\"headerlink\" title=\"符号总结\"></a>符号总结</h3><p>设 $l$ 层为卷积层：</p>\n<ul>\n<li>$f^{[l]}$：<strong>滤波器的高（或宽）</strong></li>\n<li>$p^{[l]}$：<strong>填充长度</strong></li>\n<li>$s^{[l]}$：<strong>步长</strong></li>\n<li><p>$n^{[l]}_c$：<strong>滤波器组的数量</strong></p>\n</li>\n<li><p><strong>输入维度</strong>：$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。</p>\n</li>\n<li><p><strong>输出维度</strong>：$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_c$ 。其中</p>\n</li>\n</ul>\n<p>$$n^{[l]}_H = \\biggl\\lfloor \\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$</p>\n<p>$$n^{[l]}_W = \\biggl\\lfloor \\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$</p>\n<ul>\n<li><strong>每个滤波器组的维度</strong>：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。</li>\n<li><strong>权重维度</strong>：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c \\times n^{[l]}_c$</li>\n<li><strong>偏置维度</strong>：$1 \\times 1 \\times 1 \\times n^{[l]}_c$</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Convolution-neural-network-CNN\"><a href=\"#Convolution-neural-network-CNN\" class=\"headerlink\" title=\"Convolution neural network(CNN)\"></a>Convolution neural network(CNN)</h2><p>是一种专门用来处理<strong>具有类似网格结构的数据</strong>的神经网络。例如时间序列数据(可以认为在时间轴上有规律的采样形成的一维网格)和图像数据(可以看作二维的像素网格)。</p>\n<h2 id=\"卷积运算\"><a href=\"#卷积运算\" class=\"headerlink\" title=\"卷积运算\"></a>卷积运算</h2><p>数学定义<br>$$f(t) = f_1(t) \\ast f_2(t) = \\int_{-\\infty}^{\\infty} f_1(\\tau)f_2(t - \\tau)d\\tau$$</p>\n<p>$$y(k) = f(k) \\ast h(k) = \\sum_{i = -\\infty}^{\\infty} f(i) h(k - i)$$</p>\n<p>二维图像的卷积表示<br>$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(m, n)K(i - m, j - n)$$</p>\n<p>神经网络中实现的卷积运算实际上是<strong>互相关函数</strong><br>$$S(i, j) = I(i, j) \\ast K(i, j) = \\sum_{m}\\sum_{n}I(i + m, j + n)K(m, n)$$</p>\n<h2 id=\"三个重要思想\"><a href=\"#三个重要思想\" class=\"headerlink\" title=\"三个重要思想\"></a>三个重要思想</h2><h3 id=\"稀疏交互-sparse-interactions\"><a href=\"#稀疏交互-sparse-interactions\" class=\"headerlink\" title=\"稀疏交互(sparse interactions)\"></a>稀疏交互(sparse interactions)</h3><p><strong>在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。</strong></p>\n<p><img src=\"/images/dl_pic9_2.jpg\" alt=\"\"></p>\n<p>传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中的每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而卷积网络具有稀疏交互的特征，这是使核的大小远小于输入的大小来达到的。当处理一张图像时，输入的图像可能包含成千上万个像素点，但我们可以通过只占用几十到几百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。</p>\n<h3 id=\"参数共享-parameter-sharing\"><a href=\"#参数共享-parameter-sharing\" class=\"headerlink\" title=\"参数共享(parameter sharing)\"></a>参数共享(parameter sharing)</h3><p>特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。<strong>即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。</strong></p>\n<p>在传统的神经网络中，当计算一层的输出时，权重矩阵的每个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。在卷积神经网络中，核的每一个元素都作用在输入的每一个位置上。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对每一个位置都需要学习一个单独的参数集合。</p>\n<p><img src=\"/images/dl_pic9_5.jpg\" alt=\"\"></p>\n<h3 id=\"等变表示-equivarient-representations\"><a href=\"#等变表示-equivarient-representations\" class=\"headerlink\" title=\"等变表示(equivarient representations)\"></a>等变表示(equivarient representations)</h3><p>等变的数学概念<br>$$如果函数f(x), g(x)满足 f(g(x)) = g(f(x)) 我们就说f(x)对于变换g具有等变性 $$</p>\n<p>对于卷积来说，<strong>如果令g是输入的任意平移函数，那么卷积函数对于g具有等变性。</strong><br>在图像处理中，卷积产生了一个二维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。</p>\n<h2 id=\"池化-pooling\"><a href=\"#池化-pooling\" class=\"headerlink\" title=\"池化(pooling)\"></a>池化(pooling)</h2><p>卷积网络中一个典型层包含三级<br><img src=\"/images/dl_pic9_7.jpg\" alt=\"\"></p>\n<p><strong>池化层</strong>的作用是在卷积后很好地聚合了特征，通过降维来减少运算量, 缩减模型的大小，提高计算速度，同时减小噪声提高所提取特征的稳健性。</p>\n<p><strong>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。</strong> 例如最大池化函数给出相邻区域内的最大值。</p>\n<p><strong>不管采用什么样的池化函数，当输入做出少量平移时，池化能够帮助输入的表示近似不变</strong>。局部平移不变性是一个很有用的性质，尤其当我们关心某个特征是否出现而不关心它出现的具体位置时。</p>\n<p>在很多任务中，池化对于处理不同大小的输入具有重要作用。例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定大小，而这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最初的输入大小。例如最终的池化层可能会输入4组综合统计特征，每组对于着图像的一个象限。</p>\n<h2 id=\"卷积与池化作为一种无限强的先验\"><a href=\"#卷积与池化作为一种无限强的先验\" class=\"headerlink\" title=\"卷积与池化作为一种无限强的先验\"></a>卷积与池化作为一种无限强的先验</h2><blockquote>\n<p>先验概率分布。这是一个模型参数的概率分布，它刻画了我们在看到数据之前认为什么样的模型是合理的信念。先验被认为强或者弱取决于先验中概率密度的集中程度。一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值。</p>\n</blockquote>\n<p>我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求那些处于隐藏单元的小的空间连续的接受域内的权重以外，其余权重都为零。</p>\n<p>类似地使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。</p>\n<h2 id=\"填充-Padding\"><a href=\"#填充-Padding\" class=\"headerlink\" title=\"填充(Padding)\"></a>填充(Padding)</h2><p>假设输入图片的大小为 $n \\times n$，而滤波器的大小为 $f \\times f$，则卷积后的输出图片大小为 $(n-f+1) \\times (n-f+1)$。</p>\n<p>这样就有两个问题：</p>\n<ul>\n<li>每次卷积运算后，输出图片的尺寸缩小；</li>\n<li>原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。</li>\n</ul>\n<p>为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行 <strong>填充（Padding）</strong>，以增加矩阵的大小。通常将 0 作为填充值。</p>\n<p><img src=\"/images/Padding.jpg\" alt=\"\"></p>\n<p>设每个方向扩展像素点数量为 $p$，则填充后原始图片的大小为 $(n+2p) \\times (n+2p)$，滤波器大小保持 $f \\times f$不变，则输出图片大小为 $(n+2p-f+1) \\times (n+2p-f+1)$。</p>\n<p>因此，在进行卷积运算时，我们有两种选择：</p>\n<ul>\n<li><strong>Valid 卷积</strong>：不填充，直接卷积。结果大小为 $(n-f+1) \\times (n-f+1)$；</li>\n<li><strong>Same 卷积</strong>：进行填充，并使得卷积后结果大小与输入一致，这样 $p = \\frac{f-1}{2}$。</li>\n</ul>\n<p>在计算机视觉领域，$f$通常为奇数。原因包括 Same 卷积中 $p = \\frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。</p>\n<h2 id=\"卷积步长-Stride\"><a href=\"#卷积步长-Stride\" class=\"headerlink\" title=\"卷积步长(Stride)\"></a>卷积步长(Stride)</h2><p>卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置 <strong>步长（Stride）</strong> 来压缩一部分信息。</p>\n<p>步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：</p>\n<p><img src=\"/images/Stride.jpg\" alt=\"\"></p>\n<p>设步长为 $s$，填充长度为 $p$，输入图片大小为 $n \\times n$，滤波器大小为 $f \\times f$，则卷积后图片的尺寸为：</p>\n<p>$$\\biggl\\lfloor \\frac{n+2p-f}{s}+1   \\biggr\\rfloor \\times \\biggl\\lfloor \\frac{n+2p-f}{s}+1 \\biggr\\rfloor$$</p>\n<h2 id=\"高维卷积\"><a href=\"#高维卷积\" class=\"headerlink\" title=\"高维卷积\"></a>高维卷积</h2><p>如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。</p>\n<p><img src=\"/images/Convolutions-on-RGB-image.png\" alt=\"\"></p>\n<p>设输入图片的尺寸为 $n \\times n \\times n_c$（$n_c$为通道数），滤波器尺寸为 $f \\times f \\times n_c$，则卷积后的输出图片尺寸为 $(n-f+1) \\times (n-f+1) \\times n^{‘}_c$，$n^{‘}_c$为滤波器组的个数。</p>\n<h3 id=\"符号总结\"><a href=\"#符号总结\" class=\"headerlink\" title=\"符号总结\"></a>符号总结</h3><p>设 $l$ 层为卷积层：</p>\n<ul>\n<li>$f^{[l]}$：<strong>滤波器的高（或宽）</strong></li>\n<li>$p^{[l]}$：<strong>填充长度</strong></li>\n<li>$s^{[l]}$：<strong>步长</strong></li>\n<li><p>$n^{[l]}_c$：<strong>滤波器组的数量</strong></p>\n</li>\n<li><p><strong>输入维度</strong>：$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。</p>\n</li>\n<li><p><strong>输出维度</strong>：$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_c$ 。其中</p>\n</li>\n</ul>\n<p>$$n^{[l]}_H = \\biggl\\lfloor \\frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$</p>\n<p>$$n^{[l]}_W = \\biggl\\lfloor \\frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \\biggr\\rfloor$$</p>\n<ul>\n<li><strong>每个滤波器组的维度</strong>：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。</li>\n<li><strong>权重维度</strong>：$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_c \\times n^{[l]}_c$</li>\n<li><strong>偏置维度</strong>：$1 \\times 1 \\times 1 \\times n^{[l]}_c$</li>\n</ul>\n"},{"title":"布雷默曼极限","date":"2018-08-05T13:38:25.000Z","mathjax":true,"_content":"\n**以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度**由爱因斯坦的质能效应和海森堡测不准原理得到。\n\n$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$\n\n在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。\n","source":"_posts/布雷默曼极限.md","raw":"---\ntitle: 布雷默曼极限\ndate: 2018-08-05 21:38:25\ntags: 物理\ncategories: 计算机科学\nmathjax: true\n---\n\n**以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度**由爱因斯坦的质能效应和海森堡测不准原理得到。\n\n$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$\n\n在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。\n","slug":"布雷默曼极限","published":1,"updated":"2018-08-31T03:53:16.775Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh4d001ozkvoksssjkum","content":"<p><strong>以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度</strong>由爱因斯坦的质能效应和海森堡测不准原理得到。</p>\n<p>$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$</p>\n<p>在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>以Hans-Joachim Bremermann命名的 Bremermann极限是物质世界中独立系统的最大计算速度</strong>由爱因斯坦的质能效应和海森堡测不准原理得到。</p>\n<p>$$\\frac{c^2}{h} \\approx 1.36 \\times 10^{50} bits/s\\cdot kg$$</p>\n<p>在设计加密算法时，此值很重要，因为它可用于确定加密密钥的最小大小或创建一个永远不会被暴力搜索破解的算法所需的哈希值。例如，在Bremermann极限下运行整个地球质量的计算机每秒可执行大约$10^{75}$次数学计算。如果假设只使用一个操作可以测试加密密钥，那么典型的128位密钥可以在$10^{36}$秒内被破解。但是，256位密钥（已在某些系统中使用）将需要大约两分钟才能破解。使用512位密钥会将破解时间增加到接近$10^{72}$年，而不会将加密时间增加超过常数因子（取决于所使用的加密算法）。</p>\n"},{"title":"恋爱领域中普遍存在的贬低倾向","date":"2018-07-19T13:03:37.000Z","_content":"\n## 心理阳痿\n\n### 症状\n\n受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。\n\n### 成因\n\n实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 **情感趋向**和 **肉欲趋向**。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。\n\n### 措施\n\n从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。\n\n----\n摘自弗洛伊德的《爱情心理学》第二章\n","source":"_posts/恋爱领域中普遍存在的贬低倾向.md","raw":"---\ntitle: 恋爱领域中普遍存在的贬低倾向\ndate: 2018-07-19 21:03:37\ntags: 爱情心理学\ncategories: 心理学\n---\n\n## 心理阳痿\n\n### 症状\n\n受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。\n\n### 成因\n\n实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 **情感趋向**和 **肉欲趋向**。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。\n\n### 措施\n\n从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。\n\n----\n摘自弗洛伊德的《爱情心理学》第二章\n","slug":"恋爱领域中普遍存在的贬低倾向","published":1,"updated":"2018-08-31T03:53:26.675Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh4j001tzkvoa21ov879","content":"<h2 id=\"心理阳痿\"><a href=\"#心理阳痿\" class=\"headerlink\" title=\"心理阳痿\"></a>心理阳痿</h2><h3 id=\"症状\"><a href=\"#症状\" class=\"headerlink\" title=\"症状\"></a>症状</h3><p>受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。</p>\n<h3 id=\"成因\"><a href=\"#成因\" class=\"headerlink\" title=\"成因\"></a>成因</h3><p>实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 <strong>情感趋向</strong>和 <strong>肉欲趋向</strong>。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。</p>\n<h3 id=\"措施\"><a href=\"#措施\" class=\"headerlink\" title=\"措施\"></a>措施</h3><p>从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。</p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第二章</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"心理阳痿\"><a href=\"#心理阳痿\" class=\"headerlink\" title=\"心理阳痿\"></a>心理阳痿</h2><h3 id=\"症状\"><a href=\"#症状\" class=\"headerlink\" title=\"症状\"></a>症状</h3><p>受该障碍困扰者，力比多的活动强烈而旺盛，然而在发作时，执行性欲之器官却无法实行性行为。只有在与某些人做爱时才会失败，与其他人则不会。</p>\n<h3 id=\"成因\"><a href=\"#成因\" class=\"headerlink\" title=\"成因\"></a>成因</h3><p>实际上，这是由于主体的某些心理情结尚未得到认识而产生出的抑制力。他未能克服对母亲或姐妹的乱伦固着，这一点在病因中十分显著，且普遍存在于受此障碍困扰的人身上。此外，主体在婴幼儿时期的性活动中意外获得的某些受挫印象所产生的影响力从总体上削减了理应导向女性性对象的力比多。该障碍的基础在于，在力比多的发展获得我们所谓的正常的最终形态之前，其发展历程中出现了一种抑制，而这或许也是所有神经官能障碍的基础。有两种因素的结合保证了完全正确的爱情态度，即 <strong>情感趋向</strong>和 <strong>肉欲趋向</strong>。这一发展上的抑制有两个来源：一是童年期的强烈固着，二是在反乱伦壁垒的干涉下个体在现实中遭遇了挫折。</p>\n<h3 id=\"措施\"><a href=\"#措施\" class=\"headerlink\" title=\"措施\"></a>措施</h3><p>从心理上贬低性对象，而正常情况在对性对象的过高评价，在这里则会被留给了乱伦对象及其代表。一旦完成心理上的贬低，便能自由地表达性欲。</p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第二章</p>\n"},{"title":"数据划分：训练 / 验证 / 测试集","date":"2018-07-20T06:52:44.000Z","_content":"\n## 数据划分：训练 / 验证 / 测试集\n\n应用深度学习是一个典型的迭代过程。\n\n对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：\n\n* 训练集（train set）：用训练集对算法或模型进行**训练**过程；\n* 验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行**交叉验证**，**选择出最好的模型**；\n* 测试集（test set）：最后利用测试集对模型进行测试，**获取模型运行的无偏估计**（对学习方法进行评估）。\n\n在**小数据量**的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：\n\n* 无验证集的情况：70% / 30%；\n* 有验证集的情况：60% / 20% / 20%；\n\n而在如今的**大数据时代**，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。\n\n验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。\n\n测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。\n\n* 100 万数据量：98% / 1% / 1%；\n* 超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）\n\n### 建议\n\n建议**验证集要和训练集来自于同一个分布**（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。\n\n如果不需要用**无偏估计**来评估模型的性能，则可以不需要测试集。\n\n### 补充：交叉验证（cross validation）\n\n交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。\n\n### 参考资料\n\n[无偏估计_百度百科](https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin)\n","source":"_posts/数据划分.md","raw":"---\ntitle: 数据划分：训练 / 验证 / 测试集\ndate: 2018-07-20 14:52:44\ntags: 数据\ncategories: 深度学习\n---\n\n## 数据划分：训练 / 验证 / 测试集\n\n应用深度学习是一个典型的迭代过程。\n\n对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：\n\n* 训练集（train set）：用训练集对算法或模型进行**训练**过程；\n* 验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行**交叉验证**，**选择出最好的模型**；\n* 测试集（test set）：最后利用测试集对模型进行测试，**获取模型运行的无偏估计**（对学习方法进行评估）。\n\n在**小数据量**的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：\n\n* 无验证集的情况：70% / 30%；\n* 有验证集的情况：60% / 20% / 20%；\n\n而在如今的**大数据时代**，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。\n\n验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。\n\n测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。\n\n* 100 万数据量：98% / 1% / 1%；\n* 超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）\n\n### 建议\n\n建议**验证集要和训练集来自于同一个分布**（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。\n\n如果不需要用**无偏估计**来评估模型的性能，则可以不需要测试集。\n\n### 补充：交叉验证（cross validation）\n\n交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。\n\n### 参考资料\n\n[无偏估计_百度百科](https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin)\n","slug":"数据划分","published":1,"updated":"2018-08-31T03:53:47.798Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh4m001vzkvor5qf3p8u","content":"<h2 id=\"数据划分：训练-验证-测试集\"><a href=\"#数据划分：训练-验证-测试集\" class=\"headerlink\" title=\"数据划分：训练 / 验证 / 测试集\"></a>数据划分：训练 / 验证 / 测试集</h2><p>应用深度学习是一个典型的迭代过程。</p>\n<p>对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：</p>\n<ul>\n<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>\n<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>；</li>\n<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>\n</ul>\n<p>在<strong>小数据量</strong>的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>\n<ul>\n<li>无验证集的情况：70% / 30%；</li>\n<li>有验证集的情况：60% / 20% / 20%；</li>\n</ul>\n<p>而在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>\n<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>\n<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>\n<ul>\n<li>100 万数据量：98% / 1% / 1%；</li>\n<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>\n</ul>\n<h3 id=\"建议\"><a href=\"#建议\" class=\"headerlink\" title=\"建议\"></a>建议</h3><p>建议<strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p>\n<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。</p>\n<h3 id=\"补充：交叉验证（cross-validation）\"><a href=\"#补充：交叉验证（cross-validation）\" class=\"headerlink\" title=\"补充：交叉验证（cross validation）\"></a>补充：交叉验证（cross validation）</h3><p>交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><p><a href=\"https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin\" target=\"_blank\" rel=\"noopener\">无偏估计_百度百科</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"数据划分：训练-验证-测试集\"><a href=\"#数据划分：训练-验证-测试集\" class=\"headerlink\" title=\"数据划分：训练 / 验证 / 测试集\"></a>数据划分：训练 / 验证 / 测试集</h2><p>应用深度学习是一个典型的迭代过程。</p>\n<p>对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：</p>\n<ul>\n<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>\n<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>；</li>\n<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>\n</ul>\n<p>在<strong>小数据量</strong>的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>\n<ul>\n<li>无验证集的情况：70% / 30%；</li>\n<li>有验证集的情况：60% / 20% / 20%；</li>\n</ul>\n<p>而在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>\n<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>\n<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>\n<ul>\n<li>100 万数据量：98% / 1% / 1%；</li>\n<li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li>\n</ul>\n<h3 id=\"建议\"><a href=\"#建议\" class=\"headerlink\" title=\"建议\"></a>建议</h3><p>建议<strong>验证集要和训练集来自于同一个分布</strong>（数据来源一致），可以使得机器学习算法变得更快并获得更好的效果。</p>\n<p>如果不需要用<strong>无偏估计</strong>来评估模型的性能，则可以不需要测试集。</p>\n<h3 id=\"补充：交叉验证（cross-validation）\"><a href=\"#补充：交叉验证（cross-validation）\" class=\"headerlink\" title=\"补充：交叉验证（cross validation）\"></a>补充：交叉验证（cross validation）</h3><p>交叉验证的基本思想是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p>\n<h3 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h3><p><a href=\"https://baike.baidu.com/item/%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/3370664?fr=aladdin\" target=\"_blank\" rel=\"noopener\">无偏估计_百度百科</a></p>\n"},{"title":"数据的加载-预处理-可视化","date":"2018-07-21T09:33:49.000Z","_content":"## 图片操作\n\n### 把图片转换为向量\n\n```python\ndef image2vector(image):\n    \"\"\"\n    Argument:\n    image -- a numpy array of shape (length, height, depth)\n\n    Returns:\n    v -- a vector of shape (length*height*depth, 1)\n    \"\"\"\n    v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2], 1))    \n    return v\n```\n### 读入图片\n\n```python\nmy_image = \"my_image.jpg\" # change this to the name of your image file\nmy_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\n\nplt.imshow(image)\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n```\n\n## 矩阵的正则化\n\n```python\ndef normalize(x):\n    \"\"\"\n    Implement a function that normalizes each row of the matrix x (to have unit length).\n\n    Argument:\n    x -- A numpy matrix of shape (n, m)\n\n    Returns:\n    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n    \"\"\"\n    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)  # column: axis=0\n    x = x / x_norm\n    return x\n```\n\n## 猫的数据集\n\n```python\ndef load_dataset():\n    \"\"\"\n    Returns:\n    train_set_x_orig -- shape of (209, 64, 64, 3)\n    train_set_y_orig -- shape of (1, 209)\n    test_set_x_orig -- shape of (50, 64, 64, 3)\n    test_set_y_orig -- shape of (1, 50)\n    \"\"\"\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\n    return train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes\n```\n\n### 可视化\n\n```python\nindex = 23\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\nplt.show()\n```\n\n### 向量化\n\n```python\ntrain_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n```\n\n### 标准化\n\n```python\ntrain_x_set = train_x_set_flatten / 255\ntest_x_set = test_x_set_flatten / 255\n```\n\n## 二维数据的一般操作\n\n### 加载\n\n```python\ndef load_planar_dataset():\n    \"\"\"\n    Returns:\n    X -- a numpy array shaped (2, 400) that contains features (x1, x2)\n    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)\n    \"\"\"\n    np.random.seed(1)\n    m = 400  # number of examples\n    N = int(m / 2)  # number of points per class\n    D = 2  # dimensionality\n    X = np.zeros((m, D))  # data matrix where each row is a single example\n    # labels vector (0 for red, 1 for blue)\n    Y = np.zeros((m, 1), dtype='uint8')\n    a = 4  # maximum ray of the flower\n    for j in range(2):\n        ix = range(N * j, N * (j + 1))\n        t = np.linspace(j * 3.12, (j + 1) * 3.12, N) + \\\n            np.random.randn(N) * 0.2  # theta\n        r = a * np.sin(4 * t) + np.random.randn(N) * 0.2  # radius\n        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        Y[ix] = j\n    X = X.T\n    Y = Y.T\n    return X, Y\n```\n\n### 可视化\n\n```python\nplt.scatter(X[0, :], X[1, :], c=Y.flatten(), s=40, cmp=plt.cm.Spectral)\nplt.show()\n```\n\n### 可视化决策边界\n\n```python\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y.flatten(), cmap=plt.cm.Spectral)\n```\n\n\n## Sklearn中的其他数据集\n\n```python\ndef load_extra_datasets():\n    N = 200\n    noisy_circles = sklearn.datasets.make_circles(\n        n_samples=N, factor=.5, noise=.3)\n    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n    blobs = sklearn.datasets.make_blobs(\n        n_samples=N, random_state=5, n_features=2, centers=6)\n    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(\n        mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n\n    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure\n```\n","source":"_posts/数据的加载-预处理-可视化.md","raw":"---\ntitle: 数据的加载-预处理-可视化\ndate: 2018-07-21 17:33:49\ntags: 数据\ncategories: 深度学习\n---\n## 图片操作\n\n### 把图片转换为向量\n\n```python\ndef image2vector(image):\n    \"\"\"\n    Argument:\n    image -- a numpy array of shape (length, height, depth)\n\n    Returns:\n    v -- a vector of shape (length*height*depth, 1)\n    \"\"\"\n    v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2], 1))    \n    return v\n```\n### 读入图片\n\n```python\nmy_image = \"my_image.jpg\" # change this to the name of your image file\nmy_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\nfname = \"images/\" + my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\n\nplt.imshow(image)\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n```\n\n## 矩阵的正则化\n\n```python\ndef normalize(x):\n    \"\"\"\n    Implement a function that normalizes each row of the matrix x (to have unit length).\n\n    Argument:\n    x -- A numpy matrix of shape (n, m)\n\n    Returns:\n    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n    \"\"\"\n    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)  # column: axis=0\n    x = x / x_norm\n    return x\n```\n\n## 猫的数据集\n\n```python\ndef load_dataset():\n    \"\"\"\n    Returns:\n    train_set_x_orig -- shape of (209, 64, 64, 3)\n    train_set_y_orig -- shape of (1, 209)\n    test_set_x_orig -- shape of (50, 64, 64, 3)\n    test_set_y_orig -- shape of (1, 50)\n    \"\"\"\n    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\n    return train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes\n```\n\n### 可视化\n\n```python\nindex = 23\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\nplt.show()\n```\n\n### 向量化\n\n```python\ntrain_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n```\n\n### 标准化\n\n```python\ntrain_x_set = train_x_set_flatten / 255\ntest_x_set = test_x_set_flatten / 255\n```\n\n## 二维数据的一般操作\n\n### 加载\n\n```python\ndef load_planar_dataset():\n    \"\"\"\n    Returns:\n    X -- a numpy array shaped (2, 400) that contains features (x1, x2)\n    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)\n    \"\"\"\n    np.random.seed(1)\n    m = 400  # number of examples\n    N = int(m / 2)  # number of points per class\n    D = 2  # dimensionality\n    X = np.zeros((m, D))  # data matrix where each row is a single example\n    # labels vector (0 for red, 1 for blue)\n    Y = np.zeros((m, 1), dtype='uint8')\n    a = 4  # maximum ray of the flower\n    for j in range(2):\n        ix = range(N * j, N * (j + 1))\n        t = np.linspace(j * 3.12, (j + 1) * 3.12, N) + \\\n            np.random.randn(N) * 0.2  # theta\n        r = a * np.sin(4 * t) + np.random.randn(N) * 0.2  # radius\n        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n        Y[ix] = j\n    X = X.T\n    Y = Y.T\n    return X, Y\n```\n\n### 可视化\n\n```python\nplt.scatter(X[0, :], X[1, :], c=Y.flatten(), s=40, cmp=plt.cm.Spectral)\nplt.show()\n```\n\n### 可视化决策边界\n\n```python\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y.flatten(), cmap=plt.cm.Spectral)\n```\n\n\n## Sklearn中的其他数据集\n\n```python\ndef load_extra_datasets():\n    N = 200\n    noisy_circles = sklearn.datasets.make_circles(\n        n_samples=N, factor=.5, noise=.3)\n    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n    blobs = sklearn.datasets.make_blobs(\n        n_samples=N, random_state=5, n_features=2, centers=6)\n    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(\n        mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n\n    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure\n```\n","slug":"数据的加载-预处理-可视化","published":1,"updated":"2018-08-31T03:54:02.897Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh4q001yzkvonjkru0pe","content":"<h2 id=\"图片操作\"><a href=\"#图片操作\" class=\"headerlink\" title=\"图片操作\"></a>图片操作</h2><h3 id=\"把图片转换为向量\"><a href=\"#把图片转换为向量\" class=\"headerlink\" title=\"把图片转换为向量\"></a>把图片转换为向量</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">image2vector</span><span class=\"params\">(image)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    v = image.reshape((image.shape[<span class=\"number\">0</span>] * image.shape[<span class=\"number\">1</span>] * image.shape[<span class=\"number\">2</span>], <span class=\"number\">1</span>))    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<h3 id=\"读入图片\"><a href=\"#读入图片\" class=\"headerlink\" title=\"读入图片\"></a>读入图片</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span> <span class=\"comment\"># change this to the name of your image file</span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">1</span>] <span class=\"comment\"># the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"矩阵的正则化\"><a href=\"#矩阵的正则化\" class=\"headerlink\" title=\"矩阵的正则化\"></a>矩阵的正则化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalize</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    x -- A numpy matrix of shape (n, m)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    x_norm = np.linalg.norm(x, ord=<span class=\"number\">2</span>, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)  <span class=\"comment\"># column: axis=0</span></span><br><span class=\"line\">    x = x / x_norm</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"猫的数据集\"><a href=\"#猫的数据集\" class=\"headerlink\" title=\"猫的数据集\"></a>猫的数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    train_set_x_orig -- shape of (209, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    train_set_y_orig -- shape of (1, 209)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_x_orig -- shape of (50, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_y_orig -- shape of (1, 50)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:]) <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:]) <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:]) <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:]) <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:]) <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\"></span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化\"><a href=\"#可视化\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index = <span class=\"number\">23</span></span><br><span class=\"line\">plt.imshow(train_set_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_set_y[:, index]) + <span class=\"string\">\", it's a '\"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"' picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\">test_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br></pre></td></tr></table></figure>\n<h3 id=\"标准化\"><a href=\"#标准化\" class=\"headerlink\" title=\"标准化\"></a>标准化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set = train_x_set_flatten / <span class=\"number\">255</span></span><br><span class=\"line\">test_x_set = test_x_set_flatten / <span class=\"number\">255</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"二维数据的一般操作\"><a href=\"#二维数据的一般操作\" class=\"headerlink\" title=\"二维数据的一般操作\"></a>二维数据的一般操作</h2><h3 id=\"加载\"><a href=\"#加载\" class=\"headerlink\" title=\"加载\"></a>加载</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_planar_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    X -- a numpy array shaped (2, 400) that contains features (x1, x2)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">1</span>)</span><br><span class=\"line\">    m = <span class=\"number\">400</span>  <span class=\"comment\"># number of examples</span></span><br><span class=\"line\">    N = int(m / <span class=\"number\">2</span>)  <span class=\"comment\"># number of points per class</span></span><br><span class=\"line\">    D = <span class=\"number\">2</span>  <span class=\"comment\"># dimensionality</span></span><br><span class=\"line\">    X = np.zeros((m, D))  <span class=\"comment\"># data matrix where each row is a single example</span></span><br><span class=\"line\">    <span class=\"comment\"># labels vector (0 for red, 1 for blue)</span></span><br><span class=\"line\">    Y = np.zeros((m, <span class=\"number\">1</span>), dtype=<span class=\"string\">'uint8'</span>)</span><br><span class=\"line\">    a = <span class=\"number\">4</span>  <span class=\"comment\"># maximum ray of the flower</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>):</span><br><span class=\"line\">        ix = range(N * j, N * (j + <span class=\"number\">1</span>))</span><br><span class=\"line\">        t = np.linspace(j * <span class=\"number\">3.12</span>, (j + <span class=\"number\">1</span>) * <span class=\"number\">3.12</span>, N) + \\</span><br><span class=\"line\">            np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># theta</span></span><br><span class=\"line\">        r = a * np.sin(<span class=\"number\">4</span> * t) + np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># radius</span></span><br><span class=\"line\">        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]</span><br><span class=\"line\">        Y[ix] = j</span><br><span class=\"line\">    X = X.T</span><br><span class=\"line\">    Y = Y.T</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X, Y</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化-1\"><a href=\"#可视化-1\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=Y.flatten(), s=<span class=\"number\">40</span>, cmp=plt.cm.Spectral)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化决策边界\"><a href=\"#可视化决策边界\" class=\"headerlink\" title=\"可视化决策边界\"></a>可视化决策边界</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_boundary</span><span class=\"params\">(model, X, y)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Set min and max values and give it some padding</span></span><br><span class=\"line\">    x_min, x_max = X[<span class=\"number\">0</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">0</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    y_min, y_max = X[<span class=\"number\">1</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">1</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    h = <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"comment\"># Generate a grid of points with distance h between them</span></span><br><span class=\"line\">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class=\"line\">                         np.arange(y_min, y_max, h))</span><br><span class=\"line\">    <span class=\"comment\"># Predict the function value for the whole grid</span></span><br><span class=\"line\">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">    Z = Z.reshape(xx.shape)</span><br><span class=\"line\">    <span class=\"comment\"># Plot the contour and training examples</span></span><br><span class=\"line\">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'x2'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'x1'</span>)</span><br><span class=\"line\">    plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=y.flatten(), cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Sklearn中的其他数据集\"><a href=\"#Sklearn中的其他数据集\" class=\"headerlink\" title=\"Sklearn中的其他数据集\"></a>Sklearn中的其他数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_extra_datasets</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    N = <span class=\"number\">200</span></span><br><span class=\"line\">    noisy_circles = sklearn.datasets.make_circles(</span><br><span class=\"line\">        n_samples=N, factor=<span class=\"number\">.5</span>, noise=<span class=\"number\">.3</span>)</span><br><span class=\"line\">    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=<span class=\"number\">.2</span>)</span><br><span class=\"line\">    blobs = sklearn.datasets.make_blobs(</span><br><span class=\"line\">        n_samples=N, random_state=<span class=\"number\">5</span>, n_features=<span class=\"number\">2</span>, centers=<span class=\"number\">6</span>)</span><br><span class=\"line\">    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(</span><br><span class=\"line\">        mean=<span class=\"keyword\">None</span>, cov=<span class=\"number\">0.5</span>, n_samples=N, n_features=<span class=\"number\">2</span>, n_classes=<span class=\"number\">2</span>, shuffle=<span class=\"keyword\">True</span>, random_state=<span class=\"keyword\">None</span>)</span><br><span class=\"line\">    no_structure = np.random.rand(N, <span class=\"number\">2</span>), np.random.rand(N, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"图片操作\"><a href=\"#图片操作\" class=\"headerlink\" title=\"图片操作\"></a>图片操作</h2><h3 id=\"把图片转换为向量\"><a href=\"#把图片转换为向量\" class=\"headerlink\" title=\"把图片转换为向量\"></a>把图片转换为向量</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">image2vector</span><span class=\"params\">(image)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    v = image.reshape((image.shape[<span class=\"number\">0</span>] * image.shape[<span class=\"number\">1</span>] * image.shape[<span class=\"number\">2</span>], <span class=\"number\">1</span>))    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> v</span><br></pre></td></tr></table></figure>\n<h3 id=\"读入图片\"><a href=\"#读入图片\" class=\"headerlink\" title=\"读入图片\"></a>读入图片</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_image = <span class=\"string\">\"my_image.jpg\"</span> <span class=\"comment\"># change this to the name of your image file</span></span><br><span class=\"line\">my_label_y = [<span class=\"number\">1</span>] <span class=\"comment\"># the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class=\"line\">fname = <span class=\"string\">\"images/\"</span> + my_image</span><br><span class=\"line\">image = np.array(ndimage.imread(fname, flatten=<span class=\"keyword\">False</span>))</span><br><span class=\"line\">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*<span class=\"number\">3</span>,<span class=\"number\">1</span>))</span><br><span class=\"line\">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.imshow(image)</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(np.squeeze(my_predicted_image)) + <span class=\"string\">\", your L-layer model predicts a \\\"\"</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"\\\" picture.\"</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"矩阵的正则化\"><a href=\"#矩阵的正则化\" class=\"headerlink\" title=\"矩阵的正则化\"></a>矩阵的正则化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">normalize</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Argument:</span></span><br><span class=\"line\"><span class=\"string\">    x -- A numpy matrix of shape (n, m)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    x_norm = np.linalg.norm(x, ord=<span class=\"number\">2</span>, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)  <span class=\"comment\"># column: axis=0</span></span><br><span class=\"line\">    x = x / x_norm</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x</span><br></pre></td></tr></table></figure>\n<h2 id=\"猫的数据集\"><a href=\"#猫的数据集\" class=\"headerlink\" title=\"猫的数据集\"></a>猫的数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    train_set_x_orig -- shape of (209, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    train_set_y_orig -- shape of (1, 209)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_x_orig -- shape of (50, 64, 64, 3)</span></span><br><span class=\"line\"><span class=\"string\">    test_set_y_orig -- shape of (1, 50)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    train_dataset = h5py.File(<span class=\"string\">'datasets/train_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    train_set_x_orig = np.array(train_dataset[<span class=\"string\">\"train_set_x\"</span>][:]) <span class=\"comment\"># your train set features</span></span><br><span class=\"line\">    train_set_y_orig = np.array(train_dataset[<span class=\"string\">\"train_set_y\"</span>][:]) <span class=\"comment\"># your train set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    test_dataset = h5py.File(<span class=\"string\">'datasets/test_catvnoncat.h5'</span>, <span class=\"string\">\"r\"</span>)</span><br><span class=\"line\">    test_set_x_orig = np.array(test_dataset[<span class=\"string\">\"test_set_x\"</span>][:]) <span class=\"comment\"># your test set features</span></span><br><span class=\"line\">    test_set_y_orig = np.array(test_dataset[<span class=\"string\">\"test_set_y\"</span>][:]) <span class=\"comment\"># your test set labels</span></span><br><span class=\"line\"></span><br><span class=\"line\">    classes = np.array(test_dataset[<span class=\"string\">\"list_classes\"</span>][:]) <span class=\"comment\"># the list of classes</span></span><br><span class=\"line\"></span><br><span class=\"line\">    train_set_y_orig = train_set_y_orig.reshape((<span class=\"number\">1</span>, train_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\">    test_set_y_orig = test_set_y_orig.reshape((<span class=\"number\">1</span>, test_set_y_orig.shape[<span class=\"number\">0</span>]))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_set_x_orig, train_set_y_orig, test_set_x_origtest_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化\"><a href=\"#可视化\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">index = <span class=\"number\">23</span></span><br><span class=\"line\">plt.imshow(train_set_x_orig[index])</span><br><span class=\"line\"><span class=\"keyword\">print</span> (<span class=\"string\">\"y = \"</span> + str(train_set_y[:, index]) + <span class=\"string\">\", it's a '\"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class=\"string\">\"utf-8\"</span>) +  <span class=\"string\">\"' picture.\"</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br><span class=\"line\">test_x_set_flatten = train_set_x_orig.reshape(test_set_x_orig.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>).T</span><br></pre></td></tr></table></figure>\n<h3 id=\"标准化\"><a href=\"#标准化\" class=\"headerlink\" title=\"标准化\"></a>标准化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_x_set = train_x_set_flatten / <span class=\"number\">255</span></span><br><span class=\"line\">test_x_set = test_x_set_flatten / <span class=\"number\">255</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"二维数据的一般操作\"><a href=\"#二维数据的一般操作\" class=\"headerlink\" title=\"二维数据的一般操作\"></a>二维数据的一般操作</h2><h3 id=\"加载\"><a href=\"#加载\" class=\"headerlink\" title=\"加载\"></a>加载</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_planar_dataset</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    X -- a numpy array shaped (2, 400) that contains features (x1, x2)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- a numpy array shaped (1, 400) that contains labels (0, 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">1</span>)</span><br><span class=\"line\">    m = <span class=\"number\">400</span>  <span class=\"comment\"># number of examples</span></span><br><span class=\"line\">    N = int(m / <span class=\"number\">2</span>)  <span class=\"comment\"># number of points per class</span></span><br><span class=\"line\">    D = <span class=\"number\">2</span>  <span class=\"comment\"># dimensionality</span></span><br><span class=\"line\">    X = np.zeros((m, D))  <span class=\"comment\"># data matrix where each row is a single example</span></span><br><span class=\"line\">    <span class=\"comment\"># labels vector (0 for red, 1 for blue)</span></span><br><span class=\"line\">    Y = np.zeros((m, <span class=\"number\">1</span>), dtype=<span class=\"string\">'uint8'</span>)</span><br><span class=\"line\">    a = <span class=\"number\">4</span>  <span class=\"comment\"># maximum ray of the flower</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">2</span>):</span><br><span class=\"line\">        ix = range(N * j, N * (j + <span class=\"number\">1</span>))</span><br><span class=\"line\">        t = np.linspace(j * <span class=\"number\">3.12</span>, (j + <span class=\"number\">1</span>) * <span class=\"number\">3.12</span>, N) + \\</span><br><span class=\"line\">            np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># theta</span></span><br><span class=\"line\">        r = a * np.sin(<span class=\"number\">4</span> * t) + np.random.randn(N) * <span class=\"number\">0.2</span>  <span class=\"comment\"># radius</span></span><br><span class=\"line\">        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]</span><br><span class=\"line\">        Y[ix] = j</span><br><span class=\"line\">    X = X.T</span><br><span class=\"line\">    Y = Y.T</span><br><span class=\"line\">    <span class=\"keyword\">return</span> X, Y</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化-1\"><a href=\"#可视化-1\" class=\"headerlink\" title=\"可视化\"></a>可视化</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=Y.flatten(), s=<span class=\"number\">40</span>, cmp=plt.cm.Spectral)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<h3 id=\"可视化决策边界\"><a href=\"#可视化决策边界\" class=\"headerlink\" title=\"可视化决策边界\"></a>可视化决策边界</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot_decision_boundary</span><span class=\"params\">(model, X, y)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Set min and max values and give it some padding</span></span><br><span class=\"line\">    x_min, x_max = X[<span class=\"number\">0</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">0</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    y_min, y_max = X[<span class=\"number\">1</span>, :].min() - <span class=\"number\">1</span>, X[<span class=\"number\">1</span>, :].max() + <span class=\"number\">1</span></span><br><span class=\"line\">    h = <span class=\"number\">0.01</span></span><br><span class=\"line\">    <span class=\"comment\"># Generate a grid of points with distance h between them</span></span><br><span class=\"line\">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class=\"line\">                         np.arange(y_min, y_max, h))</span><br><span class=\"line\">    <span class=\"comment\"># Predict the function value for the whole grid</span></span><br><span class=\"line\">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class=\"line\">    Z = Z.reshape(xx.shape)</span><br><span class=\"line\">    <span class=\"comment\"># Plot the contour and training examples</span></span><br><span class=\"line\">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'x2'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'x1'</span>)</span><br><span class=\"line\">    plt.scatter(X[<span class=\"number\">0</span>, :], X[<span class=\"number\">1</span>, :], c=y.flatten(), cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure>\n<h2 id=\"Sklearn中的其他数据集\"><a href=\"#Sklearn中的其他数据集\" class=\"headerlink\" title=\"Sklearn中的其他数据集\"></a>Sklearn中的其他数据集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_extra_datasets</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    N = <span class=\"number\">200</span></span><br><span class=\"line\">    noisy_circles = sklearn.datasets.make_circles(</span><br><span class=\"line\">        n_samples=N, factor=<span class=\"number\">.5</span>, noise=<span class=\"number\">.3</span>)</span><br><span class=\"line\">    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=<span class=\"number\">.2</span>)</span><br><span class=\"line\">    blobs = sklearn.datasets.make_blobs(</span><br><span class=\"line\">        n_samples=N, random_state=<span class=\"number\">5</span>, n_features=<span class=\"number\">2</span>, centers=<span class=\"number\">6</span>)</span><br><span class=\"line\">    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(</span><br><span class=\"line\">        mean=<span class=\"keyword\">None</span>, cov=<span class=\"number\">0.5</span>, n_samples=N, n_features=<span class=\"number\">2</span>, n_classes=<span class=\"number\">2</span>, shuffle=<span class=\"keyword\">True</span>, random_state=<span class=\"keyword\">None</span>)</span><br><span class=\"line\">    no_structure = np.random.rand(N, <span class=\"number\">2</span>), np.random.rand(N, <span class=\"number\">2</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure</span><br></pre></td></tr></table></figure>\n"},{"title":"标准化输入","date":"2018-07-20T08:27:20.000Z","mathjax":true,"_content":"## 标准化输入\n\n使用标准化处理输入 X 能够有效加速收敛。\n\n### 标准化公式\n\n$$x = \\frac{x - \\mu}{\\sigma}$$\n\n其中，\n\n$$\\mu = \\frac{1}{m}\\sum^m\\_{i=1}x^{(i)}$$\n\n$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m\\_{i=1}{x^{(i)}}^2}$$\n\n（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）\n\n### 使用标准化的原因\n\n![why_normalize](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png)\n\n有图可知，使用标准化前后，成本函数的形状有较大差别。\n\n在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。\n","source":"_posts/标准化输入.md","raw":"---\ntitle: 标准化输入\ndate: 2018-07-20 16:27:20\ntags: 优化算法\ncategories: 深度学习\nmathjax: true\n---\n## 标准化输入\n\n使用标准化处理输入 X 能够有效加速收敛。\n\n### 标准化公式\n\n$$x = \\frac{x - \\mu}{\\sigma}$$\n\n其中，\n\n$$\\mu = \\frac{1}{m}\\sum^m\\_{i=1}x^{(i)}$$\n\n$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m\\_{i=1}{x^{(i)}}^2}$$\n\n（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）\n\n### 使用标准化的原因\n\n![why_normalize](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png)\n\n有图可知，使用标准化前后，成本函数的形状有较大差别。\n\n在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。\n","slug":"标准化输入","published":1,"updated":"2018-08-31T03:54:12.437Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh4x0022zkvob6xght3c","content":"<h2 id=\"标准化输入\"><a href=\"#标准化输入\" class=\"headerlink\" title=\"标准化输入\"></a>标准化输入</h2><p>使用标准化处理输入 X 能够有效加速收敛。</p>\n<h3 id=\"标准化公式\"><a href=\"#标准化公式\" class=\"headerlink\" title=\"标准化公式\"></a>标准化公式</h3><p>$$x = \\frac{x - \\mu}{\\sigma}$$</p>\n<p>其中，</p>\n<p>$$\\mu = \\frac{1}{m}\\sum^m_{i=1}x^{(i)}$$</p>\n<p>$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m_{i=1}{x^{(i)}}^2}$$</p>\n<p>（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）</p>\n<h3 id=\"使用标准化的原因\"><a href=\"#使用标准化的原因\" class=\"headerlink\" title=\"使用标准化的原因\"></a>使用标准化的原因</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png\" alt=\"why_normalize\"></p>\n<p>有图可知，使用标准化前后，成本函数的形状有较大差别。</p>\n<p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"标准化输入\"><a href=\"#标准化输入\" class=\"headerlink\" title=\"标准化输入\"></a>标准化输入</h2><p>使用标准化处理输入 X 能够有效加速收敛。</p>\n<h3 id=\"标准化公式\"><a href=\"#标准化公式\" class=\"headerlink\" title=\"标准化公式\"></a>标准化公式</h3><p>$$x = \\frac{x - \\mu}{\\sigma}$$</p>\n<p>其中，</p>\n<p>$$\\mu = \\frac{1}{m}\\sum^m_{i=1}x^{(i)}$$</p>\n<p>$$\\sigma = \\sqrt{\\frac{1}{m}\\sum^m_{i=1}{x^{(i)}}^2}$$</p>\n<p>（注意，课程上对应内容中的标准化公式疑似有误，将标准差写成了方差，此处进行修正）</p>\n<h3 id=\"使用标准化的原因\"><a href=\"#使用标准化的原因\" class=\"headerlink\" title=\"使用标准化的原因\"></a>使用标准化的原因</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/why_normalize.png\" alt=\"why_normalize\"></p>\n<p>有图可知，使用标准化前后，成本函数的形状有较大差别。</p>\n<p>在不使用标准化的成本函数中，如果设置一个较小的学习率，可能需要很多次迭代才能到达全局最优解；而如果使用了标准化，那么无论从哪个位置开始迭代，都能以相对较少的迭代次数找到全局最优解。</p>\n"},{"title":"梯度检验","date":"2018-07-20T08:31:29.000Z","mathjax":true,"_content":"## 梯度检验（Gradient checking）\n\n### 梯度的数值逼近\n\n使用双边误差的方法去逼近导数，精度要高于单边误差。\n\n* 单边误差：\n\n![one-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$\n\n误差：$O(\\varepsilon)$\n\n* 双边误差求导（即导数的定义）：\n\n![two-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$\n\n误差：$O(\\varepsilon^2)$\n\n当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。\n\n### 梯度检验的实施\n\n#### 连接参数\n\n将 $W^{[1]}$，$b^{[1]}$，...，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，\n\n$$J(W^{[1]}, b^{[1]}, ..., W^{[L]}，b^{[L]}) = J(\\theta)$$\n\n同时，对 $dW^{[1]}$，$db^{[1]}$，...，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。\n\n![dictionary_to_vector](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png)\n\n现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。\n\n#### 进行梯度检验\n\n求得一个梯度逼近值\n\n$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i+\\varepsilon, ...) - J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i-\\varepsilon, ...)}{2\\varepsilon}$$\n\n应该\n\n$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$\n\n因此，我们用梯度检验值\n\n$$\\frac{||d\\theta\\_{approx} - d\\theta||\\_2}{||d\\theta\\_{approx}||\\_2+||d\\theta||\\_2}$$\n\n检验反向传播的实施是否正确。其中，\n\n$${||x||}\\_2 = \\sum^N\\_{i=1}{|x_i|}^2$$\n\n表示向量 x 的 2-范数（也称“欧几里德范数”）。\n\n如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。\n\n### 在神经网络实施梯度检验的实用技巧和注意事项\n\n1. 不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；\n2. 如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；\n3. 当成本函数包含正则项时，也需要带上正则项进行检验；\n4. 梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；\n","source":"_posts/梯度检验.md","raw":"---\ntitle: 梯度检验\ndate: 2018-07-20 16:31:29\ntags: 优化算法\ncategories: 深度学习\nmathjax: true\n---\n## 梯度检验（Gradient checking）\n\n### 梯度的数值逼近\n\n使用双边误差的方法去逼近导数，精度要高于单边误差。\n\n* 单边误差：\n\n![one-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$\n\n误差：$O(\\varepsilon)$\n\n* 双边误差求导（即导数的定义）：\n\n![two-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png)\n\n$$f'(\\theta) = \\lim\\_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$\n\n误差：$O(\\varepsilon^2)$\n\n当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。\n\n### 梯度检验的实施\n\n#### 连接参数\n\n将 $W^{[1]}$，$b^{[1]}$，...，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，\n\n$$J(W^{[1]}, b^{[1]}, ..., W^{[L]}，b^{[L]}) = J(\\theta)$$\n\n同时，对 $dW^{[1]}$，$db^{[1]}$，...，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。\n\n![dictionary_to_vector](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png)\n\n现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。\n\n#### 进行梯度检验\n\n求得一个梯度逼近值\n\n$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i+\\varepsilon, ...) - J(\\theta\\_1, \\theta\\_2, ..., \\theta\\_i-\\varepsilon, ...)}{2\\varepsilon}$$\n\n应该\n\n$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$\n\n因此，我们用梯度检验值\n\n$$\\frac{||d\\theta\\_{approx} - d\\theta||\\_2}{||d\\theta\\_{approx}||\\_2+||d\\theta||\\_2}$$\n\n检验反向传播的实施是否正确。其中，\n\n$${||x||}\\_2 = \\sum^N\\_{i=1}{|x_i|}^2$$\n\n表示向量 x 的 2-范数（也称“欧几里德范数”）。\n\n如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。\n\n### 在神经网络实施梯度检验的实用技巧和注意事项\n\n1. 不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；\n2. 如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；\n3. 当成本函数包含正则项时，也需要带上正则项进行检验；\n4. 梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；\n","slug":"梯度检验","published":1,"updated":"2018-08-31T03:54:25.694Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh510025zkvok9x6lwgd","content":"<h2 id=\"梯度检验（Gradient-checking）\"><a href=\"#梯度检验（Gradient-checking）\" class=\"headerlink\" title=\"梯度检验（Gradient checking）\"></a>梯度检验（Gradient checking）</h2><h3 id=\"梯度的数值逼近\"><a href=\"#梯度的数值逼近\" class=\"headerlink\" title=\"梯度的数值逼近\"></a>梯度的数值逼近</h3><p>使用双边误差的方法去逼近导数，精度要高于单边误差。</p>\n<ul>\n<li>单边误差：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png\" alt=\"one-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon)$</p>\n<ul>\n<li>双边误差求导（即导数的定义）：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png\" alt=\"two-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon^2)$</p>\n<p>当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。</p>\n<h3 id=\"梯度检验的实施\"><a href=\"#梯度检验的实施\" class=\"headerlink\" title=\"梯度检验的实施\"></a>梯度检验的实施</h3><h4 id=\"连接参数\"><a href=\"#连接参数\" class=\"headerlink\" title=\"连接参数\"></a>连接参数</h4><p>将 $W^{[1]}$，$b^{[1]}$，…，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，</p>\n<p>$$J(W^{[1]}, b^{[1]}, …, W^{[L]}，b^{[L]}) = J(\\theta)$$</p>\n<p>同时，对 $dW^{[1]}$，$db^{[1]}$，…，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png\" alt=\"dictionary_to_vector\"></p>\n<p>现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。</p>\n<h4 id=\"进行梯度检验\"><a href=\"#进行梯度检验\" class=\"headerlink\" title=\"进行梯度检验\"></a>进行梯度检验</h4><p>求得一个梯度逼近值</p>\n<p>$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta_1, \\theta_2, …, \\theta_i+\\varepsilon, …) - J(\\theta_1, \\theta_2, …, \\theta_i-\\varepsilon, …)}{2\\varepsilon}$$</p>\n<p>应该</p>\n<p>$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$</p>\n<p>因此，我们用梯度检验值</p>\n<p>$$\\frac{||d\\theta_{approx} - d\\theta||_2}{||d\\theta_{approx}||_2+||d\\theta||_2}$$</p>\n<p>检验反向传播的实施是否正确。其中，</p>\n<p>$${||x||}_2 = \\sum^N_{i=1}{|x_i|}^2$$</p>\n<p>表示向量 x 的 2-范数（也称“欧几里德范数”）。</p>\n<p>如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。</p>\n<h3 id=\"在神经网络实施梯度检验的实用技巧和注意事项\"><a href=\"#在神经网络实施梯度检验的实用技巧和注意事项\" class=\"headerlink\" title=\"在神经网络实施梯度检验的实用技巧和注意事项\"></a>在神经网络实施梯度检验的实用技巧和注意事项</h3><ol>\n<li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；</li>\n<li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li>\n<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>\n<li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"梯度检验（Gradient-checking）\"><a href=\"#梯度检验（Gradient-checking）\" class=\"headerlink\" title=\"梯度检验（Gradient checking）\"></a>梯度检验（Gradient checking）</h2><h3 id=\"梯度的数值逼近\"><a href=\"#梯度的数值逼近\" class=\"headerlink\" title=\"梯度的数值逼近\"></a>梯度的数值逼近</h3><p>使用双边误差的方法去逼近导数，精度要高于单边误差。</p>\n<ul>\n<li>单边误差：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png\" alt=\"one-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta)}{\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon)$</p>\n<ul>\n<li>双边误差求导（即导数的定义）：</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png\" alt=\"two-sided-difference\"></p>\n<p>$$f’(\\theta) = \\lim_{\\varepsilon\\to 0} = \\frac{f(\\theta + \\varepsilon) - (\\theta - \\varepsilon)}{2\\varepsilon}$$</p>\n<p>误差：$O(\\varepsilon^2)$</p>\n<p>当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。</p>\n<h3 id=\"梯度检验的实施\"><a href=\"#梯度检验的实施\" class=\"headerlink\" title=\"梯度检验的实施\"></a>梯度检验的实施</h3><h4 id=\"连接参数\"><a href=\"#连接参数\" class=\"headerlink\" title=\"连接参数\"></a>连接参数</h4><p>将 $W^{[1]}$，$b^{[1]}$，…，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，</p>\n<p>$$J(W^{[1]}, b^{[1]}, …, W^{[L]}，b^{[L]}) = J(\\theta)$$</p>\n<p>同时，对 $dW^{[1]}$，$db^{[1]}$，…，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png\" alt=\"dictionary_to_vector\"></p>\n<p>现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。</p>\n<h4 id=\"进行梯度检验\"><a href=\"#进行梯度检验\" class=\"headerlink\" title=\"进行梯度检验\"></a>进行梯度检验</h4><p>求得一个梯度逼近值</p>\n<p>$$d\\theta_{approx}[i] ＝ \\frac{J(\\theta_1, \\theta_2, …, \\theta_i+\\varepsilon, …) - J(\\theta_1, \\theta_2, …, \\theta_i-\\varepsilon, …)}{2\\varepsilon}$$</p>\n<p>应该</p>\n<p>$$\\approx{d\\theta[i]} = \\frac{\\partial J}{\\partial \\theta_i}$$</p>\n<p>因此，我们用梯度检验值</p>\n<p>$$\\frac{||d\\theta_{approx} - d\\theta||_2}{||d\\theta_{approx}||_2+||d\\theta||_2}$$</p>\n<p>检验反向传播的实施是否正确。其中，</p>\n<p>$${||x||}_2 = \\sum^N_{i=1}{|x_i|}^2$$</p>\n<p>表示向量 x 的 2-范数（也称“欧几里德范数”）。</p>\n<p>如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。</p>\n<h3 id=\"在神经网络实施梯度检验的实用技巧和注意事项\"><a href=\"#在神经网络实施梯度检验的实用技巧和注意事项\" class=\"headerlink\" title=\"在神经网络实施梯度检验的实用技巧和注意事项\"></a>在神经网络实施梯度检验的实用技巧和注意事项</h3><ol>\n<li>不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；</li>\n<li>如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；</li>\n<li>当成本函数包含正则项时，也需要带上正则项进行检验；</li>\n<li>梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；</li>\n</ol>\n"},{"title":"梯度消失和梯度爆炸","date":"2018-07-20T08:25:16.000Z","categroies":"深度学习","mathjax":true,"_content":"\n## 梯度消失和梯度爆炸\n\n在梯度函数上出现的以指数级递增或者递减的情况分别称为**梯度爆炸**或者**梯度消失**。\n\n假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：\n\n$$\\hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X$$\n\n* 对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；\n* 对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。\n\n对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。\n\n### 利用初始化缓解梯度消失和爆炸\n\n根据\n\n$$z={w}_1{x}\\_1+{w}\\_2{x}\\_2 + ... + {w}\\_n{x}\\_n + b$$\n\n可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。\n\n为了得到较小的 wi，设置`Var(wi)=1/n`，这里称为 **Xavier initialization**。\n\n```python\nWL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1/n)\n```\n\n其中 n 是输入的神经元个数，即`WL.shape[1]`。\n\n这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。\n\n同理，也有 **He Initialization**。它和  Xavier initialization 唯一的区别是`Var(wi)=2/n`，适用于 **ReLU** 作为激活函数时。\n\n当激活函数使用 ReLU 时，`Var(wi)=2/n`；当激活函数使用 tanh 时，`Var(wi)=1/n`。\n","source":"_posts/梯度消失和梯度爆炸.md","raw":"---\ntitle: 梯度消失和梯度爆炸\ndate: 2018-07-20 16:25:16\ntags: 优化算法\ncategroies: 深度学习\nmathjax: true\n---\n\n## 梯度消失和梯度爆炸\n\n在梯度函数上出现的以指数级递增或者递减的情况分别称为**梯度爆炸**或者**梯度消失**。\n\n假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：\n\n$$\\hat{y} = W^{[L]}W^{[L-1]}...W^{[2]}W^{[1]}X$$\n\n* 对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；\n* 对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。\n\n对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。\n\n### 利用初始化缓解梯度消失和爆炸\n\n根据\n\n$$z={w}_1{x}\\_1+{w}\\_2{x}\\_2 + ... + {w}\\_n{x}\\_n + b$$\n\n可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。\n\n为了得到较小的 wi，设置`Var(wi)=1/n`，这里称为 **Xavier initialization**。\n\n```python\nWL = np.random.randn(WL.shape[0], WL.shape[1]) * np.sqrt(1/n)\n```\n\n其中 n 是输入的神经元个数，即`WL.shape[1]`。\n\n这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。\n\n同理，也有 **He Initialization**。它和  Xavier initialization 唯一的区别是`Var(wi)=2/n`，适用于 **ReLU** 作为激活函数时。\n\n当激活函数使用 ReLU 时，`Var(wi)=2/n`；当激活函数使用 tanh 时，`Var(wi)=1/n`。\n","slug":"梯度消失和梯度爆炸","published":1,"updated":"2018-08-31T03:54:36.141Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh560029zkvo8cwnvc14","content":"<h2 id=\"梯度消失和梯度爆炸\"><a href=\"#梯度消失和梯度爆炸\" class=\"headerlink\" title=\"梯度消失和梯度爆炸\"></a>梯度消失和梯度爆炸</h2><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸</strong>或者<strong>梯度消失</strong>。</p>\n<p>假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：</p>\n<p>$$\\hat{y} = W^{[L]}W^{[L-1]}…W^{[2]}W^{[1]}X$$</p>\n<ul>\n<li>对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；</li>\n<li>对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。</li>\n</ul>\n<p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>\n<h3 id=\"利用初始化缓解梯度消失和爆炸\"><a href=\"#利用初始化缓解梯度消失和爆炸\" class=\"headerlink\" title=\"利用初始化缓解梯度消失和爆炸\"></a>利用初始化缓解梯度消失和爆炸</h3><p>根据</p>\n<p>$$z={w}_1{x}_1+{w}_2{x}_2 + … + {w}_n{x}_n + b$$</p>\n<p>可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。</p>\n<p>为了得到较小的 wi，设置<code>Var(wi)=1/n</code>，这里称为 <strong>Xavier initialization</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WL = np.random.randn(WL.shape[<span class=\"number\">0</span>], WL.shape[<span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">1</span>/n)</span><br></pre></td></tr></table></figure>\n<p>其中 n 是输入的神经元个数，即<code>WL.shape[1]</code>。</p>\n<p>这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p>\n<p>同理，也有 <strong>He Initialization</strong>。它和  Xavier initialization 唯一的区别是<code>Var(wi)=2/n</code>，适用于 <strong>ReLU</strong> 作为激活函数时。</p>\n<p>当激活函数使用 ReLU 时，<code>Var(wi)=2/n</code>；当激活函数使用 tanh 时，<code>Var(wi)=1/n</code>。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"梯度消失和梯度爆炸\"><a href=\"#梯度消失和梯度爆炸\" class=\"headerlink\" title=\"梯度消失和梯度爆炸\"></a>梯度消失和梯度爆炸</h2><p>在梯度函数上出现的以指数级递增或者递减的情况分别称为<strong>梯度爆炸</strong>或者<strong>梯度消失</strong>。</p>\n<p>假定 $g(z) = z, b^{[l]} = 0$，对于目标输出有：</p>\n<p>$$\\hat{y} = W^{[L]}W^{[L-1]}…W^{[2]}W^{[1]}X$$</p>\n<ul>\n<li>对于 $W^{[l]}$的值大于 1 的情况，激活函数的值将以指数级递增；</li>\n<li>对于 $W^{[l]}$的值小于 1 的情况，激活函数的值将以指数级递减。</li>\n</ul>\n<p>对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。</p>\n<h3 id=\"利用初始化缓解梯度消失和爆炸\"><a href=\"#利用初始化缓解梯度消失和爆炸\" class=\"headerlink\" title=\"利用初始化缓解梯度消失和爆炸\"></a>利用初始化缓解梯度消失和爆炸</h3><p>根据</p>\n<p>$$z={w}_1{x}_1+{w}_2{x}_2 + … + {w}_n{x}_n + b$$</p>\n<p>可知，当输入的数量 n 较大时，我们希望每个 wi 的值都小一些，这样它们的和得到的 z 也较小。</p>\n<p>为了得到较小的 wi，设置<code>Var(wi)=1/n</code>，这里称为 <strong>Xavier initialization</strong>。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">WL = np.random.randn(WL.shape[<span class=\"number\">0</span>], WL.shape[<span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">1</span>/n)</span><br></pre></td></tr></table></figure>\n<p>其中 n 是输入的神经元个数，即<code>WL.shape[1]</code>。</p>\n<p>这样，激活函数的输入 x 近似设置成均值为 0，标准方差为 1，神经元输出 z 的方差就正则化到 1 了。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p>\n<p>同理，也有 <strong>He Initialization</strong>。它和  Xavier initialization 唯一的区别是<code>Var(wi)=2/n</code>，适用于 <strong>ReLU</strong> 作为激活函数时。</p>\n<p>当激活函数使用 ReLU 时，<code>Var(wi)=2/n</code>；当激活函数使用 tanh 时，<code>Var(wi)=1/n</code>。</p>\n"},{"title":"模型估计：偏差 / 方差","date":"2018-07-20T07:56:17.000Z","_content":"## 模型估计：偏差 / 方差\n\n**“偏差-方差分解”（bias-variance decomposition）**是解释学习算法泛化性能的一种重要工具。\n\n泛化误差可分解为偏差、方差与噪声之和：\n\n* **偏差**：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了**学习算法本身的拟合能力**；\n* **方差**：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动所造成的影响**；\n* **噪声**：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了**学习问题本身的难度**。\n\n偏差-方差分解说明，**泛化性能**是由**学习算法的能力**、**数据的充分性**以及**学习任务本身的难度**所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。\n\n<!-- 以上摘自周志华《机器学习》 -->\n\n在**欠拟合（underfitting）**的情况下，出现**高偏差（high bias）**的情况，即不能很好地对数据进行分类。\n\n当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现**过拟合（overfitting）**的情况，在验证集上出现**高方差（high variance）**的现象。\n\n当训练出一个模型以后，如果：\n\n* 训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；\n* 训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；\n* 训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；\n* 训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。\n\n偏差和方差的权衡问题对于模型来说十分重要。\n\n最优误差通常也称为“贝叶斯误差”。\n\n### 应对方法\n\n存在高偏差：\n\n* 扩大网络规模，如添加隐藏层或隐藏单元数目；\n* 寻找合适的网络架构，使用更大的 NN 结构；\n* 花费更长时间训练。\n\n存在高方差：\n\n* 获取更多的数据；\n* 正则化（regularization）；\n* 寻找更合适的网络结构。\n\n不断尝试，直到找到低偏差、低方差的框架。\n\n在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。\n","source":"_posts/模型估计.md","raw":"---\ntitle: 模型估计：偏差 / 方差\ndate: 2018-07-20 15:56:17\ntags: 模型估计\ncategories: 深度学习\n---\n## 模型估计：偏差 / 方差\n\n**“偏差-方差分解”（bias-variance decomposition）**是解释学习算法泛化性能的一种重要工具。\n\n泛化误差可分解为偏差、方差与噪声之和：\n\n* **偏差**：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了**学习算法本身的拟合能力**；\n* **方差**：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动所造成的影响**；\n* **噪声**：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了**学习问题本身的难度**。\n\n偏差-方差分解说明，**泛化性能**是由**学习算法的能力**、**数据的充分性**以及**学习任务本身的难度**所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。\n\n<!-- 以上摘自周志华《机器学习》 -->\n\n在**欠拟合（underfitting）**的情况下，出现**高偏差（high bias）**的情况，即不能很好地对数据进行分类。\n\n当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现**过拟合（overfitting）**的情况，在验证集上出现**高方差（high variance）**的现象。\n\n当训练出一个模型以后，如果：\n\n* 训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；\n* 训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；\n* 训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；\n* 训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。\n\n偏差和方差的权衡问题对于模型来说十分重要。\n\n最优误差通常也称为“贝叶斯误差”。\n\n### 应对方法\n\n存在高偏差：\n\n* 扩大网络规模，如添加隐藏层或隐藏单元数目；\n* 寻找合适的网络架构，使用更大的 NN 结构；\n* 花费更长时间训练。\n\n存在高方差：\n\n* 获取更多的数据；\n* 正则化（regularization）；\n* 寻找更合适的网络结构。\n\n不断尝试，直到找到低偏差、低方差的框架。\n\n在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。\n","slug":"模型估计","published":1,"updated":"2018-08-31T03:54:44.876Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh5b002dzkvoa1339jfq","content":"<h2 id=\"模型估计：偏差-方差\"><a href=\"#模型估计：偏差-方差\" class=\"headerlink\" title=\"模型估计：偏差 / 方差\"></a>模型估计：偏差 / 方差</h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>\n<p>泛化误差可分解为偏差、方差与噪声之和：</p>\n<ul>\n<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>\n<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>\n<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>\n</ul>\n<p>偏差-方差分解说明，<strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>以及<strong>学习任务本身的难度</strong>所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。</p>\n<!-- 以上摘自周志华《机器学习》 -->\n<p>在<strong>欠拟合（underfitting）</strong>的情况下，出现<strong>高偏差（high bias）</strong>的情况，即不能很好地对数据进行分类。</p>\n<p>当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现<strong>过拟合（overfitting）</strong>的情况，在验证集上出现<strong>高方差（high variance）</strong>的现象。</p>\n<p>当训练出一个模型以后，如果：</p>\n<ul>\n<li>训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；</li>\n<li>训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；</li>\n<li>训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；</li>\n<li>训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。</li>\n</ul>\n<p>偏差和方差的权衡问题对于模型来说十分重要。</p>\n<p>最优误差通常也称为“贝叶斯误差”。</p>\n<h3 id=\"应对方法\"><a href=\"#应对方法\" class=\"headerlink\" title=\"应对方法\"></a>应对方法</h3><p>存在高偏差：</p>\n<ul>\n<li>扩大网络规模，如添加隐藏层或隐藏单元数目；</li>\n<li>寻找合适的网络架构，使用更大的 NN 结构；</li>\n<li>花费更长时间训练。</li>\n</ul>\n<p>存在高方差：</p>\n<ul>\n<li>获取更多的数据；</li>\n<li>正则化（regularization）；</li>\n<li>寻找更合适的网络结构。</li>\n</ul>\n<p>不断尝试，直到找到低偏差、低方差的框架。</p>\n<p>在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"模型估计：偏差-方差\"><a href=\"#模型估计：偏差-方差\" class=\"headerlink\" title=\"模型估计：偏差 / 方差\"></a>模型估计：偏差 / 方差</h2><p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>\n<p>泛化误差可分解为偏差、方差与噪声之和：</p>\n<ul>\n<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>\n<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>\n<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>\n</ul>\n<p>偏差-方差分解说明，<strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>以及<strong>学习任务本身的难度</strong>所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。</p>\n<!-- 以上摘自周志华《机器学习》 -->\n<p>在<strong>欠拟合（underfitting）</strong>的情况下，出现<strong>高偏差（high bias）</strong>的情况，即不能很好地对数据进行分类。</p>\n<p>当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现<strong>过拟合（overfitting）</strong>的情况，在验证集上出现<strong>高方差（high variance）</strong>的现象。</p>\n<p>当训练出一个模型以后，如果：</p>\n<ul>\n<li>训练集的错误率较小，而验证集的错误率却较大，说明模型存在较大方差，可能出现了过拟合；</li>\n<li>训练集和开发集的错误率都较大，且两者相当，说明模型存在较大偏差，可能出现了欠拟合；</li>\n<li>训练集错误率较大，且开发集的错误率远较训练集大，说明方差和偏差都较大，模型很差；</li>\n<li>训练集和开发集的错误率都较小，且两者的相差也较小，说明方差和偏差都较小，这个模型效果比较好。</li>\n</ul>\n<p>偏差和方差的权衡问题对于模型来说十分重要。</p>\n<p>最优误差通常也称为“贝叶斯误差”。</p>\n<h3 id=\"应对方法\"><a href=\"#应对方法\" class=\"headerlink\" title=\"应对方法\"></a>应对方法</h3><p>存在高偏差：</p>\n<ul>\n<li>扩大网络规模，如添加隐藏层或隐藏单元数目；</li>\n<li>寻找合适的网络架构，使用更大的 NN 结构；</li>\n<li>花费更长时间训练。</li>\n</ul>\n<p>存在高方差：</p>\n<ul>\n<li>获取更多的数据；</li>\n<li>正则化（regularization）；</li>\n<li>寻找更合适的网络结构。</li>\n</ul>\n<p>不断尝试，直到找到低偏差、低方差的框架。</p>\n<p>在深度学习的早期阶段，没有太多方法能做到只减少偏差或方差而不影响到另外一方。而在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以在不增加某一方的前提下减少另一方的值。</p>\n"},{"title":"正则化（regularization）","date":"2018-07-20T07:58:06.000Z","mathjax":true,"_content":"## 正则化（regularization）\n\n**正则化**是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。\n\n### Logistic 回归中的正则化\n\n对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：\n\n$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2\\_2$$\n\n* L2 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}^2\\_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}w^2\\_j = \\frac{\\lambda}{2m}w^Tw$$\n\n* L1 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}\\_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}{|w\\_j|}$$\n\n其中，λ 为**正则化因子**，是**超参数**。\n\n由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。\n\n**注意**，`lambda`在 Python 中属于保留字，所以在编程的时候，用`lambd`代替这里的正则化因子。\n\n### 神经网络中的正则化\n\n对于神经网络，加入正则化的成本函数：\n\n$$J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$\n\n因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此\n\n$${||w^{[l]}||}^2\\_F = \\sum^{n^{[l-1]}}\\_{i=1}\\sum^{n^{[l]}}\\_{j=1}(w^{[l]}\\_{ij})^2$$\n\n```python\nL2_regularization_cost = 1./m * lambd/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\ncost = cross_entropy_cost + L2_regularization_cost\n```\n\n该矩阵范数被称为**弗罗贝尼乌斯范数（Frobenius Norm）**，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。\n\n#### 权重衰减（Weight decay）\n\n**在加入正则化项后，梯度变为**（反向传播要按这个计算）：\n\n$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$\n\n```python\ndW = 1./m * np.dot(dZ, A_prev.T) + lambd / m * W\n```\n\n代入梯度更新公式：\n\n$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$\n\n可得：\n\n$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$\n\n$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n其中，因为 $1 - \\frac{\\alpha\\lambda}{m}<1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为**权重衰减（Weight Decay）**。\n\n### 正则化可以减小过拟合的原因\n\n#### 直观解释\n\n正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，**直观上**相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。\n\n#### 数学解释\n\n假设神经元中使用的激活函数为`g(z) = tanh(z)`（sigmoid 同理）。\n\n![regularization_prevent_overfitting](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png)\n\n在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，`tanh(z)`函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。\n\n#### 其他解释\n\n在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。\n","source":"_posts/正则化.md","raw":"---\ntitle: 正则化（regularization）\ndate: 2018-07-20 15:58:06\ntags: 正则化\ncategories: 深度学习\nmathjax: true\n---\n## 正则化（regularization）\n\n**正则化**是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。\n\n### Logistic 回归中的正则化\n\n对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：\n\n$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2\\_2$$\n\n* L2 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}^2\\_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}w^2\\_j = \\frac{\\lambda}{2m}w^Tw$$\n\n* L1 正则化：\n\n$$\\frac{\\lambda}{2m}{||w||}\\_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n\\_x}{|w\\_j|}$$\n\n其中，λ 为**正则化因子**，是**超参数**。\n\n由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。\n\n**注意**，`lambda`在 Python 中属于保留字，所以在编程的时候，用`lambd`代替这里的正则化因子。\n\n### 神经网络中的正则化\n\n对于神经网络，加入正则化的成本函数：\n\n$$J(w^{[1]}, b^{[1]}, ..., w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$\n\n因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此\n\n$${||w^{[l]}||}^2\\_F = \\sum^{n^{[l-1]}}\\_{i=1}\\sum^{n^{[l]}}\\_{j=1}(w^{[l]}\\_{ij})^2$$\n\n```python\nL2_regularization_cost = 1./m * lambd/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\ncost = cross_entropy_cost + L2_regularization_cost\n```\n\n该矩阵范数被称为**弗罗贝尼乌斯范数（Frobenius Norm）**，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。\n\n#### 权重衰减（Weight decay）\n\n**在加入正则化项后，梯度变为**（反向传播要按这个计算）：\n\n$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$\n\n```python\ndW = 1./m * np.dot(dZ, A_prev.T) + lambd / m * W\n```\n\n代入梯度更新公式：\n\n$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$\n\n可得：\n\n$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$\n\n$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$\n\n其中，因为 $1 - \\frac{\\alpha\\lambda}{m}<1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为**权重衰减（Weight Decay）**。\n\n### 正则化可以减小过拟合的原因\n\n#### 直观解释\n\n正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，**直观上**相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。\n\n#### 数学解释\n\n假设神经元中使用的激活函数为`g(z) = tanh(z)`（sigmoid 同理）。\n\n![regularization_prevent_overfitting](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png)\n\n在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，`tanh(z)`函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。\n\n#### 其他解释\n\n在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。\n","slug":"正则化","published":1,"updated":"2018-08-31T03:54:49.461Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh5f002hzkvohf9e76a1","content":"<h2 id=\"正则化（regularization）\"><a href=\"#正则化（regularization）\" class=\"headerlink\" title=\"正则化（regularization）\"></a>正则化（regularization）</h2><p><strong>正则化</strong>是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。</p>\n<h3 id=\"Logistic-回归中的正则化\"><a href=\"#Logistic-回归中的正则化\" class=\"headerlink\" title=\"Logistic 回归中的正则化\"></a>Logistic 回归中的正则化</h3><p>对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：</p>\n<p>$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2_2$$</p>\n<ul>\n<li>L2 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}^2_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}w^2_j = \\frac{\\lambda}{2m}w^Tw$$</p>\n<ul>\n<li>L1 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}{|w_j|}$$</p>\n<p>其中，λ 为<strong>正则化因子</strong>，是<strong>超参数</strong>。</p>\n<p>由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。</p>\n<p><strong>注意</strong>，<code>lambda</code>在 Python 中属于保留字，所以在编程的时候，用<code>lambd</code>代替这里的正则化因子。</p>\n<h3 id=\"神经网络中的正则化\"><a href=\"#神经网络中的正则化\" class=\"headerlink\" title=\"神经网络中的正则化\"></a>神经网络中的正则化</h3><p>对于神经网络，加入正则化的成本函数：</p>\n<p>$$J(w^{[1]}, b^{[1]}, …, w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$</p>\n<p>因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此</p>\n<p>$${||w^{[l]}||}^2_F = \\sum^{n^{[l-1]}}_{i=1}\\sum^{n^{[l]}}_{j=1}(w^{[l]}_{ij})^2$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">L2_regularization_cost = <span class=\"number\">1.</span>/m * lambd/<span class=\"number\">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))</span><br><span class=\"line\">cost = cross_entropy_cost + L2_regularization_cost</span><br></pre></td></tr></table></figure>\n<p>该矩阵范数被称为<strong>弗罗贝尼乌斯范数（Frobenius Norm）</strong>，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。</p>\n<h4 id=\"权重衰减（Weight-decay）\"><a href=\"#权重衰减（Weight-decay）\" class=\"headerlink\" title=\"权重衰减（Weight decay）\"></a>权重衰减（Weight decay）</h4><p><strong>在加入正则化项后，梯度变为</strong>（反向传播要按这个计算）：</p>\n<p>$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dW = <span class=\"number\">1.</span>/m * np.dot(dZ, A_prev.T) + lambd / m * W</span><br></pre></td></tr></table></figure>\n<p>代入梯度更新公式：</p>\n<p>$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$</p>\n<p>可得：</p>\n<p>$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$</p>\n<p>$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>其中，因为 $1 - \\frac{\\alpha\\lambda}{m}&lt;1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为<strong>权重衰减（Weight Decay）</strong>。</p>\n<h3 id=\"正则化可以减小过拟合的原因\"><a href=\"#正则化可以减小过拟合的原因\" class=\"headerlink\" title=\"正则化可以减小过拟合的原因\"></a>正则化可以减小过拟合的原因</h3><h4 id=\"直观解释\"><a href=\"#直观解释\" class=\"headerlink\" title=\"直观解释\"></a>直观解释</h4><p>正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，<strong>直观上</strong>相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。</p>\n<h4 id=\"数学解释\"><a href=\"#数学解释\" class=\"headerlink\" title=\"数学解释\"></a>数学解释</h4><p>假设神经元中使用的激活函数为<code>g(z) = tanh(z)</code>（sigmoid 同理）。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png\" alt=\"regularization_prevent_overfitting\"></p>\n<p>在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，<code>tanh(z)</code>函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。</p>\n<h4 id=\"其他解释\"><a href=\"#其他解释\" class=\"headerlink\" title=\"其他解释\"></a>其他解释</h4><p>在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"正则化（regularization）\"><a href=\"#正则化（regularization）\" class=\"headerlink\" title=\"正则化（regularization）\"></a>正则化（regularization）</h2><p><strong>正则化</strong>是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。</p>\n<h3 id=\"Logistic-回归中的正则化\"><a href=\"#Logistic-回归中的正则化\" class=\"headerlink\" title=\"Logistic 回归中的正则化\"></a>Logistic 回归中的正则化</h3><p>对于 Logistic 回归，加入 L2 正则化（也称“L2 范数”）的成本函数：</p>\n<p>$$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}{||w||}^2_2$$</p>\n<ul>\n<li>L2 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}^2_2 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}w^2_j = \\frac{\\lambda}{2m}w^Tw$$</p>\n<ul>\n<li>L1 正则化：</li>\n</ul>\n<p>$$\\frac{\\lambda}{2m}{||w||}_1 = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}{|w_j|}$$</p>\n<p>其中，λ 为<strong>正则化因子</strong>，是<strong>超参数</strong>。</p>\n<p>由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。</p>\n<p><strong>注意</strong>，<code>lambda</code>在 Python 中属于保留字，所以在编程的时候，用<code>lambd</code>代替这里的正则化因子。</p>\n<h3 id=\"神经网络中的正则化\"><a href=\"#神经网络中的正则化\" class=\"headerlink\" title=\"神经网络中的正则化\"></a>神经网络中的正则化</h3><p>对于神经网络，加入正则化的成本函数：</p>\n<p>$$J(w^{[1]}, b^{[1]}, …, w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})+\\frac{\\lambda}{2m}\\sum_{l=1}^L{||w^{[l]}||}^2_F$$</p>\n<p>因为 w 的大小为 ($n^{[l−1]}$, $n^{[l]}$)，因此</p>\n<p>$${||w^{[l]}||}^2_F = \\sum^{n^{[l-1]}}_{i=1}\\sum^{n^{[l]}}_{j=1}(w^{[l]}_{ij})^2$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">L2_regularization_cost = <span class=\"number\">1.</span>/m * lambd/<span class=\"number\">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))</span><br><span class=\"line\">cost = cross_entropy_cost + L2_regularization_cost</span><br></pre></td></tr></table></figure>\n<p>该矩阵范数被称为<strong>弗罗贝尼乌斯范数（Frobenius Norm）</strong>，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。</p>\n<h4 id=\"权重衰减（Weight-decay）\"><a href=\"#权重衰减（Weight-decay）\" class=\"headerlink\" title=\"权重衰减（Weight decay）\"></a>权重衰减（Weight decay）</h4><p><strong>在加入正则化项后，梯度变为</strong>（反向传播要按这个计算）：</p>\n<p>$$dW^{[l]}= \\frac{\\partial L}{\\partial w^{[l]}} +\\frac{\\lambda}{m}W^{[l]}$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dW = <span class=\"number\">1.</span>/m * np.dot(dZ, A_prev.T) + lambd / m * W</span><br></pre></td></tr></table></figure>\n<p>代入梯度更新公式：</p>\n<p>$$W^{[l]} := W^{[l]}-\\alpha dW^{[l]}$$</p>\n<p>可得：</p>\n<p>$$W^{[l]} := W^{[l]} - \\alpha [\\frac{\\partial L}{\\partial w^{[l]}} + \\frac{\\lambda}{m}W^{[l]}]$$</p>\n<p>$$= W^{[l]} - \\alpha \\frac{\\lambda}{m}W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>$$= (1 - \\frac{\\alpha\\lambda}{m})W^{[l]} - \\alpha \\frac{\\partial L}{\\partial w^{[l]}}$$</p>\n<p>其中，因为 $1 - \\frac{\\alpha\\lambda}{m}&lt;1$，会给原来的 $W^{[l]}$一个衰减的参数，因此 L2 正则化项也被称为<strong>权重衰减（Weight Decay）</strong>。</p>\n<h3 id=\"正则化可以减小过拟合的原因\"><a href=\"#正则化可以减小过拟合的原因\" class=\"headerlink\" title=\"正则化可以减小过拟合的原因\"></a>正则化可以减小过拟合的原因</h3><h4 id=\"直观解释\"><a href=\"#直观解释\" class=\"headerlink\" title=\"直观解释\"></a>直观解释</h4><p>正则化因子设置的足够大的情况下，为了使成本函数最小化，权重矩阵 W 就会被设置为接近于 0 的值，<strong>直观上</strong>相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。</p>\n<h4 id=\"数学解释\"><a href=\"#数学解释\" class=\"headerlink\" title=\"数学解释\"></a>数学解释</h4><p>假设神经元中使用的激活函数为<code>g(z) = tanh(z)</code>（sigmoid 同理）。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/regularization_prevent_overfitting.png\" alt=\"regularization_prevent_overfitting\"></p>\n<p>在加入正则化项后，当 λ  增大，导致 $W^{[l]}$减小，$Z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$便会减小。由上图可知，在 z 较小（接近于 0）的区域里，<code>tanh(z)</code>函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。</p>\n<h4 id=\"其他解释\"><a href=\"#其他解释\" class=\"headerlink\" title=\"其他解释\"></a>其他解释</h4><p>在权值 $w^{[L]}$变小之下，输入样本 X 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。</p>\n"},{"title":"深度卷积神经网络:实例探究","date":"2018-08-26T09:33:47.000Z","mathjax":true,"_content":"## 经典卷积网络\n\n### LeNet-5\n\n![LeNet-5](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/LeNet-5.png)\n\n特点：\n\n* LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。\n* 该模型总共包含了约 6 万个参数，远少于标准神经网络所需。\n* 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer->POOL layer->CONV layer->POOL layer->FC layer->FC layer->OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。\n* 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。\n\n相关论文：[LeCun et.al., 1998. Gradient-based learning applied to document recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791&tag=1)。\n\n### AlexNet\n\n![AlexNet](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/AlexNet.png)\n\n特点：\n\n* AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。\n* 当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。\n\n相关论文：[Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)。这是一篇易于理解并且影响巨大的论文，计算机视觉群体自此开始重视深度学习。\n\n### VGG\n\n![VGG](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/VGG.png)\n\n特点：\n\n* VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。\n* 超参数较少，只需要专注于构建卷积层。\n* 结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。\n* VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。\n\n相关论文：[Simonvan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556.pdf)。\n\n## 残差网络\n\n因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。**残差网络（Residual Networks，简称为 ResNets）** 可以有效解决这个问题。\n\n![Residual-block](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-block.jpg)\n\n上图的结构被称为 **残差块（Residual block**。 通过 **捷径（Short cut，或者称跳远连接，Skip connections）** 可以将 $a^{[l]}$ 添加到第二个 ReLU 过程中，直接建立 $a^{[l]}$ 与 $a^{[l+2]}$ 之间的隔层联系。表达式如下：\n\n$$z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$$\n\n$$a^{[l+1]} = g(z^{[l+1]})$$\n\n$$z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$$\n\n$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$\n\n构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。\n\n![Residual-Network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-Network.jpg)\n\n为了便于区分，在 ResNets 的论文[He et al., 2015. Deep residual networks for image recognition](https://arxiv.org/pdf/1512.03385.pdf)中，非残差网络被称为 **普通网络（Plain Network）**。 将它变为残差网络的方法是加上所有的跳远连接。\n\n在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。\n\n![ResNet-Training-Error](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Training-Error.jpg)\n\n残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。\n\n### 残差网络有效的原因\n\n假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。\n\n![Why-do-residual-networks-work](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Why-do-residual-networks-work.jpg)\n\n则有：\n\n$$\n\\begin{equation}\n\\begin{split}\n a^{[l+2]} &= g(z^{[l+2]}+a^{[l]})\n     \\\\\\ &= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})\n\\end{split}\n\\end{equation}\n$$\n\n当发生梯度消失时，$W^{[l+2]}\\approx0$，$b^{[l+2]}\\approx0$，则有：\n\n$$a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}$$\n\n因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。\n\n注意，如果 $a^{[l]}$ 与 $a^{[l+2]}$ 的维度不同，需要引入矩阵 $W\\_s$ 与 $a^{[l]}$ 相乘，使得二者的维度相匹配。参数矩阵 $W\\_s$ 既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$ 截断或者补零。\n\n![ResNet-Paper](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Paper.png)\n\n上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。\n\n## 1x1 卷积\n\n1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。\n\n![1x1-Conv-1](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-1.png)\n\n而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。\n\n池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。\n\n![1x1-Conv-2](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-2.png)\n\n虽然论文[Lin et al., 2013. Network in network](https://arxiv.org/pdf/1312.4400.pdf)中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。\n\n## Inception 网络\n\n在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 **Inception 网络的作用**即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。\n\n![Motivation-for-inception-network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Motivation-for-inception-network.jpg)\n\n如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。\n\n相关论文：[Szegedy et al., 2014, Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf)\n\n### 计算成本问题\n\n在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：\n\n![The-problem-of-computational-cost](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/The-problem-of-computational-cost.png)\n\n图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。\n\n为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。\n\n![Using-1x1-convolution](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Using-1x1-convolution.png)\n\n对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作**瓶颈层（Bottleneck layer）**。\n\n改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。\n\n只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。\n\n### 完整的 Inception 网络\n\n![Inception-module](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-module.jpg)\n\n上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。\n\n多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：\n\n![Inception-network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-network.jpg)\n\n注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。\n\n经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。\n\n## 使用开源的实现方案\n\n很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。\n\n## 迁移学习\n\n在“搭建机器学习项目”课程中，[迁移学习](http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Structuring_Machine_Learning_Projects/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89?id=%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0)已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做**预训练**，然后转换到自己感兴趣的任务上，有助于加速开发。\n\n对于已训练好的卷积神经网络，可以将所有层都看作是**冻结的**，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。\n\n而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。\n\n上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。\n\n## 数据扩增\n\n计算机视觉领域的应用都需要大量的数据。当数据不够时，**数据扩增（Data Augmentation）**就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。\n\n其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，**PCA 颜色增强**指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。具体的 PCA 颜色增强做法可以查阅 AlexNet 的相关论文或者开源代码。\n\n在构建大型神经网络的时候，数据扩增和模型训练可以由两个或多个不同的线程并行来实现。\n\n## 计算机视觉现状\n\n通常，学习算法有两种知识来源：\n\n* 被标记的数据\n* 手工工程\n\n**手工工程（Hand-engineering，又称 hacks）** 指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。\n\n另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：\n\n* 集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出\n* Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均\n\n但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。\n","source":"_posts/深度卷积神经网络-实例探究.md","raw":"---\ntitle: 深度卷积神经网络:实例探究\ndate: 2018-08-26 17:33:47\ntags: CNN\ncategories: 深度学习\nmathjax: true\n---\n## 经典卷积网络\n\n### LeNet-5\n\n![LeNet-5](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/LeNet-5.png)\n\n特点：\n\n* LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。\n* 该模型总共包含了约 6 万个参数，远少于标准神经网络所需。\n* 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer->POOL layer->CONV layer->POOL layer->FC layer->FC layer->OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。\n* 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。\n\n相关论文：[LeCun et.al., 1998. Gradient-based learning applied to document recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791&tag=1)。\n\n### AlexNet\n\n![AlexNet](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/AlexNet.png)\n\n特点：\n\n* AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。\n* 当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。\n\n相关论文：[Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)。这是一篇易于理解并且影响巨大的论文，计算机视觉群体自此开始重视深度学习。\n\n### VGG\n\n![VGG](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/VGG.png)\n\n特点：\n\n* VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。\n* 超参数较少，只需要专注于构建卷积层。\n* 结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。\n* VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。\n\n相关论文：[Simonvan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition](https://arxiv.org/pdf/1409.1556.pdf)。\n\n## 残差网络\n\n因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。**残差网络（Residual Networks，简称为 ResNets）** 可以有效解决这个问题。\n\n![Residual-block](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-block.jpg)\n\n上图的结构被称为 **残差块（Residual block**。 通过 **捷径（Short cut，或者称跳远连接，Skip connections）** 可以将 $a^{[l]}$ 添加到第二个 ReLU 过程中，直接建立 $a^{[l]}$ 与 $a^{[l+2]}$ 之间的隔层联系。表达式如下：\n\n$$z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$$\n\n$$a^{[l+1]} = g(z^{[l+1]})$$\n\n$$z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$$\n\n$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$\n\n构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。\n\n![Residual-Network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-Network.jpg)\n\n为了便于区分，在 ResNets 的论文[He et al., 2015. Deep residual networks for image recognition](https://arxiv.org/pdf/1512.03385.pdf)中，非残差网络被称为 **普通网络（Plain Network）**。 将它变为残差网络的方法是加上所有的跳远连接。\n\n在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。\n\n![ResNet-Training-Error](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Training-Error.jpg)\n\n残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。\n\n### 残差网络有效的原因\n\n假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。\n\n![Why-do-residual-networks-work](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Why-do-residual-networks-work.jpg)\n\n则有：\n\n$$\n\\begin{equation}\n\\begin{split}\n a^{[l+2]} &= g(z^{[l+2]}+a^{[l]})\n     \\\\\\ &= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})\n\\end{split}\n\\end{equation}\n$$\n\n当发生梯度消失时，$W^{[l+2]}\\approx0$，$b^{[l+2]}\\approx0$，则有：\n\n$$a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}$$\n\n因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。\n\n注意，如果 $a^{[l]}$ 与 $a^{[l+2]}$ 的维度不同，需要引入矩阵 $W\\_s$ 与 $a^{[l]}$ 相乘，使得二者的维度相匹配。参数矩阵 $W\\_s$ 既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$ 截断或者补零。\n\n![ResNet-Paper](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Paper.png)\n\n上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。\n\n## 1x1 卷积\n\n1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。\n\n![1x1-Conv-1](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-1.png)\n\n而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。\n\n池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。\n\n![1x1-Conv-2](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-2.png)\n\n虽然论文[Lin et al., 2013. Network in network](https://arxiv.org/pdf/1312.4400.pdf)中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。\n\n## Inception 网络\n\n在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 **Inception 网络的作用**即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。\n\n![Motivation-for-inception-network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Motivation-for-inception-network.jpg)\n\n如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。\n\n相关论文：[Szegedy et al., 2014, Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf)\n\n### 计算成本问题\n\n在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：\n\n![The-problem-of-computational-cost](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/The-problem-of-computational-cost.png)\n\n图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。\n\n为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。\n\n![Using-1x1-convolution](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Using-1x1-convolution.png)\n\n对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作**瓶颈层（Bottleneck layer）**。\n\n改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。\n\n只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。\n\n### 完整的 Inception 网络\n\n![Inception-module](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-module.jpg)\n\n上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。\n\n多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：\n\n![Inception-network](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-network.jpg)\n\n注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。\n\n经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。\n\n## 使用开源的实现方案\n\n很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。\n\n## 迁移学习\n\n在“搭建机器学习项目”课程中，[迁移学习](http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Structuring_Machine_Learning_Projects/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89?id=%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0)已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做**预训练**，然后转换到自己感兴趣的任务上，有助于加速开发。\n\n对于已训练好的卷积神经网络，可以将所有层都看作是**冻结的**，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。\n\n而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。\n\n上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。\n\n## 数据扩增\n\n计算机视觉领域的应用都需要大量的数据。当数据不够时，**数据扩增（Data Augmentation）**就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。\n\n其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，**PCA 颜色增强**指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。具体的 PCA 颜色增强做法可以查阅 AlexNet 的相关论文或者开源代码。\n\n在构建大型神经网络的时候，数据扩增和模型训练可以由两个或多个不同的线程并行来实现。\n\n## 计算机视觉现状\n\n通常，学习算法有两种知识来源：\n\n* 被标记的数据\n* 手工工程\n\n**手工工程（Hand-engineering，又称 hacks）** 指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。\n\n另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：\n\n* 集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出\n* Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均\n\n但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。\n","slug":"深度卷积神经网络-实例探究","published":1,"updated":"2018-08-26T09:48:38.084Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh5j002lzkvowgp5nb6k","content":"<h2 id=\"经典卷积网络\"><a href=\"#经典卷积网络\" class=\"headerlink\" title=\"经典卷积网络\"></a>经典卷积网络</h2><h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/LeNet-5.png\" alt=\"LeNet-5\"></p>\n<p>特点：</p>\n<ul>\n<li>LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。</li>\n<li>该模型总共包含了约 6 万个参数，远少于标准神经网络所需。</li>\n<li>典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。</li>\n<li>当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。</li>\n</ul>\n<p>相关论文：<a href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791&amp;tag=1\" target=\"_blank\" rel=\"noopener\">LeCun et.al., 1998. Gradient-based learning applied to document recognition</a>。</p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/AlexNet.png\" alt=\"AlexNet\"></p>\n<p>特点：</p>\n<ul>\n<li>AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。</li>\n<li>当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。</li>\n</ul>\n<p>相关论文：<a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks</a>。这是一篇易于理解并且影响巨大的论文，计算机视觉群体自此开始重视深度学习。</p>\n<h3 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/VGG.png\" alt=\"VGG\"></p>\n<p>特点：</p>\n<ul>\n<li>VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。</li>\n<li>超参数较少，只需要专注于构建卷积层。</li>\n<li>结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。</li>\n<li>VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。</li>\n</ul>\n<p>相关论文：<a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"noopener\">Simonvan &amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition</a>。</p>\n<h2 id=\"残差网络\"><a href=\"#残差网络\" class=\"headerlink\" title=\"残差网络\"></a>残差网络</h2><p>因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。<strong>残差网络（Residual Networks，简称为 ResNets）</strong> 可以有效解决这个问题。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-block.jpg\" alt=\"Residual-block\"></p>\n<p>上图的结构被称为 <strong>残差块（Residual block</strong>。 通过 <strong>捷径（Short cut，或者称跳远连接，Skip connections）</strong> 可以将 $a^{[l]}$ 添加到第二个 ReLU 过程中，直接建立 $a^{[l]}$ 与 $a^{[l+2]}$ 之间的隔层联系。表达式如下：</p>\n<p>$$z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$$</p>\n<p>$$a^{[l+1]} = g(z^{[l+1]})$$</p>\n<p>$$z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$$</p>\n<p>$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$</p>\n<p>构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-Network.jpg\" alt=\"Residual-Network\"></p>\n<p>为了便于区分，在 ResNets 的论文<a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"noopener\">He et al., 2015. Deep residual networks for image recognition</a>中，非残差网络被称为 <strong>普通网络（Plain Network）</strong>。 将它变为残差网络的方法是加上所有的跳远连接。</p>\n<p>在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Training-Error.jpg\" alt=\"ResNet-Training-Error\"></p>\n<p>残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。</p>\n<h3 id=\"残差网络有效的原因\"><a href=\"#残差网络有效的原因\" class=\"headerlink\" title=\"残差网络有效的原因\"></a>残差网络有效的原因</h3><p>假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Why-do-residual-networks-work.jpg\" alt=\"Why-do-residual-networks-work\"></p>\n<p>则有：</p>\n<p>$$<br>\\begin{equation}<br>\\begin{split}<br> a^{[l+2]} &amp;= g(z^{[l+2]}+a^{[l]})<br>     \\\\ &amp;= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})<br>\\end{split}<br>\\end{equation}<br>$$</p>\n<p>当发生梯度消失时，$W^{[l+2]}\\approx0$，$b^{[l+2]}\\approx0$，则有：</p>\n<p>$$a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}$$</p>\n<p>因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。</p>\n<p>注意，如果 $a^{[l]}$ 与 $a^{[l+2]}$ 的维度不同，需要引入矩阵 $W_s$ 与 $a^{[l]}$ 相乘，使得二者的维度相匹配。参数矩阵 $W_s$ 既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$ 截断或者补零。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Paper.png\" alt=\"ResNet-Paper\"></p>\n<p>上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。</p>\n<h2 id=\"1x1-卷积\"><a href=\"#1x1-卷积\" class=\"headerlink\" title=\"1x1 卷积\"></a>1x1 卷积</h2><p>1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-1.png\" alt=\"1x1-Conv-1\"></p>\n<p>而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。</p>\n<p>池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-2.png\" alt=\"1x1-Conv-2\"></p>\n<p>虽然论文<a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"noopener\">Lin et al., 2013. Network in network</a>中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。</p>\n<h2 id=\"Inception-网络\"><a href=\"#Inception-网络\" class=\"headerlink\" title=\"Inception 网络\"></a>Inception 网络</h2><p>在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 <strong>Inception 网络的作用</strong>即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Motivation-for-inception-network.jpg\" alt=\"Motivation-for-inception-network\"></p>\n<p>如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。</p>\n<p>相关论文：<a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"noopener\">Szegedy et al., 2014, Going Deeper with Convolutions</a></p>\n<h3 id=\"计算成本问题\"><a href=\"#计算成本问题\" class=\"headerlink\" title=\"计算成本问题\"></a>计算成本问题</h3><p>在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/The-problem-of-computational-cost.png\" alt=\"The-problem-of-computational-cost\"></p>\n<p>图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。</p>\n<p>为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Using-1x1-convolution.png\" alt=\"Using-1x1-convolution\"></p>\n<p>对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作<strong>瓶颈层（Bottleneck layer）</strong>。</p>\n<p>改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。</p>\n<p>只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。</p>\n<h3 id=\"完整的-Inception-网络\"><a href=\"#完整的-Inception-网络\" class=\"headerlink\" title=\"完整的 Inception 网络\"></a>完整的 Inception 网络</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-module.jpg\" alt=\"Inception-module\"></p>\n<p>上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。</p>\n<p>多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-network.jpg\" alt=\"Inception-network\"></p>\n<p>注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。</p>\n<p>经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。</p>\n<h2 id=\"使用开源的实现方案\"><a href=\"#使用开源的实现方案\" class=\"headerlink\" title=\"使用开源的实现方案\"></a>使用开源的实现方案</h2><p>很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。</p>\n<h2 id=\"迁移学习\"><a href=\"#迁移学习\" class=\"headerlink\" title=\"迁移学习\"></a>迁移学习</h2><p>在“搭建机器学习项目”课程中，<a href=\"http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Structuring_Machine_Learning_Projects/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89?id=%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0\" target=\"_blank\" rel=\"noopener\">迁移学习</a>已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做<strong>预训练</strong>，然后转换到自己感兴趣的任务上，有助于加速开发。</p>\n<p>对于已训练好的卷积神经网络，可以将所有层都看作是<strong>冻结的</strong>，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。</p>\n<p>而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。</p>\n<p>上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。</p>\n<h2 id=\"数据扩增\"><a href=\"#数据扩增\" class=\"headerlink\" title=\"数据扩增\"></a>数据扩增</h2><p>计算机视觉领域的应用都需要大量的数据。当数据不够时，<strong>数据扩增（Data Augmentation）</strong>就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。</p>\n<p>其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，<strong>PCA 颜色增强</strong>指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。具体的 PCA 颜色增强做法可以查阅 AlexNet 的相关论文或者开源代码。</p>\n<p>在构建大型神经网络的时候，数据扩增和模型训练可以由两个或多个不同的线程并行来实现。</p>\n<h2 id=\"计算机视觉现状\"><a href=\"#计算机视觉现状\" class=\"headerlink\" title=\"计算机视觉现状\"></a>计算机视觉现状</h2><p>通常，学习算法有两种知识来源：</p>\n<ul>\n<li>被标记的数据</li>\n<li>手工工程</li>\n</ul>\n<p><strong>手工工程（Hand-engineering，又称 hacks）</strong> 指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。</p>\n<p>另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：</p>\n<ul>\n<li>集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出</li>\n<li>Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均</li>\n</ul>\n<p>但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"经典卷积网络\"><a href=\"#经典卷积网络\" class=\"headerlink\" title=\"经典卷积网络\"></a>经典卷积网络</h2><h3 id=\"LeNet-5\"><a href=\"#LeNet-5\" class=\"headerlink\" title=\"LeNet-5\"></a>LeNet-5</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/LeNet-5.png\" alt=\"LeNet-5\"></p>\n<p>特点：</p>\n<ul>\n<li>LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。</li>\n<li>该模型总共包含了约 6 万个参数，远少于标准神经网络所需。</li>\n<li>典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。</li>\n<li>当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。</li>\n</ul>\n<p>相关论文：<a href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=726791&amp;tag=1\" target=\"_blank\" rel=\"noopener\">LeCun et.al., 1998. Gradient-based learning applied to document recognition</a>。</p>\n<h3 id=\"AlexNet\"><a href=\"#AlexNet\" class=\"headerlink\" title=\"AlexNet\"></a>AlexNet</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/AlexNet.png\" alt=\"AlexNet\"></p>\n<p>特点：</p>\n<ul>\n<li>AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。</li>\n<li>当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。</li>\n</ul>\n<p>相关论文：<a href=\"http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" target=\"_blank\" rel=\"noopener\">Krizhevsky et al.,2012. ImageNet classification with deep convolutional neural networks</a>。这是一篇易于理解并且影响巨大的论文，计算机视觉群体自此开始重视深度学习。</p>\n<h3 id=\"VGG\"><a href=\"#VGG\" class=\"headerlink\" title=\"VGG\"></a>VGG</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/VGG.png\" alt=\"VGG\"></p>\n<p>特点：</p>\n<ul>\n<li>VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。</li>\n<li>超参数较少，只需要专注于构建卷积层。</li>\n<li>结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。</li>\n<li>VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。</li>\n</ul>\n<p>相关论文：<a href=\"https://arxiv.org/pdf/1409.1556.pdf\" target=\"_blank\" rel=\"noopener\">Simonvan &amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition</a>。</p>\n<h2 id=\"残差网络\"><a href=\"#残差网络\" class=\"headerlink\" title=\"残差网络\"></a>残差网络</h2><p>因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。<strong>残差网络（Residual Networks，简称为 ResNets）</strong> 可以有效解决这个问题。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-block.jpg\" alt=\"Residual-block\"></p>\n<p>上图的结构被称为 <strong>残差块（Residual block</strong>。 通过 <strong>捷径（Short cut，或者称跳远连接，Skip connections）</strong> 可以将 $a^{[l]}$ 添加到第二个 ReLU 过程中，直接建立 $a^{[l]}$ 与 $a^{[l+2]}$ 之间的隔层联系。表达式如下：</p>\n<p>$$z^{[l+1]} = W^{[l+1]}a^{[l]} + b^{[l+1]}$$</p>\n<p>$$a^{[l+1]} = g(z^{[l+1]})$$</p>\n<p>$$z^{[l+2]} = W^{[l+2]}a^{[l+1]} + b^{[l+2]}$$</p>\n<p>$$a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$$</p>\n<p>构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Residual-Network.jpg\" alt=\"Residual-Network\"></p>\n<p>为了便于区分，在 ResNets 的论文<a href=\"https://arxiv.org/pdf/1512.03385.pdf\" target=\"_blank\" rel=\"noopener\">He et al., 2015. Deep residual networks for image recognition</a>中，非残差网络被称为 <strong>普通网络（Plain Network）</strong>。 将它变为残差网络的方法是加上所有的跳远连接。</p>\n<p>在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Training-Error.jpg\" alt=\"ResNet-Training-Error\"></p>\n<p>残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。</p>\n<h3 id=\"残差网络有效的原因\"><a href=\"#残差网络有效的原因\" class=\"headerlink\" title=\"残差网络有效的原因\"></a>残差网络有效的原因</h3><p>假设有一个大型神经网络，其输入为 $X$，输出为 $a^{[l]}$。给这个神经网络额外增加两层，输出为 $a^{[l+2]}$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Why-do-residual-networks-work.jpg\" alt=\"Why-do-residual-networks-work\"></p>\n<p>则有：</p>\n<p>$$<br>\\begin{equation}<br>\\begin{split}<br> a^{[l+2]} &amp;= g(z^{[l+2]}+a^{[l]})<br>     \\\\ &amp;= g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})<br>\\end{split}<br>\\end{equation}<br>$$</p>\n<p>当发生梯度消失时，$W^{[l+2]}\\approx0$，$b^{[l+2]}\\approx0$，则有：</p>\n<p>$$a^{[l+2]} = g(a^{[l]}) = ReLU(a^{[l]}) = a^{[l]}$$</p>\n<p>因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。</p>\n<p>注意，如果 $a^{[l]}$ 与 $a^{[l+2]}$ 的维度不同，需要引入矩阵 $W_s$ 与 $a^{[l]}$ 相乘，使得二者的维度相匹配。参数矩阵 $W_s$ 既可以通过模型训练得到，也可以作为固定值，仅使 $a^{[l]}$ 截断或者补零。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/ResNet-Paper.png\" alt=\"ResNet-Paper\"></p>\n<p>上图是论文提供的 CNN 中 ResNet 的一个典型结构。卷积层通常使用 Same 卷积以保持维度相同，而不同类型层之间的连接（例如卷积层和池化层），如果维度不同，则需要引入矩阵 $W_s$。</p>\n<h2 id=\"1x1-卷积\"><a href=\"#1x1-卷积\" class=\"headerlink\" title=\"1x1 卷积\"></a>1x1 卷积</h2><p>1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-1.png\" alt=\"1x1-Conv-1\"></p>\n<p>而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。</p>\n<p>池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/1x1-Conv-2.png\" alt=\"1x1-Conv-2\"></p>\n<p>虽然论文<a href=\"https://arxiv.org/pdf/1312.4400.pdf\" target=\"_blank\" rel=\"noopener\">Lin et al., 2013. Network in network</a>中关于架构的详细内容并没有得到广泛应用，但是 1x1 卷积的理念十分有影响力，许多神经网络架构（包括 Inception 网络）都受到它的影响。</p>\n<h2 id=\"Inception-网络\"><a href=\"#Inception-网络\" class=\"headerlink\" title=\"Inception 网络\"></a>Inception 网络</h2><p>在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 <strong>Inception 网络的作用</strong>即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Motivation-for-inception-network.jpg\" alt=\"Motivation-for-inception-network\"></p>\n<p>如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。</p>\n<p>相关论文：<a href=\"https://arxiv.org/pdf/1409.4842.pdf\" target=\"_blank\" rel=\"noopener\">Szegedy et al., 2014, Going Deeper with Convolutions</a></p>\n<h3 id=\"计算成本问题\"><a href=\"#计算成本问题\" class=\"headerlink\" title=\"计算成本问题\"></a>计算成本问题</h3><p>在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/The-problem-of-computational-cost.png\" alt=\"The-problem-of-computational-cost\"></p>\n<p>图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。</p>\n<p>为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Using-1x1-convolution.png\" alt=\"Using-1x1-convolution\"></p>\n<p>对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作<strong>瓶颈层（Bottleneck layer）</strong>。</p>\n<p>改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。</p>\n<p>只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。</p>\n<h3 id=\"完整的-Inception-网络\"><a href=\"#完整的-Inception-网络\" class=\"headerlink\" title=\"完整的 Inception 网络\"></a>完整的 Inception 网络</h3><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-module.jpg\" alt=\"Inception-module\"></p>\n<p>上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。</p>\n<p>多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Convolutional_Neural_Networks/Inception-network.jpg\" alt=\"Inception-network\"></p>\n<p>注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。</p>\n<p>经过研究者们的不断发展，Inception 模型的 V2、V3、V4 以及引入残差网络的版本被提出，这些变体都基于 Inception V1 版本的基础思想上。顺便一提，Inception 模型的名字来自电影《盗梦空间》。</p>\n<h2 id=\"使用开源的实现方案\"><a href=\"#使用开源的实现方案\" class=\"headerlink\" title=\"使用开源的实现方案\"></a>使用开源的实现方案</h2><p>很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。</p>\n<h2 id=\"迁移学习\"><a href=\"#迁移学习\" class=\"headerlink\" title=\"迁移学习\"></a>迁移学习</h2><p>在“搭建机器学习项目”课程中，<a href=\"http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Structuring_Machine_Learning_Projects/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88ML%EF%BC%89%E7%AD%96%E7%95%A5%EF%BC%882%EF%BC%89?id=%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0\" target=\"_blank\" rel=\"noopener\">迁移学习</a>已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做<strong>预训练</strong>，然后转换到自己感兴趣的任务上，有助于加速开发。</p>\n<p>对于已训练好的卷积神经网络，可以将所有层都看作是<strong>冻结的</strong>，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。</p>\n<p>而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。</p>\n<p>上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。</p>\n<h2 id=\"数据扩增\"><a href=\"#数据扩增\" class=\"headerlink\" title=\"数据扩增\"></a>数据扩增</h2><p>计算机视觉领域的应用都需要大量的数据。当数据不够时，<strong>数据扩增（Data Augmentation）</strong>就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。</p>\n<p>其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，<strong>PCA 颜色增强</strong>指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。具体的 PCA 颜色增强做法可以查阅 AlexNet 的相关论文或者开源代码。</p>\n<p>在构建大型神经网络的时候，数据扩增和模型训练可以由两个或多个不同的线程并行来实现。</p>\n<h2 id=\"计算机视觉现状\"><a href=\"#计算机视觉现状\" class=\"headerlink\" title=\"计算机视觉现状\"></a>计算机视觉现状</h2><p>通常，学习算法有两种知识来源：</p>\n<ul>\n<li>被标记的数据</li>\n<li>手工工程</li>\n</ul>\n<p><strong>手工工程（Hand-engineering，又称 hacks）</strong> 指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。</p>\n<p>另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：</p>\n<ul>\n<li>集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出</li>\n<li>Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均</li>\n</ul>\n<p>但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。</p>\n"},{"title":"深度学习中的优化算法","date":"2018-08-04T05:26:01.000Z","mathjax":true,"_content":"深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。\n\n## batch 梯度下降法\n\n**batch 梯度下降法**（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。\n\n对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。\n\n但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 **mini-batch**。\n\n## Mini-Batch 梯度下降法\n\n**Mini-Batch 梯度下降法**（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。\n\n使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。\n\nbatch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：\n\n![training-with-mini-batch-gradient-descent](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png)\n\n### batch 的不同大小（size）带来的影响\n\n* mini-batch 的大小为 1，即是**随机梯度下降法（stochastic gradient descent）**，每个样本都是独立的 mini-batch；\n* mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；\n\n![choosing-mini-batch-size](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png)\n\n* batch 梯度下降法：\n    * 对所有 m 个训练样本执行一次梯度下降，**每一次迭代时间较长，训练过程慢**； \n    * 相对噪声低一些，幅度也大一些；\n    * 成本函数总是向减小的方向下降。\n\n* 随机梯度下降法：\n    * 对每一个训练样本执行一次梯度下降，训练速度快，但**丢失了向量化带来的计算加速**；\n    * 有很多噪声，减小学习率可以适当；\n    * 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。\n\n因此，选择一个`1 < size < m`的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。\n\n### mini-batch 大小的选择\n\n* 如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；\n* 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、...、$2^9$；\n* mini-batch 的大小要符合 CPU/GPU 内存。\n\nmini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。\n\n### 获得 mini-batch 的步骤\n\n1. 将数据集打乱；\n2. 按照既定的大小分割数据集；\n\n其中打乱数据集的代码：\n\n```py\nm = X.shape[1] \npermutation = list(np.random.permutation(m))\nshuffled_X = X[:, permutation]\nshuffled_Y = Y[:, permutation].reshape((1,m))\n```\n\n`np.random.permutation`与`np.random.shuffle`有两处不同：\n\n1. 如果传给`permutation`一个矩阵，它会返回一个洗牌后的矩阵副本；而`shuffle`只是对一个矩阵进行洗牌，没有返回值。\n2. 如果传入一个整数，它会返回一个洗牌后的`arange`。\n\n### 符号表示\n\n* 使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；\n* 使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；\n* 现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。\n\n## 指数平均加权\n\n**指数加权平均（Exponentially Weight Average）**是一种常用的序列数据处理方式，计算公式为：\n\n$$\nS\\_t = \n\\begin{cases} \nY\\_1, &t = 1 \\\\\\\\ \n\\beta S\\_{t-1} + (1-\\beta)Y_t, &t > 1 \n\\end{cases}\n$$\n\n其中 $Y\\_t$ 为 t 下的实际值，$S\\_t$ 为 t 下加权平均后的值，β 为权重值。\n\n指数加权平均数在统计学中被称为“指数加权移动平均值”。\n\n![Exponentially-weight-average](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png)\n\n给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。\n\n当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。**β 越大相当于求取平均利用的天数越多**，曲线自然就会越平滑而且越滞后。\n\n### 理解指数平均加权\n\n当 β 为 0.9 时，\n\n$$v\\_{100} = 0.9v\\_{99} + 0.1 \\theta\\_{100}$$\n\n$$v\\_{99} = 0.9v\\_{98} + 0.1 \\theta\\_{99}$$\n\n$$v\\_{98} = 0.9v\\_{97} + 0.1 \\theta\\_{98}$$\n$$...$$\n\n展开：\n\n$$v\\_{100} = 0.1 \\theta\\_{100} + 0.1 \\* 0.9 \\theta\\_{99} + 0.1 \\* {(0.9)}^2 \\theta\\_{98} + ...$$\n\n其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作**偏差修正（Bias Correction）**。\n\n根据函数极限的一条定理：\n\n$$\\lim\\_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$\n\n当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。\n\n因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。\n\n$$v\\_t = \\beta v\\_{t-1} + (1 - \\beta)\\theta_t$$\n\n考虑到代码，只需要不断更新 v 即可：\n\n$$v := \\beta v + (1 - \\beta)\\theta_t$$\n<!--此处应有公式的实现代码-->\n\n指数平均加权并**不是最精准**的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。\n\n指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此**效率极高，且节省成本**。\n\n### 指数平均加权的偏差修正\n\n我们通常有\n\n$$v\\_0 = 0$$\n$$v\\_1 = 0.98v\\_0 + 0.02\\theta\\_1$$\n\n因此，$v\\_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。\n\n因此，我们修改公式为\n\n$$v\\_t = \\frac{\\beta v\\_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$\n\n随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。\n\n## 动量梯度下降法\n\n**动量梯度下降（Gradient Descent with Momentum）**是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：\n\nfor l = 1, .. , L：\n\n$$v\\_{dW^{[l]}} = \\beta v\\_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$\n$$v\\_{db^{[l]}} = \\beta v\\_{db^{[l]}} + (1 - \\beta) db^{[l]}$$\n$$W^{[l]} := W^{[l]} - \\alpha v\\_{dW^{[l]}}$$\n$$b^{[l]} := b^{[l]} - \\alpha v\\_{db^{[l]}}$$\n\n其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。\n\n![Gradient-Descent-with-Momentum](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png)\n\n进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。\n\n而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。\n\n当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。\n\n另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。\n\n### 动量梯度下降法的形象解释\n\n将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v\\_{dw}$、$v\\_{db}$ 相当于速度。\n\n小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。\n\n## RMSProp 算法\n\n**RMSProp（Root Mean Square Prop，均方根支）**算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：\n\n$$s\\_{dw} = \\beta s\\_{dw} + (1 - \\beta)(dw)^2$$\n$$s\\_{db} = \\beta s\\_{db} + (1 - \\beta)(db)^2$$\n$$w := w - \\alpha \\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n$$b := b - \\alpha \\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。\n\n当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s\\_{dw}$、$s\\_{db}$也会较大，最终使得\n\n$$\\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n\n和\n\n$$\\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。\n\n![RMSProp](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png)\n\nRMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。\n\n注意，β 也是一个超参数。\n\n## Adam 优化算法\n\n**Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）**基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：\n\n首先进行初始化：\n\n$$v\\_{dW} = 0, s\\_{dW} = 0, v\\_{db} = 0, s\\_{db} = 0$$\n\n用每一个 mini-batch 计算 dW、db，第 t 次迭代时：\n\n$$v\\_{dW} = \\beta\\_1 v\\_{dW} + (1 - \\beta\\_1) dW$$\n$$v\\_{db} = \\beta\\_1 v\\_{db} + (1 - \\beta\\_1) db$$\n$$s\\_{dW} = \\beta\\_2 s\\_{dW} + (1 - \\beta\\_2) (dW)^2$$\n$$s\\_{db} = \\beta\\_2 s\\_{db} + (1 - \\beta\\_2) (db)^2$$\n\n一般使用 Adam 算法时需要计算偏差修正：\n\n$$v^{corrected}\\_{dW} = \\frac{v\\_{dW}}{1-{\\beta\\_1}^t}$$\n$$v^{corrected}\\_{db} = \\frac{v\\_{db}}{1-{\\beta\\_1}^t}$$\n$$s^{corrected}\\_{dW} = \\frac{s\\_{dW}}{1-{\\beta\\_2}^t}$$\n$$s^{corrected}\\_{db} = \\frac{s\\_{db}}{1-{\\beta\\_2}^t}$$\n\n所以，更新 W、b 时有：\n\n$$W := W - \\alpha \\frac{v^{corrected}\\_{dW}}{\\sqrt{s^{corrected}\\_{dW} + \\epsilon}}$$\n\n$$b := b - \\alpha \\frac{v^{corrected}\\_{db}}{\\sqrt{s^{corrected}\\_{db}} + \\epsilon}$$\n\n（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）\n\n### 超参数的选择\n\nAdam 优化算法有很多的超参数，其中\n\n* 学习率 α：需要尝试一系列的值，来寻找比较合适的；\n* β1：常用的缺省值为 0.9；\n* β2：Adam 算法的作者建议为 0.999；\n* ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；\n\nβ1、β2、ϵ 通常不需要调试。\n\n## 学习率衰减\n\n如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。\n\n而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。\n\n最常用的学习率衰减方法：\n\n$$\\alpha = \\frac{1}{1 + decay\\\\\\_rate \\* epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n其中，`decay_rate`为衰减率（超参数），`epoch_num`为将所有的训练样本完整过一遍的次数。\n\n* 指数衰减：\n\n$$\\alpha = 0.95^{epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n* 其他：\n\n$$\\alpha = \\frac{k}{\\sqrt{epoch\\\\\\_num}} \\* \\alpha\\_0$$\n\n* 离散下降\n\n对于较小的模型，也有人会在训练时根据进度手动调小学习率。\n\n## 局部最优问题\n\n![saddle](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png)\n\n**鞍点（saddle）**是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。\n\n结论：\n\n* 在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；\n* 鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。\n","source":"_posts/深度学习中的优化算法.md","raw":"---\ntitle: 深度学习中的优化算法\ndate: 2018-08-04 13:26:01\ntags: 优化算法\ncategories: 深度学习\nmathjax: true\n---\n深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。\n\n## batch 梯度下降法\n\n**batch 梯度下降法**（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。\n\n对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。\n\n但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 **mini-batch**。\n\n## Mini-Batch 梯度下降法\n\n**Mini-Batch 梯度下降法**（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。\n\n使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。\n\nbatch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：\n\n![training-with-mini-batch-gradient-descent](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png)\n\n### batch 的不同大小（size）带来的影响\n\n* mini-batch 的大小为 1，即是**随机梯度下降法（stochastic gradient descent）**，每个样本都是独立的 mini-batch；\n* mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；\n\n![choosing-mini-batch-size](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png)\n\n* batch 梯度下降法：\n    * 对所有 m 个训练样本执行一次梯度下降，**每一次迭代时间较长，训练过程慢**； \n    * 相对噪声低一些，幅度也大一些；\n    * 成本函数总是向减小的方向下降。\n\n* 随机梯度下降法：\n    * 对每一个训练样本执行一次梯度下降，训练速度快，但**丢失了向量化带来的计算加速**；\n    * 有很多噪声，减小学习率可以适当；\n    * 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。\n\n因此，选择一个`1 < size < m`的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。\n\n### mini-batch 大小的选择\n\n* 如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；\n* 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、...、$2^9$；\n* mini-batch 的大小要符合 CPU/GPU 内存。\n\nmini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。\n\n### 获得 mini-batch 的步骤\n\n1. 将数据集打乱；\n2. 按照既定的大小分割数据集；\n\n其中打乱数据集的代码：\n\n```py\nm = X.shape[1] \npermutation = list(np.random.permutation(m))\nshuffled_X = X[:, permutation]\nshuffled_Y = Y[:, permutation].reshape((1,m))\n```\n\n`np.random.permutation`与`np.random.shuffle`有两处不同：\n\n1. 如果传给`permutation`一个矩阵，它会返回一个洗牌后的矩阵副本；而`shuffle`只是对一个矩阵进行洗牌，没有返回值。\n2. 如果传入一个整数，它会返回一个洗牌后的`arange`。\n\n### 符号表示\n\n* 使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；\n* 使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；\n* 现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。\n\n## 指数平均加权\n\n**指数加权平均（Exponentially Weight Average）**是一种常用的序列数据处理方式，计算公式为：\n\n$$\nS\\_t = \n\\begin{cases} \nY\\_1, &t = 1 \\\\\\\\ \n\\beta S\\_{t-1} + (1-\\beta)Y_t, &t > 1 \n\\end{cases}\n$$\n\n其中 $Y\\_t$ 为 t 下的实际值，$S\\_t$ 为 t 下加权平均后的值，β 为权重值。\n\n指数加权平均数在统计学中被称为“指数加权移动平均值”。\n\n![Exponentially-weight-average](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png)\n\n给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。\n\n当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。**β 越大相当于求取平均利用的天数越多**，曲线自然就会越平滑而且越滞后。\n\n### 理解指数平均加权\n\n当 β 为 0.9 时，\n\n$$v\\_{100} = 0.9v\\_{99} + 0.1 \\theta\\_{100}$$\n\n$$v\\_{99} = 0.9v\\_{98} + 0.1 \\theta\\_{99}$$\n\n$$v\\_{98} = 0.9v\\_{97} + 0.1 \\theta\\_{98}$$\n$$...$$\n\n展开：\n\n$$v\\_{100} = 0.1 \\theta\\_{100} + 0.1 \\* 0.9 \\theta\\_{99} + 0.1 \\* {(0.9)}^2 \\theta\\_{98} + ...$$\n\n其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作**偏差修正（Bias Correction）**。\n\n根据函数极限的一条定理：\n\n$$\\lim\\_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$\n\n当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。\n\n因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。\n\n$$v\\_t = \\beta v\\_{t-1} + (1 - \\beta)\\theta_t$$\n\n考虑到代码，只需要不断更新 v 即可：\n\n$$v := \\beta v + (1 - \\beta)\\theta_t$$\n<!--此处应有公式的实现代码-->\n\n指数平均加权并**不是最精准**的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。\n\n指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此**效率极高，且节省成本**。\n\n### 指数平均加权的偏差修正\n\n我们通常有\n\n$$v\\_0 = 0$$\n$$v\\_1 = 0.98v\\_0 + 0.02\\theta\\_1$$\n\n因此，$v\\_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。\n\n因此，我们修改公式为\n\n$$v\\_t = \\frac{\\beta v\\_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$\n\n随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。\n\n## 动量梯度下降法\n\n**动量梯度下降（Gradient Descent with Momentum）**是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：\n\nfor l = 1, .. , L：\n\n$$v\\_{dW^{[l]}} = \\beta v\\_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$\n$$v\\_{db^{[l]}} = \\beta v\\_{db^{[l]}} + (1 - \\beta) db^{[l]}$$\n$$W^{[l]} := W^{[l]} - \\alpha v\\_{dW^{[l]}}$$\n$$b^{[l]} := b^{[l]} - \\alpha v\\_{db^{[l]}}$$\n\n其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。\n\n![Gradient-Descent-with-Momentum](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png)\n\n进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。\n\n而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。\n\n当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。\n\n另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。\n\n### 动量梯度下降法的形象解释\n\n将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v\\_{dw}$、$v\\_{db}$ 相当于速度。\n\n小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。\n\n## RMSProp 算法\n\n**RMSProp（Root Mean Square Prop，均方根支）**算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：\n\n$$s\\_{dw} = \\beta s\\_{dw} + (1 - \\beta)(dw)^2$$\n$$s\\_{db} = \\beta s\\_{db} + (1 - \\beta)(db)^2$$\n$$w := w - \\alpha \\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n$$b := b - \\alpha \\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。\n\n当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s\\_{dw}$、$s\\_{db}$也会较大，最终使得\n\n$$\\frac{dw}{\\sqrt{s\\_{dw} + \\epsilon}}$$\n\n和\n\n$$\\frac{db}{\\sqrt{s\\_{db} + \\epsilon}}$$\n\n较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。\n\n![RMSProp](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png)\n\nRMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。\n\n注意，β 也是一个超参数。\n\n## Adam 优化算法\n\n**Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）**基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：\n\n首先进行初始化：\n\n$$v\\_{dW} = 0, s\\_{dW} = 0, v\\_{db} = 0, s\\_{db} = 0$$\n\n用每一个 mini-batch 计算 dW、db，第 t 次迭代时：\n\n$$v\\_{dW} = \\beta\\_1 v\\_{dW} + (1 - \\beta\\_1) dW$$\n$$v\\_{db} = \\beta\\_1 v\\_{db} + (1 - \\beta\\_1) db$$\n$$s\\_{dW} = \\beta\\_2 s\\_{dW} + (1 - \\beta\\_2) (dW)^2$$\n$$s\\_{db} = \\beta\\_2 s\\_{db} + (1 - \\beta\\_2) (db)^2$$\n\n一般使用 Adam 算法时需要计算偏差修正：\n\n$$v^{corrected}\\_{dW} = \\frac{v\\_{dW}}{1-{\\beta\\_1}^t}$$\n$$v^{corrected}\\_{db} = \\frac{v\\_{db}}{1-{\\beta\\_1}^t}$$\n$$s^{corrected}\\_{dW} = \\frac{s\\_{dW}}{1-{\\beta\\_2}^t}$$\n$$s^{corrected}\\_{db} = \\frac{s\\_{db}}{1-{\\beta\\_2}^t}$$\n\n所以，更新 W、b 时有：\n\n$$W := W - \\alpha \\frac{v^{corrected}\\_{dW}}{\\sqrt{s^{corrected}\\_{dW} + \\epsilon}}$$\n\n$$b := b - \\alpha \\frac{v^{corrected}\\_{db}}{\\sqrt{s^{corrected}\\_{db}} + \\epsilon}$$\n\n（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）\n\n### 超参数的选择\n\nAdam 优化算法有很多的超参数，其中\n\n* 学习率 α：需要尝试一系列的值，来寻找比较合适的；\n* β1：常用的缺省值为 0.9；\n* β2：Adam 算法的作者建议为 0.999；\n* ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；\n\nβ1、β2、ϵ 通常不需要调试。\n\n## 学习率衰减\n\n如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。\n\n而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。\n\n最常用的学习率衰减方法：\n\n$$\\alpha = \\frac{1}{1 + decay\\\\\\_rate \\* epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n其中，`decay_rate`为衰减率（超参数），`epoch_num`为将所有的训练样本完整过一遍的次数。\n\n* 指数衰减：\n\n$$\\alpha = 0.95^{epoch\\\\\\_num} \\* \\alpha\\_0$$\n\n* 其他：\n\n$$\\alpha = \\frac{k}{\\sqrt{epoch\\\\\\_num}} \\* \\alpha\\_0$$\n\n* 离散下降\n\n对于较小的模型，也有人会在训练时根据进度手动调小学习率。\n\n## 局部最优问题\n\n![saddle](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png)\n\n**鞍点（saddle）**是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。\n\n结论：\n\n* 在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；\n* 鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。\n","slug":"深度学习中的优化算法","published":1,"updated":"2018-08-19T01:59:35.807Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh5n002pzkvopy2jr5jc","content":"<p>深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。</p>\n<h2 id=\"batch-梯度下降法\"><a href=\"#batch-梯度下降法\" class=\"headerlink\" title=\"batch 梯度下降法\"></a>batch 梯度下降法</h2><p><strong>batch 梯度下降法</strong>（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。</p>\n<p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。</p>\n<p>但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 <strong>mini-batch</strong>。</p>\n<h2 id=\"Mini-Batch-梯度下降法\"><a href=\"#Mini-Batch-梯度下降法\" class=\"headerlink\" title=\"Mini-Batch 梯度下降法\"></a>Mini-Batch 梯度下降法</h2><p><strong>Mini-Batch 梯度下降法</strong>（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。</p>\n<p>使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。</p>\n<p>batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png\" alt=\"training-with-mini-batch-gradient-descent\"></p>\n<h3 id=\"batch-的不同大小（size）带来的影响\"><a href=\"#batch-的不同大小（size）带来的影响\" class=\"headerlink\" title=\"batch 的不同大小（size）带来的影响\"></a>batch 的不同大小（size）带来的影响</h3><ul>\n<li>mini-batch 的大小为 1，即是<strong>随机梯度下降法（stochastic gradient descent）</strong>，每个样本都是独立的 mini-batch；</li>\n<li>mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png\" alt=\"choosing-mini-batch-size\"></p>\n<ul>\n<li><p>batch 梯度下降法：</p>\n<ul>\n<li>对所有 m 个训练样本执行一次梯度下降，<strong>每一次迭代时间较长，训练过程慢</strong>； </li>\n<li>相对噪声低一些，幅度也大一些；</li>\n<li>成本函数总是向减小的方向下降。</li>\n</ul>\n</li>\n<li><p>随机梯度下降法：</p>\n<ul>\n<li>对每一个训练样本执行一次梯度下降，训练速度快，但<strong>丢失了向量化带来的计算加速</strong>；</li>\n<li>有很多噪声，减小学习率可以适当；</li>\n<li>成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。</li>\n</ul>\n</li>\n</ul>\n<p>因此，选择一个<code>1 &lt; size &lt; m</code>的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。</p>\n<h3 id=\"mini-batch-大小的选择\"><a href=\"#mini-batch-大小的选择\" class=\"headerlink\" title=\"mini-batch 大小的选择\"></a>mini-batch 大小的选择</h3><ul>\n<li>如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；</li>\n<li>如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、…、$2^9$；</li>\n<li>mini-batch 的大小要符合 CPU/GPU 内存。</li>\n</ul>\n<p>mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。</p>\n<h3 id=\"获得-mini-batch-的步骤\"><a href=\"#获得-mini-batch-的步骤\" class=\"headerlink\" title=\"获得 mini-batch 的步骤\"></a>获得 mini-batch 的步骤</h3><ol>\n<li>将数据集打乱；</li>\n<li>按照既定的大小分割数据集；</li>\n</ol>\n<p>其中打乱数据集的代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m = X.shape[<span class=\"number\">1</span>] </span><br><span class=\"line\">permutation = list(np.random.permutation(m))</span><br><span class=\"line\">shuffled_X = X[:, permutation]</span><br><span class=\"line\">shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br></pre></td></tr></table></figure>\n<p><code>np.random.permutation</code>与<code>np.random.shuffle</code>有两处不同：</p>\n<ol>\n<li>如果传给<code>permutation</code>一个矩阵，它会返回一个洗牌后的矩阵副本；而<code>shuffle</code>只是对一个矩阵进行洗牌，没有返回值。</li>\n<li>如果传入一个整数，它会返回一个洗牌后的<code>arange</code>。</li>\n</ol>\n<h3 id=\"符号表示\"><a href=\"#符号表示\" class=\"headerlink\" title=\"符号表示\"></a>符号表示</h3><ul>\n<li>使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；</li>\n<li>使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；</li>\n<li>现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。</li>\n</ul>\n<h2 id=\"指数平均加权\"><a href=\"#指数平均加权\" class=\"headerlink\" title=\"指数平均加权\"></a>指数平均加权</h2><p><strong>指数加权平均（Exponentially Weight Average）</strong>是一种常用的序列数据处理方式，计算公式为：</p>\n<p>$$<br>S_t =<br>\\begin{cases}<br>Y_1, &amp;t = 1 \\\\<br>\\beta S_{t-1} + (1-\\beta)Y_t, &amp;t &gt; 1<br>\\end{cases}<br>$$</p>\n<p>其中 $Y_t$ 为 t 下的实际值，$S_t$ 为 t 下加权平均后的值，β 为权重值。</p>\n<p>指数加权平均数在统计学中被称为“指数加权移动平均值”。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png\" alt=\"Exponentially-weight-average\"></p>\n<p>给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。</p>\n<p>当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。<strong>β 越大相当于求取平均利用的天数越多</strong>，曲线自然就会越平滑而且越滞后。</p>\n<h3 id=\"理解指数平均加权\"><a href=\"#理解指数平均加权\" class=\"headerlink\" title=\"理解指数平均加权\"></a>理解指数平均加权</h3><p>当 β 为 0.9 时，</p>\n<p>$$v_{100} = 0.9v_{99} + 0.1 \\theta_{100}$$</p>\n<p>$$v_{99} = 0.9v_{98} + 0.1 \\theta_{99}$$</p>\n<p>$$v_{98} = 0.9v_{97} + 0.1 \\theta_{98}$$<br>$$…$$</p>\n<p>展开：</p>\n<p>$$v_{100} = 0.1 \\theta_{100} + 0.1 * 0.9 \\theta_{99} + 0.1 * {(0.9)}^2 \\theta_{98} + …$$</p>\n<p>其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作<strong>偏差修正（Bias Correction）</strong>。</p>\n<p>根据函数极限的一条定理：</p>\n<p>$$\\lim_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$</p>\n<p>当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。</p>\n<p>因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。</p>\n<p>$$v_t = \\beta v_{t-1} + (1 - \\beta)\\theta_t$$</p>\n<p>考虑到代码，只需要不断更新 v 即可：</p>\n<p>$$v := \\beta v + (1 - \\beta)\\theta_t$$<br><!--此处应有公式的实现代码--></p>\n<p>指数平均加权并<strong>不是最精准</strong>的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。</p>\n<p>指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此<strong>效率极高，且节省成本</strong>。</p>\n<h3 id=\"指数平均加权的偏差修正\"><a href=\"#指数平均加权的偏差修正\" class=\"headerlink\" title=\"指数平均加权的偏差修正\"></a>指数平均加权的偏差修正</h3><p>我们通常有</p>\n<p>$$v_0 = 0$$<br>$$v_1 = 0.98v_0 + 0.02\\theta_1$$</p>\n<p>因此，$v_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。</p>\n<p>因此，我们修改公式为</p>\n<p>$$v_t = \\frac{\\beta v_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$</p>\n<p>随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。</p>\n<h2 id=\"动量梯度下降法\"><a href=\"#动量梯度下降法\" class=\"headerlink\" title=\"动量梯度下降法\"></a>动量梯度下降法</h2><p><strong>动量梯度下降（Gradient Descent with Momentum）</strong>是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：</p>\n<p>for l = 1, .. , L：</p>\n<p>$$v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$<br>$$v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]}$$<br>$$W^{[l]} := W^{[l]} - \\alpha v_{dW^{[l]}}$$<br>$$b^{[l]} := b^{[l]} - \\alpha v_{db^{[l]}}$$</p>\n<p>其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png\" alt=\"Gradient-Descent-with-Momentum\"></p>\n<p>进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。</p>\n<p>而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。</p>\n<p>当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。</p>\n<p>另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。</p>\n<h3 id=\"动量梯度下降法的形象解释\"><a href=\"#动量梯度下降法的形象解释\" class=\"headerlink\" title=\"动量梯度下降法的形象解释\"></a>动量梯度下降法的形象解释</h3><p>将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v_{dw}$、$v_{db}$ 相当于速度。</p>\n<p>小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。</p>\n<h2 id=\"RMSProp-算法\"><a href=\"#RMSProp-算法\" class=\"headerlink\" title=\"RMSProp 算法\"></a>RMSProp 算法</h2><p><strong>RMSProp（Root Mean Square Prop，均方根支）</strong>算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：</p>\n<p>$$s_{dw} = \\beta s_{dw} + (1 - \\beta)(dw)^2$$<br>$$s_{db} = \\beta s_{db} + (1 - \\beta)(db)^2$$<br>$$w := w - \\alpha \\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$<br>$$b := b - \\alpha \\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。</p>\n<p>当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s_{dw}$、$s_{db}$也会较大，最终使得</p>\n<p>$$\\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$</p>\n<p>和</p>\n<p>$$\\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png\" alt=\"RMSProp\"></p>\n<p>RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。</p>\n<p>注意，β 也是一个超参数。</p>\n<h2 id=\"Adam-优化算法\"><a href=\"#Adam-优化算法\" class=\"headerlink\" title=\"Adam 优化算法\"></a>Adam 优化算法</h2><p><strong>Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）</strong>基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：</p>\n<p>首先进行初始化：</p>\n<p>$$v_{dW} = 0, s_{dW} = 0, v_{db} = 0, s_{db} = 0$$</p>\n<p>用每一个 mini-batch 计算 dW、db，第 t 次迭代时：</p>\n<p>$$v_{dW} = \\beta_1 v_{dW} + (1 - \\beta_1) dW$$<br>$$v_{db} = \\beta_1 v_{db} + (1 - \\beta_1) db$$<br>$$s_{dW} = \\beta_2 s_{dW} + (1 - \\beta_2) (dW)^2$$<br>$$s_{db} = \\beta_2 s_{db} + (1 - \\beta_2) (db)^2$$</p>\n<p>一般使用 Adam 算法时需要计算偏差修正：</p>\n<p>$$v^{corrected}_{dW} = \\frac{v_{dW}}{1-{\\beta_1}^t}$$<br>$$v^{corrected}_{db} = \\frac{v_{db}}{1-{\\beta_1}^t}$$<br>$$s^{corrected}_{dW} = \\frac{s_{dW}}{1-{\\beta_2}^t}$$<br>$$s^{corrected}_{db} = \\frac{s_{db}}{1-{\\beta_2}^t}$$</p>\n<p>所以，更新 W、b 时有：</p>\n<p>$$W := W - \\alpha \\frac{v^{corrected}_{dW}}{\\sqrt{s^{corrected}_{dW} + \\epsilon}}$$</p>\n<p>$$b := b - \\alpha \\frac{v^{corrected}_{db}}{\\sqrt{s^{corrected}_{db}} + \\epsilon}$$</p>\n<p>（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）</p>\n<h3 id=\"超参数的选择\"><a href=\"#超参数的选择\" class=\"headerlink\" title=\"超参数的选择\"></a>超参数的选择</h3><p>Adam 优化算法有很多的超参数，其中</p>\n<ul>\n<li>学习率 α：需要尝试一系列的值，来寻找比较合适的；</li>\n<li>β1：常用的缺省值为 0.9；</li>\n<li>β2：Adam 算法的作者建议为 0.999；</li>\n<li>ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；</li>\n</ul>\n<p>β1、β2、ϵ 通常不需要调试。</p>\n<h2 id=\"学习率衰减\"><a href=\"#学习率衰减\" class=\"headerlink\" title=\"学习率衰减\"></a>学习率衰减</h2><p>如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。</p>\n<p>而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。</p>\n<p>最常用的学习率衰减方法：</p>\n<p>$$\\alpha = \\frac{1}{1 + decay\\_rate * epoch\\_num} * \\alpha_0$$</p>\n<p>其中，<code>decay_rate</code>为衰减率（超参数），<code>epoch_num</code>为将所有的训练样本完整过一遍的次数。</p>\n<ul>\n<li>指数衰减：</li>\n</ul>\n<p>$$\\alpha = 0.95^{epoch\\_num} * \\alpha_0$$</p>\n<ul>\n<li>其他：</li>\n</ul>\n<p>$$\\alpha = \\frac{k}{\\sqrt{epoch\\_num}} * \\alpha_0$$</p>\n<ul>\n<li>离散下降</li>\n</ul>\n<p>对于较小的模型，也有人会在训练时根据进度手动调小学习率。</p>\n<h2 id=\"局部最优问题\"><a href=\"#局部最优问题\" class=\"headerlink\" title=\"局部最优问题\"></a>局部最优问题</h2><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png\" alt=\"saddle\"></p>\n<p><strong>鞍点（saddle）</strong>是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。</p>\n<p>结论：</p>\n<ul>\n<li>在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；</li>\n<li>鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>深度学习难以在大数据领域发挥最大效果的一个原因是，在巨大的数据集基础上进行训练速度很慢。而优化算法能够帮助快速训练模型，大大提高效率。</p>\n<h2 id=\"batch-梯度下降法\"><a href=\"#batch-梯度下降法\" class=\"headerlink\" title=\"batch 梯度下降法\"></a>batch 梯度下降法</h2><p><strong>batch 梯度下降法</strong>（批梯度下降法，我们之前一直使用的梯度下降法）是最常用的梯度下降形式，即同时处理整个训练集。其在更新参数时使用所有的样本来进行更新。</p>\n<p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会比较慢。</p>\n<p>但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 <strong>mini-batch</strong>。</p>\n<h2 id=\"Mini-Batch-梯度下降法\"><a href=\"#Mini-Batch-梯度下降法\" class=\"headerlink\" title=\"Mini-Batch 梯度下降法\"></a>Mini-Batch 梯度下降法</h2><p><strong>Mini-Batch 梯度下降法</strong>（小批量梯度下降法）每次同时处理单个的 mini-batch，其他与 batch 梯度下降法一致。</p>\n<p>使用 batch 梯度下降法，对整个训练集的一次遍历只能做一个梯度下降；而使用 Mini-Batch 梯度下降法，对整个训练集的一次遍历（称为一个 epoch）能做 mini-batch 个数个梯度下降。之后，可以一直遍历训练集，直到最后收敛到一个合适的精度。</p>\n<p>batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势如下：</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/training-with-mini-batch-gradient-descent.png\" alt=\"training-with-mini-batch-gradient-descent\"></p>\n<h3 id=\"batch-的不同大小（size）带来的影响\"><a href=\"#batch-的不同大小（size）带来的影响\" class=\"headerlink\" title=\"batch 的不同大小（size）带来的影响\"></a>batch 的不同大小（size）带来的影响</h3><ul>\n<li>mini-batch 的大小为 1，即是<strong>随机梯度下降法（stochastic gradient descent）</strong>，每个样本都是独立的 mini-batch；</li>\n<li>mini-batch 的大小为 m（数据集大小），即是 batch 梯度下降法；</li>\n</ul>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/choosing-mini-batch-size.png\" alt=\"choosing-mini-batch-size\"></p>\n<ul>\n<li><p>batch 梯度下降法：</p>\n<ul>\n<li>对所有 m 个训练样本执行一次梯度下降，<strong>每一次迭代时间较长，训练过程慢</strong>； </li>\n<li>相对噪声低一些，幅度也大一些；</li>\n<li>成本函数总是向减小的方向下降。</li>\n</ul>\n</li>\n<li><p>随机梯度下降法：</p>\n<ul>\n<li>对每一个训练样本执行一次梯度下降，训练速度快，但<strong>丢失了向量化带来的计算加速</strong>；</li>\n<li>有很多噪声，减小学习率可以适当；</li>\n<li>成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动。</li>\n</ul>\n</li>\n</ul>\n<p>因此，选择一个<code>1 &lt; size &lt; m</code>的合适的大小进行 Mini-batch 梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。</p>\n<h3 id=\"mini-batch-大小的选择\"><a href=\"#mini-batch-大小的选择\" class=\"headerlink\" title=\"mini-batch 大小的选择\"></a>mini-batch 大小的选择</h3><ul>\n<li>如果训练样本的大小比较小，如 $m \\lt 2000$ 时，选择 batch 梯度下降法；</li>\n<li>如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为 $2^6$、$2^7$、…、$2^9$；</li>\n<li>mini-batch 的大小要符合 CPU/GPU 内存。</li>\n</ul>\n<p>mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。</p>\n<h3 id=\"获得-mini-batch-的步骤\"><a href=\"#获得-mini-batch-的步骤\" class=\"headerlink\" title=\"获得 mini-batch 的步骤\"></a>获得 mini-batch 的步骤</h3><ol>\n<li>将数据集打乱；</li>\n<li>按照既定的大小分割数据集；</li>\n</ol>\n<p>其中打乱数据集的代码：</p>\n<figure class=\"highlight py\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">m = X.shape[<span class=\"number\">1</span>] </span><br><span class=\"line\">permutation = list(np.random.permutation(m))</span><br><span class=\"line\">shuffled_X = X[:, permutation]</span><br><span class=\"line\">shuffled_Y = Y[:, permutation].reshape((<span class=\"number\">1</span>,m))</span><br></pre></td></tr></table></figure>\n<p><code>np.random.permutation</code>与<code>np.random.shuffle</code>有两处不同：</p>\n<ol>\n<li>如果传给<code>permutation</code>一个矩阵，它会返回一个洗牌后的矩阵副本；而<code>shuffle</code>只是对一个矩阵进行洗牌，没有返回值。</li>\n<li>如果传入一个整数，它会返回一个洗牌后的<code>arange</code>。</li>\n</ol>\n<h3 id=\"符号表示\"><a href=\"#符号表示\" class=\"headerlink\" title=\"符号表示\"></a>符号表示</h3><ul>\n<li>使用上角小括号 i 表示训练集里的值，$x^{(i)}$ 是第 i 个训练样本；</li>\n<li>使用上角中括号 l 表示神经网络的层数，$z^{[l]}$ 表示神经网络中第 l 层的 z 值；</li>\n<li>现在引入大括号 t 来代表不同的 mini-batch，因此有 $X^{t}$、$Y^{t}$。</li>\n</ul>\n<h2 id=\"指数平均加权\"><a href=\"#指数平均加权\" class=\"headerlink\" title=\"指数平均加权\"></a>指数平均加权</h2><p><strong>指数加权平均（Exponentially Weight Average）</strong>是一种常用的序列数据处理方式，计算公式为：</p>\n<p>$$<br>S_t =<br>\\begin{cases}<br>Y_1, &amp;t = 1 \\\\<br>\\beta S_{t-1} + (1-\\beta)Y_t, &amp;t &gt; 1<br>\\end{cases}<br>$$</p>\n<p>其中 $Y_t$ 为 t 下的实际值，$S_t$ 为 t 下加权平均后的值，β 为权重值。</p>\n<p>指数加权平均数在统计学中被称为“指数加权移动平均值”。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Exponentially-weight-average.png\" alt=\"Exponentially-weight-average\"></p>\n<p>给定一个时间序列，例如伦敦一年每天的气温值，图中蓝色的点代表真实数据。对于一个即时的气温值，取权重值 β 为 0.9，根据求得的值可以得到图中的红色曲线，它反映了气温变化的大致趋势。</p>\n<p>当取权重值 β=0.98 时，可以得到图中更为平滑的绿色曲线。而当取权重值 β=0.5 时，得到图中噪点更多的黄色曲线。<strong>β 越大相当于求取平均利用的天数越多</strong>，曲线自然就会越平滑而且越滞后。</p>\n<h3 id=\"理解指数平均加权\"><a href=\"#理解指数平均加权\" class=\"headerlink\" title=\"理解指数平均加权\"></a>理解指数平均加权</h3><p>当 β 为 0.9 时，</p>\n<p>$$v_{100} = 0.9v_{99} + 0.1 \\theta_{100}$$</p>\n<p>$$v_{99} = 0.9v_{98} + 0.1 \\theta_{99}$$</p>\n<p>$$v_{98} = 0.9v_{97} + 0.1 \\theta_{98}$$<br>$$…$$</p>\n<p>展开：</p>\n<p>$$v_{100} = 0.1 \\theta_{100} + 0.1 * 0.9 \\theta_{99} + 0.1 * {(0.9)}^2 \\theta_{98} + …$$</p>\n<p>其中 θi 指第 i 天的实际数据。所有 θ 前面的系数（不包括 0.1）相加起来为 1 或者接近于 1，这些系数被称作<strong>偏差修正（Bias Correction）</strong>。</p>\n<p>根据函数极限的一条定理：</p>\n<p>$$\\lim_{\\beta\\to 0}(1 - \\beta)^\\frac{1}{\\beta} = \\frac{1}{e} \\approx 0.368$$</p>\n<p>当 β 为 0.9 时，可以当作把过去 10 天的气温指数加权平均作为当日的气温，因为 10 天后权重已经下降到了当天的 1/3 左右。同理，当 β 为 0.98 时，可以把过去 50 天的气温指数加权平均作为当日的气温。</p>\n<p>因此，在计算当前时刻的平均值时，只需要前一天的平均值和当前时刻的值。</p>\n<p>$$v_t = \\beta v_{t-1} + (1 - \\beta)\\theta_t$$</p>\n<p>考虑到代码，只需要不断更新 v 即可：</p>\n<p>$$v := \\beta v + (1 - \\beta)\\theta_t$$<br><!--此处应有公式的实现代码--></p>\n<p>指数平均加权并<strong>不是最精准</strong>的计算平均数的方法，你可以直接计算过去 10 天或 50 天的平均值来得到更好的估计，但缺点是保存数据需要占用更多内存，执行更加复杂，计算成本更加高昂。</p>\n<p>指数加权平均数公式的好处之一在于它只需要一行代码，且占用极少内存，因此<strong>效率极高，且节省成本</strong>。</p>\n<h3 id=\"指数平均加权的偏差修正\"><a href=\"#指数平均加权的偏差修正\" class=\"headerlink\" title=\"指数平均加权的偏差修正\"></a>指数平均加权的偏差修正</h3><p>我们通常有</p>\n<p>$$v_0 = 0$$<br>$$v_1 = 0.98v_0 + 0.02\\theta_1$$</p>\n<p>因此，$v_1$ 仅为第一个数据的 0.02（或者说 1-β），显然不准确。往后递推同理。</p>\n<p>因此，我们修改公式为</p>\n<p>$$v_t = \\frac{\\beta v_{t-1} + (1 - \\beta)\\theta_t}{1-\\beta^t}$$</p>\n<p>随着 t 的增大，β 的 t 次方趋近于 0。因此当 t 很大的时候，偏差修正几乎没有作用，但是在前期学习可以帮助更好的预测数据。在实际过程中，一般会忽略前期偏差的影响。</p>\n<h2 id=\"动量梯度下降法\"><a href=\"#动量梯度下降法\" class=\"headerlink\" title=\"动量梯度下降法\"></a>动量梯度下降法</h2><p><strong>动量梯度下降（Gradient Descent with Momentum）</strong>是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为：</p>\n<p>for l = 1, .. , L：</p>\n<p>$$v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]}$$<br>$$v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]}$$<br>$$W^{[l]} := W^{[l]} - \\alpha v_{dW^{[l]}}$$<br>$$b^{[l]} := b^{[l]} - \\alpha v_{db^{[l]}}$$</p>\n<p>其中，将动量衰减参数 β 设置为 0.9 是超参数的一个常见且效果不错的选择。当 β 被设置为 0 时，显然就成了 batch 梯度下降法。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/Gradient-Descent-with-Momentum.png\" alt=\"Gradient-Descent-with-Momentum\"></p>\n<p>进行一般的梯度下降将会得到图中的蓝色曲线，由于存在上下波动，减缓了梯度下降的速度，因此只能使用一个较小的学习率进行迭代。如果用较大的学习率，结果可能会像紫色曲线一样偏离函数的范围。</p>\n<p>而使用动量梯度下降时，通过累加过去的梯度值来减少抵达最小值路径上的波动，加速了收敛，因此在横轴方向下降得更快，从而得到图中红色的曲线。</p>\n<p>当前后梯度方向一致时，动量梯度下降能够加速学习；而前后梯度方向不一致时，动量梯度下降能够抑制震荡。</p>\n<p>另外，在 10 次迭代之后，移动平均已经不再是一个具有偏差的预测。因此实际在使用梯度下降法或者动量梯度下降法时，不会同时进行偏差修正。</p>\n<h3 id=\"动量梯度下降法的形象解释\"><a href=\"#动量梯度下降法的形象解释\" class=\"headerlink\" title=\"动量梯度下降法的形象解释\"></a>动量梯度下降法的形象解释</h3><p>将成本函数想象为一个碗状，从顶部开始运动的小球向下滚，其中 dw，db 想象成球的加速度；而 $v_{dw}$、$v_{db}$ 相当于速度。</p>\n<p>小球在向下滚动的过程中，因为加速度的存在速度会变快，但是由于 β 的存在，其值小于 1，可以认为是摩擦力，所以球不会无限加速下去。</p>\n<h2 id=\"RMSProp-算法\"><a href=\"#RMSProp-算法\" class=\"headerlink\" title=\"RMSProp 算法\"></a>RMSProp 算法</h2><p><strong>RMSProp（Root Mean Square Prop，均方根支）</strong>算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。具体过程为（省略了 l）：</p>\n<p>$$s_{dw} = \\beta s_{dw} + (1 - \\beta)(dw)^2$$<br>$$s_{db} = \\beta s_{db} + (1 - \\beta)(db)^2$$<br>$$w := w - \\alpha \\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$<br>$$b := b - \\alpha \\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>其中，ϵ 是一个实际操作时加上的较小数（例如10^-8），为了防止分母太小而导致的数值不稳定。</p>\n<p>当 dw 或 db 较大时，$(dw)^2$、$(db)^2$会较大，进而 $s_{dw}$、$s_{db}$也会较大，最终使得</p>\n<p>$$\\frac{dw}{\\sqrt{s_{dw} + \\epsilon}}$$</p>\n<p>和</p>\n<p>$$\\frac{db}{\\sqrt{s_{db} + \\epsilon}}$$</p>\n<p>较小，从而减小某些维度梯度更新波动较大的情况，使下降速度变得更快。</p>\n<p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/RMSProp.png\" alt=\"RMSProp\"></p>\n<p>RMSProp 有助于减少抵达最小值路径上的摆动，并允许使用一个更大的学习率 α，从而加快算法学习速度。并且，它和 Adam 优化算法已被证明适用于不同的深度学习网络结构。</p>\n<p>注意，β 也是一个超参数。</p>\n<h2 id=\"Adam-优化算法\"><a href=\"#Adam-优化算法\" class=\"headerlink\" title=\"Adam 优化算法\"></a>Adam 优化算法</h2><p><strong>Adam 优化算法（Adaptive Moment Estimation，自适应矩估计）</strong>基本上就是将 Momentum 和 RMSProp 算法结合在一起，通常有超越二者单独时的效果。具体过程如下（省略了 l）：</p>\n<p>首先进行初始化：</p>\n<p>$$v_{dW} = 0, s_{dW} = 0, v_{db} = 0, s_{db} = 0$$</p>\n<p>用每一个 mini-batch 计算 dW、db，第 t 次迭代时：</p>\n<p>$$v_{dW} = \\beta_1 v_{dW} + (1 - \\beta_1) dW$$<br>$$v_{db} = \\beta_1 v_{db} + (1 - \\beta_1) db$$<br>$$s_{dW} = \\beta_2 s_{dW} + (1 - \\beta_2) (dW)^2$$<br>$$s_{db} = \\beta_2 s_{db} + (1 - \\beta_2) (db)^2$$</p>\n<p>一般使用 Adam 算法时需要计算偏差修正：</p>\n<p>$$v^{corrected}_{dW} = \\frac{v_{dW}}{1-{\\beta_1}^t}$$<br>$$v^{corrected}_{db} = \\frac{v_{db}}{1-{\\beta_1}^t}$$<br>$$s^{corrected}_{dW} = \\frac{s_{dW}}{1-{\\beta_2}^t}$$<br>$$s^{corrected}_{db} = \\frac{s_{db}}{1-{\\beta_2}^t}$$</p>\n<p>所以，更新 W、b 时有：</p>\n<p>$$W := W - \\alpha \\frac{v^{corrected}_{dW}}{\\sqrt{s^{corrected}_{dW} + \\epsilon}}$$</p>\n<p>$$b := b - \\alpha \\frac{v^{corrected}_{db}}{\\sqrt{s^{corrected}_{db}} + \\epsilon}$$</p>\n<p>（可以看到 Andrew 在这里 ϵ 没有写到平方根里去，和他在 RMSProp 中写的不太一样。考虑到 ϵ 所起的作用，我感觉影响不大）</p>\n<h3 id=\"超参数的选择\"><a href=\"#超参数的选择\" class=\"headerlink\" title=\"超参数的选择\"></a>超参数的选择</h3><p>Adam 优化算法有很多的超参数，其中</p>\n<ul>\n<li>学习率 α：需要尝试一系列的值，来寻找比较合适的；</li>\n<li>β1：常用的缺省值为 0.9；</li>\n<li>β2：Adam 算法的作者建议为 0.999；</li>\n<li>ϵ：不重要，不会影响算法表现，Adam 算法的作者建议为 $10^{-8}$；</li>\n</ul>\n<p>β1、β2、ϵ 通常不需要调试。</p>\n<h2 id=\"学习率衰减\"><a href=\"#学习率衰减\" class=\"headerlink\" title=\"学习率衰减\"></a>学习率衰减</h2><p>如果设置一个固定的学习率 α，在最小值点附近，由于不同的 batch 中存在一定的噪声，因此不会精确收敛，而是始终在最小值周围一个较大的范围内波动。</p>\n<p>而如果随着时间慢慢减少学习率 α 的大小，在初期 α 较大时，下降的步长较大，能以较快的速度进行梯度下降；而后期逐步减小 α 的值，即减小步长，有助于算法的收敛，更容易接近最优解。</p>\n<p>最常用的学习率衰减方法：</p>\n<p>$$\\alpha = \\frac{1}{1 + decay\\_rate * epoch\\_num} * \\alpha_0$$</p>\n<p>其中，<code>decay_rate</code>为衰减率（超参数），<code>epoch_num</code>为将所有的训练样本完整过一遍的次数。</p>\n<ul>\n<li>指数衰减：</li>\n</ul>\n<p>$$\\alpha = 0.95^{epoch\\_num} * \\alpha_0$$</p>\n<ul>\n<li>其他：</li>\n</ul>\n<p>$$\\alpha = \\frac{k}{\\sqrt{epoch\\_num}} * \\alpha_0$$</p>\n<ul>\n<li>离散下降</li>\n</ul>\n<p>对于较小的模型，也有人会在训练时根据进度手动调小学习率。</p>\n<h2 id=\"局部最优问题\"><a href=\"#局部最优问题\" class=\"headerlink\" title=\"局部最优问题\"></a>局部最优问题</h2><p><img src=\"https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/saddle.png\" alt=\"saddle\"></p>\n<p><strong>鞍点（saddle）</strong>是函数上的导数为零，但不是轴上局部极值的点。当我们建立一个神经网络时，通常梯度为零的点是上图所示的鞍点，而非局部最小值。减少损失的难度也来自误差曲面中的鞍点，而不是局部最低点。因为在一个具有高维度空间的成本函数中，如果梯度为 0，那么在每个方向，成本函数或是凸函数，或是凹函数。而所有维度均需要是凹函数的概率极小，因此在低维度的局部最优点的情况并不适用于高维度。</p>\n<p>结论：</p>\n<ul>\n<li>在训练较大的神经网络、存在大量参数，并且成本函数被定义在较高的维度空间时，困在极差的局部最优中是不大可能的；</li>\n<li>鞍点附近的平稳段会使得学习非常缓慢，而这也是动量梯度下降法、RMSProp 以及 Adam 优化算法能够加速学习的原因，它们能帮助尽早走出平稳段。</li>\n</ul>\n"},{"title":"男人的对象选择中的一种特殊类型","date":"2018-07-19T10:15:03.000Z","_content":"\n首先我将描述一种对象选择类型--选择的主体是男人--特点是设定一系列的“恋爱的必要条件”。\n\n1. 在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 **“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人--如未婚少女或心无所系的已婚妇女--他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。** 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。\n2. 第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，**名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。** 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。\n\n现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。**它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。**\n\n---\n\n摘自弗洛伊德的《爱情心理学》第一章\n","source":"_posts/男人的对象选择中的一种特殊类型.md","raw":"---\ntitle: 男人的对象选择中的一种特殊类型\ndate: 2018-07-19 18:15:03\ntags: 爱情心理学\ncategories: 心理学\n---\n\n首先我将描述一种对象选择类型--选择的主体是男人--特点是设定一系列的“恋爱的必要条件”。\n\n1. 在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 **“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人--如未婚少女或心无所系的已婚妇女--他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。** 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。\n2. 第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，**名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。** 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。\n\n现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。**它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。**\n\n---\n\n摘自弗洛伊德的《爱情心理学》第一章\n","slug":"男人的对象选择中的一种特殊类型","published":1,"updated":"2018-08-31T03:55:01.946Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh5s002szkvoyghay2nv","content":"<p>首先我将描述一种对象选择类型–选择的主体是男人–特点是设定一系列的“恋爱的必要条件”。</p>\n<ol>\n<li>在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 <strong>“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人–如未婚少女或心无所系的已婚妇女–他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。</strong> 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。</li>\n<li>第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，<strong>名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。</strong> 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。</li>\n</ol>\n<p>现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。<strong>它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。</strong></p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第一章</p>\n","site":{"data":{}},"excerpt":"","more":"<p>首先我将描述一种对象选择类型–选择的主体是男人–特点是设定一系列的“恋爱的必要条件”。</p>\n<ol>\n<li>在这些恋爱的先决条件中，有一条是通行的：只要你在一个人身上发现了它，就能在他身上找到这个类型的其他特点。这个先决条件就是得有 <strong>“受到伤害的第三方”；也就是说，这类男人永远不会选择没有归属的女人–如未婚少女或心无所系的已婚妇女–他只会选择已被其他男人占有的女人，这个其他男人可以是丈夫、未婚夫或朋友。</strong> 这个先决条件的作用十分强大，以至于只要一个女人不属于某个男人，那么在对象选择中，她就会遭到该类型的男人的忽视或拒绝；而一旦她与另一个男人确立了关系，就会立刻成为该类男人发泄激情的对象。</li>\n<li>第二个先决条件或许不像第一条那样，在该类型的每个男人身上都能找到，不过它也同样引人瞩目。第二条就是，<strong>名声无可指责的纯洁女人永远无法成为被选择的对象，只有在性方面声名狼藉、忠诚度和可信度都受到怀疑的女人才能激起该类男人的兴趣。</strong> 不过他们选择的范围也相当大，从并不厌恶调情、略有丑闻的已婚女子到性生活淫乱的妓女或深谙爱情艺术的熟女，不一而足。</li>\n</ol>\n<p>现在让我们审视一番这种类型的人的不同特征：所爱之女人必须有所归属并像个妓女；他对这样的女人评价极高；他有体验嫉妒的需要；他对这样的女人忠贞不二，而又可与多个女人更替地保持和谐之爱；他有拯救女人的强烈愿望。表面看来，这很难来自同一根源。然而，精神分析关于这种人生活史的探讨却可以轻松地找到这种单一根源。这种人对象选择的奇怪条件及示爱的单一方式，与正常人的爱具有相同的心理根源。<strong>它们源于对母亲柔情的婴儿固着，其表现乃是这种固着的结果。</strong></p>\n<hr>\n<p>摘自弗洛伊德的《爱情心理学》第一章</p>\n"},{"title":"短诗三首","date":"2018-07-19T02:25:47.000Z","_content":"\n### 确认过眼神\n\n```\n它，睁开朦胧的双眼\n看见天空\n穿着蓝色的裙子\n在微风中跳舞\n红了脸\n害羞地躲到云朵中\n\n它，热烈地燃烧\n想带给天空温暖\n却抵挡不了黑夜的到来\n黑夜一无所有\n却能给天空安慰\n\n它，裹在云朵中哭泣\n眼泪在泥土上溅起水花\n弄脏了天空的裙子\n又停止哭泣\n给天空画上了美丽的彩虹\n```\n\n### 1900\n```\n冬天来了\n你迫不及待地等待夏天\n夏天来了\n你还活在死寂的冬天\n```\n\n### 远方的山\n```\n有些情\n像远方的山\n不知所起，亦不知所终\n隔着呼啸的海浪\n任心被激打\n激打出沉默的浪花\n何不做那海燕\n到山那边歇歇脚\n```\n","source":"_posts/短诗三首.md","raw":"---\ntitle: 短诗三首\ndate: 2018-07-19 10:25:47\ntags: 现代诗\ncategories: 文学\n---\n\n### 确认过眼神\n\n```\n它，睁开朦胧的双眼\n看见天空\n穿着蓝色的裙子\n在微风中跳舞\n红了脸\n害羞地躲到云朵中\n\n它，热烈地燃烧\n想带给天空温暖\n却抵挡不了黑夜的到来\n黑夜一无所有\n却能给天空安慰\n\n它，裹在云朵中哭泣\n眼泪在泥土上溅起水花\n弄脏了天空的裙子\n又停止哭泣\n给天空画上了美丽的彩虹\n```\n\n### 1900\n```\n冬天来了\n你迫不及待地等待夏天\n夏天来了\n你还活在死寂的冬天\n```\n\n### 远方的山\n```\n有些情\n像远方的山\n不知所起，亦不知所终\n隔着呼啸的海浪\n任心被激打\n激打出沉默的浪花\n何不做那海燕\n到山那边歇歇脚\n```\n","slug":"短诗三首","published":1,"updated":"2018-08-31T03:55:08.779Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh5w002vzkvo3hxzy07d","content":"<h3 id=\"确认过眼神\"><a href=\"#确认过眼神\" class=\"headerlink\" title=\"确认过眼神\"></a>确认过眼神</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">它，睁开朦胧的双眼</span><br><span class=\"line\">看见天空</span><br><span class=\"line\">穿着蓝色的裙子</span><br><span class=\"line\">在微风中跳舞</span><br><span class=\"line\">红了脸</span><br><span class=\"line\">害羞地躲到云朵中</span><br><span class=\"line\"></span><br><span class=\"line\">它，热烈地燃烧</span><br><span class=\"line\">想带给天空温暖</span><br><span class=\"line\">却抵挡不了黑夜的到来</span><br><span class=\"line\">黑夜一无所有</span><br><span class=\"line\">却能给天空安慰</span><br><span class=\"line\"></span><br><span class=\"line\">它，裹在云朵中哭泣</span><br><span class=\"line\">眼泪在泥土上溅起水花</span><br><span class=\"line\">弄脏了天空的裙子</span><br><span class=\"line\">又停止哭泣</span><br><span class=\"line\">给天空画上了美丽的彩虹</span><br></pre></td></tr></table></figure>\n<h3 id=\"1900\"><a href=\"#1900\" class=\"headerlink\" title=\"1900\"></a>1900</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">冬天来了</span><br><span class=\"line\">你迫不及待地等待夏天</span><br><span class=\"line\">夏天来了</span><br><span class=\"line\">你还活在死寂的冬天</span><br></pre></td></tr></table></figure>\n<h3 id=\"远方的山\"><a href=\"#远方的山\" class=\"headerlink\" title=\"远方的山\"></a>远方的山</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">有些情</span><br><span class=\"line\">像远方的山</span><br><span class=\"line\">不知所起，亦不知所终</span><br><span class=\"line\">隔着呼啸的海浪</span><br><span class=\"line\">任心被激打</span><br><span class=\"line\">激打出沉默的浪花</span><br><span class=\"line\">何不做那海燕</span><br><span class=\"line\">到山那边歇歇脚</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"确认过眼神\"><a href=\"#确认过眼神\" class=\"headerlink\" title=\"确认过眼神\"></a>确认过眼神</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">它，睁开朦胧的双眼</span><br><span class=\"line\">看见天空</span><br><span class=\"line\">穿着蓝色的裙子</span><br><span class=\"line\">在微风中跳舞</span><br><span class=\"line\">红了脸</span><br><span class=\"line\">害羞地躲到云朵中</span><br><span class=\"line\"></span><br><span class=\"line\">它，热烈地燃烧</span><br><span class=\"line\">想带给天空温暖</span><br><span class=\"line\">却抵挡不了黑夜的到来</span><br><span class=\"line\">黑夜一无所有</span><br><span class=\"line\">却能给天空安慰</span><br><span class=\"line\"></span><br><span class=\"line\">它，裹在云朵中哭泣</span><br><span class=\"line\">眼泪在泥土上溅起水花</span><br><span class=\"line\">弄脏了天空的裙子</span><br><span class=\"line\">又停止哭泣</span><br><span class=\"line\">给天空画上了美丽的彩虹</span><br></pre></td></tr></table></figure>\n<h3 id=\"1900\"><a href=\"#1900\" class=\"headerlink\" title=\"1900\"></a>1900</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">冬天来了</span><br><span class=\"line\">你迫不及待地等待夏天</span><br><span class=\"line\">夏天来了</span><br><span class=\"line\">你还活在死寂的冬天</span><br></pre></td></tr></table></figure>\n<h3 id=\"远方的山\"><a href=\"#远方的山\" class=\"headerlink\" title=\"远方的山\"></a>远方的山</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">有些情</span><br><span class=\"line\">像远方的山</span><br><span class=\"line\">不知所起，亦不知所终</span><br><span class=\"line\">隔着呼啸的海浪</span><br><span class=\"line\">任心被激打</span><br><span class=\"line\">激打出沉默的浪花</span><br><span class=\"line\">何不做那海燕</span><br><span class=\"line\">到山那边歇歇脚</span><br></pre></td></tr></table></figure>\n"},{"title":"神经网络中的通用函数代码","date":"2018-07-21T08:58:55.000Z","_content":"\n## 激活函数\n\n### Sigmoid\n\n```python\ndef sigmoid(Z):\n    \"\"\"\n    Implements the sigmoid activation in numpy\n\n    Arguments:\n    Z -- numpy array of any shape\n\n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    A = 1 / (1 + np.exp(-Z))\n    cache = Z\n    return A, cache\n\ndef sigmoid_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single SIGMOID unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    s = 1 / (1 + np.exp(-Z))\n    dZ = dA * s * (1 - s)\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n### Relu\n\n```python\ndef relu(Z):\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    A = np.maximum(0, Z)\n    assert(A.shape == Z.shape)\n    cache = Z\n    return A, cache\n\ndef relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n    # When z <= 0, you should set dz to 0 as well.\n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n## 代价函数\n\n### 交叉熵\n\n```python\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    m = Y.shape[1]\n    # Compute loss from aL and y.\n    cost = -1 / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    return cost\n```\n\n### L1 and L2 Loss\n\n```python\ndef L1(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n\n    Returns:\n    loss -- the value of the L1 loss function defined above\n    \"\"\"\n    loss = np.sum(abs(yhat - y))\n    return loss\n\ndef L2(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n\n    Returns:\n    loss -- the value of the L2 loss function defined above\n    \"\"\"\n    loss = np.sum((y - yhat) ** 2)\n    return loss\n```\n\n## 线性函数\n\n```python\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter\n    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    Z = np.dot(W, A) + b\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    return Z, cache\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = 1 / m * np.dot(dZ, A_prev.T)\n    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    return dA_prev, dW, db\n```\n\n## 线性到激活层\n\n```python\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value\n    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n    return A, cache\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n\n    Arguments:\n    dA -- post-activation gradient for current layer l\n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    return dA_prev, dW, db\n```\n\n## L层前馈网络\n\n```python\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n\n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n\n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")    # 注意这里是 A\n    caches.append(cache)\n    assert(AL.shape == (1, X.shape[1]))\n    return AL, caches\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n\n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n\n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ...\n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ...\n    \"\"\"\n    grads = {}\n    L = len(caches)  # the number of layers\n    # m = AL.shape[1]\n    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL\n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    # Lth layer (SIGMOID -> LINEAR) gradients.\n    current_cache = caches[L - 1]\n    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n    for l in reversed(range(L - 1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], caches[l], 'relu')\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\n```\n\n## 参数初始化\n\n```python\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n\n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l - 1])  # He initialization\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n    return parameters\n```\n\n## 参数更新（梯度下降）\n\n```python\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n\n    Arguments:\n    parameters -- python dictionary containing your parameters\n    grads -- python dictionary containing your gradients, output of L_model_backward\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters\n                  parameters[\"W\" + str(l)] = ...\n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    L = len(parameters) // 2  # number of layers in the neural network\n    # Update rule for each parameter. Use a for loop.\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n    return parameters\n```\n\n## 训练模型\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 预测\n\n```python\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n\n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n\n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    m = X.shape[1]\n    p = np.zeros((1, m))\n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0, i] > 0.5:\n            p[0, i] = 1\n        else:\n            p[0, i] = 0\n    print(\"Accuracy: \" + str(np.sum((p == y) / m)))\n    return p\n```\n","source":"_posts/神经网络中的通用函数代码.md","raw":"---\ntitle: 神经网络中的通用函数代码\ndate: 2018-07-21 16:58:55\ntags: 神经网络\ncategories: 深度学习\n---\n\n## 激活函数\n\n### Sigmoid\n\n```python\ndef sigmoid(Z):\n    \"\"\"\n    Implements the sigmoid activation in numpy\n\n    Arguments:\n    Z -- numpy array of any shape\n\n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    A = 1 / (1 + np.exp(-Z))\n    cache = Z\n    return A, cache\n\ndef sigmoid_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single SIGMOID unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    s = 1 / (1 + np.exp(-Z))\n    dZ = dA * s * (1 - s)\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n### Relu\n\n```python\ndef relu(Z):\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    A = np.maximum(0, Z)\n    assert(A.shape == Z.shape)\n    cache = Z\n    return A, cache\n\ndef relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n    # When z <= 0, you should set dz to 0 as well.\n    dZ[Z <= 0] = 0\n    assert (dZ.shape == Z.shape)\n    return dZ\n```\n\n## 代价函数\n\n### 交叉熵\n\n```python\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    m = Y.shape[1]\n    # Compute loss from aL and y.\n    cost = -1 / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    return cost\n```\n\n### L1 and L2 Loss\n\n```python\ndef L1(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n\n    Returns:\n    loss -- the value of the L1 loss function defined above\n    \"\"\"\n    loss = np.sum(abs(yhat - y))\n    return loss\n\ndef L2(yhat, y):\n    \"\"\"\n    Arguments:\n    yhat -- vector of size m (predicted labels)\n    y -- vector of size m (true labels)\n\n    Returns:\n    loss -- the value of the L2 loss function defined above\n    \"\"\"\n    loss = np.sum((y - yhat) ** 2)\n    return loss\n```\n\n## 线性函数\n\n```python\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter\n    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    Z = np.dot(W, A) + b\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    return Z, cache\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = 1 / m * np.dot(dZ, A_prev.T)\n    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    return dA_prev, dW, db\n```\n\n## 线性到激活层\n\n```python\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value\n    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n    return A, cache\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n\n    Arguments:\n    dA -- post-activation gradient for current layer l\n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    return dA_prev, dW, db\n```\n\n## L层前馈网络\n\n```python\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n\n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n\n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")    # 注意这里是 A\n    caches.append(cache)\n    assert(AL.shape == (1, X.shape[1]))\n    return AL, caches\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n\n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n\n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ...\n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ...\n    \"\"\"\n    grads = {}\n    L = len(caches)  # the number of layers\n    # m = AL.shape[1]\n    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL\n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    # Lth layer (SIGMOID -> LINEAR) gradients.\n    current_cache = caches[L - 1]\n    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n    for l in reversed(range(L - 1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], caches[l], 'relu')\n        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\n```\n\n## 参数初始化\n\n```python\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n\n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l - 1])  # He initialization\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n    return parameters\n```\n\n## 参数更新（梯度下降）\n\n```python\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n\n    Arguments:\n    parameters -- python dictionary containing your parameters\n    grads -- python dictionary containing your gradients, output of L_model_backward\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters\n                  parameters[\"W\" + str(l)] = ...\n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    L = len(parameters) // 2  # number of layers in the neural network\n    # Update rule for each parameter. Use a for loop.\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n    return parameters\n```\n\n## 训练模型\n\n```python\ndef L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):  # lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n\n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []                         # keep track of cost\n    # Parameters initialization.\n    parameters = initialize_parameters_deep(layers_dims)\n\n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        AL, caches = L_model_forward(X, parameters)\n        # Compute cost.\n        cost = compute_cost(AL, Y)\n        # Backward propagation.\n        grads = L_model_backward(AL, Y, caches)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate=0.0075)\n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    return parameters\n```\n\n## 预测\n\n```python\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n\n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n\n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    m = X.shape[1]\n    p = np.zeros((1, m))\n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0, i] > 0.5:\n            p[0, i] = 1\n        else:\n            p[0, i] = 0\n    print(\"Accuracy: \" + str(np.sum((p == y) / m)))\n    return p\n```\n","slug":"神经网络中的通用函数代码","published":1,"updated":"2018-08-31T03:55:23.326Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh61002zzkvodd3d1q3b","content":"<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><h3 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements the sigmoid activation in numpy</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- numpy array of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    s = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    dZ = dA * s * (<span class=\"number\">1</span> - s)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h3 id=\"Relu\"><a href=\"#Relu\" class=\"headerlink\" title=\"Relu\"></a>Relu</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the RELU function.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- Output of the linear layer, of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = np.maximum(<span class=\"number\">0</span>, Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(A.shape == Z.shape)</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single RELU unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    dZ = np.array(dA, copy=<span class=\"keyword\">True</span>)  <span class=\"comment\"># just converting dz to a correct object.</span></span><br><span class=\"line\">    <span class=\"comment\"># When z &lt;= 0, you should set dz to 0 as well.</span></span><br><span class=\"line\">    dZ[Z &lt;= <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h2 id=\"代价函数\"><a href=\"#代价函数\" class=\"headerlink\" title=\"代价函数\"></a>代价函数</h2><h3 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a>交叉熵</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_cost</span><span class=\"params\">(AL, Y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the cost function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    cost -- cross-entropy cost</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = Y.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Compute loss from aL and y.</span></span><br><span class=\"line\">    cost = <span class=\"number\">-1</span> / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(<span class=\"number\">1</span> - Y, np.log(<span class=\"number\">1</span> - AL).T))</span><br><span class=\"line\">    cost = np.squeeze(cost)      <span class=\"comment\"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(cost.shape == ())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cost</span><br></pre></td></tr></table></figure>\n<h3 id=\"L1-and-L2-Loss\"><a href=\"#L1-and-L2-Loss\" class=\"headerlink\" title=\"L1 and L2 Loss\"></a>L1 and L2 Loss</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L1</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L1 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum(abs(yhat - y))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L2</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L2 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum((y - yhat) ** <span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性函数\"><a href=\"#线性函数\" class=\"headerlink\" title=\"线性函数\"></a>线性函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_forward</span><span class=\"params\">(A, W, b)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear part of a layer's forward propagation.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- the input of the activation function, also called pre-activation parameter</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = np.dot(W, A) + b</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(Z.shape == (W.shape[<span class=\"number\">0</span>], A.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (A, W, b)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Z, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_backward</span><span class=\"params\">(dZ, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A_prev, W, b = cache</span><br><span class=\"line\">    m = A_prev.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    dW = <span class=\"number\">1</span> / m * np.dot(dZ, A_prev.T)</span><br><span class=\"line\">    db = <span class=\"number\">1</span> / m * np.sum(dZ, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    dA_prev = np.dot(W.T, dZ)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dW.shape == W.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (db.shape == b.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性到激活层\"><a href=\"#线性到激活层\" class=\"headerlink\" title=\"线性到激活层\"></a>线性到激活层</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_forward</span><span class=\"params\">(A_prev, W, b, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- the output of the activation function, also called the post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";</span></span><br><span class=\"line\"><span class=\"string\">             stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = sigmoid(Z)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = relu(Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (A.shape == (W.shape[<span class=\"number\">0</span>], A_prev.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (linear_cache, activation_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_backward</span><span class=\"params\">(dA, cache, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient for current layer l</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    linear_cache, activation_cache = cache</span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        dZ = relu_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"L层前馈网络\"><a href=\"#L层前馈网络\" class=\"headerlink\" title=\"L层前馈网络\"></a>L层前馈网络</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_forward</span><span class=\"params\">(X, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- output of initialize_parameters_deep()</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- last post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    caches = []</span><br><span class=\"line\">    A = X</span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        A_prev = A</span><br><span class=\"line\">        A, cache = linear_activation_forward(A_prev, parameters[<span class=\"string\">'W'</span> + str(l)], parameters[<span class=\"string\">'b'</span> + str(l)], <span class=\"string\">\"relu\"</span>)</span><br><span class=\"line\">        caches.append(cache)</span><br><span class=\"line\">    <span class=\"comment\"># Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    AL, cache = linear_activation_forward(A, parameters[<span class=\"string\">'W'</span> + str(L)], parameters[<span class=\"string\">'b'</span> + str(L)], <span class=\"string\">\"sigmoid\"</span>)    <span class=\"comment\"># 注意这里是 A</span></span><br><span class=\"line\">    caches.append(cache)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(AL.shape == (<span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> AL, caches</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_backward</span><span class=\"params\">(AL, Y, caches)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    grads -- A dictionary with the gradients</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dA\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    grads = &#123;&#125;</span><br><span class=\"line\">    L = len(caches)  <span class=\"comment\"># the number of layers</span></span><br><span class=\"line\">    <span class=\"comment\"># m = AL.shape[1]</span></span><br><span class=\"line\">    Y = Y.reshape(AL.shape)  <span class=\"comment\"># after this line, Y is the same shape as AL</span></span><br><span class=\"line\">    <span class=\"comment\"># Initializing the backpropagation</span></span><br><span class=\"line\">    dAL = - (np.divide(Y, AL) - np.divide(<span class=\"number\">1</span> - Y, <span class=\"number\">1</span> - AL))</span><br><span class=\"line\">    <span class=\"comment\"># Lth layer (SIGMOID -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">    current_cache = caches[L - <span class=\"number\">1</span>]</span><br><span class=\"line\">    grads[<span class=\"string\">\"dA\"</span> + str(L)], grads[<span class=\"string\">\"dW\"</span> + str(L)], grads[<span class=\"string\">\"db\"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class=\"string\">'sigmoid'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> reversed(range(L - <span class=\"number\">1</span>)):</span><br><span class=\"line\">        <span class=\"comment\"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">        current_cache = caches[l]</span><br><span class=\"line\">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">2</span>)], caches[l], <span class=\"string\">'relu'</span>)</span><br><span class=\"line\">        grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">1</span>)] = dA_prev_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)] = dW_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)] = db_temp</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grads</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数初始化\"><a href=\"#参数初始化\" class=\"headerlink\" title=\"参数初始化\"></a>参数初始化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_parameters_deep</span><span class=\"params\">(layer_dims)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":</span></span><br><span class=\"line\"><span class=\"string\">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class=\"line\"><span class=\"string\">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">3</span>)</span><br><span class=\"line\">    parameters = &#123;&#125;</span><br><span class=\"line\">    L = len(layer_dims)            <span class=\"comment\"># number of layers in the network</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">2</span> / layer_dims[l - <span class=\"number\">1</span>])  <span class=\"comment\"># He initialization</span></span><br><span class=\"line\">        parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'b'</span> + str(l)].shape == (layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数更新（梯度下降）\"><a href=\"#参数更新（梯度下降）\" class=\"headerlink\" title=\"参数更新（梯度下降）\"></a>参数更新（梯度下降）</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters</span><span class=\"params\">(parameters, grads, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using gradient descent</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"W\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"b\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Update rule for each parameter. Use a for loop.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"预测\"><a href=\"#预测\" class=\"headerlink\" title=\"预测\"></a>预测</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(X, y, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    This function is used to predict the results of a  L-layer neural network.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data set of examples you would like to label</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters of the trained model</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    p -- predictions for the given dataset X</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    p = np.zeros((<span class=\"number\">1</span>, m))</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    probas, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># convert probas to 0/1 predictions</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, probas.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> probas[<span class=\"number\">0</span>, i] &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">0</span></span><br><span class=\"line\">    print(<span class=\"string\">\"Accuracy: \"</span> + str(np.sum((p == y) / m)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><h3 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements the sigmoid activation in numpy</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- numpy array of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sigmoid_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    s = <span class=\"number\">1</span> / (<span class=\"number\">1</span> + np.exp(-Z))</span><br><span class=\"line\">    dZ = dA * s * (<span class=\"number\">1</span> - s)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h3 id=\"Relu\"><a href=\"#Relu\" class=\"headerlink\" title=\"Relu\"></a>Relu</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu</span><span class=\"params\">(Z)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the RELU function.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- Output of the linear layer, of any shape</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A = np.maximum(<span class=\"number\">0</span>, Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(A.shape == Z.shape)</span><br><span class=\"line\">    cache = Z</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu_backward</span><span class=\"params\">(dA, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for a single RELU unit.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient, of any shape</span></span><br><span class=\"line\"><span class=\"string\">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = cache</span><br><span class=\"line\">    dZ = np.array(dA, copy=<span class=\"keyword\">True</span>)  <span class=\"comment\"># just converting dz to a correct object.</span></span><br><span class=\"line\">    <span class=\"comment\"># When z &lt;= 0, you should set dz to 0 as well.</span></span><br><span class=\"line\">    dZ[Z &lt;= <span class=\"number\">0</span>] = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dZ.shape == Z.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dZ</span><br></pre></td></tr></table></figure>\n<h2 id=\"代价函数\"><a href=\"#代价函数\" class=\"headerlink\" title=\"代价函数\"></a>代价函数</h2><h3 id=\"交叉熵\"><a href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"></a>交叉熵</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">compute_cost</span><span class=\"params\">(AL, Y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the cost function</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    cost -- cross-entropy cost</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = Y.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    <span class=\"comment\"># Compute loss from aL and y.</span></span><br><span class=\"line\">    cost = <span class=\"number\">-1</span> / m * np.sum(np.dot(Y, np.log(AL).T) + np.dot(<span class=\"number\">1</span> - Y, np.log(<span class=\"number\">1</span> - AL).T))</span><br><span class=\"line\">    cost = np.squeeze(cost)      <span class=\"comment\"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(cost.shape == ())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cost</span><br></pre></td></tr></table></figure>\n<h3 id=\"L1-and-L2-Loss\"><a href=\"#L1-and-L2-Loss\" class=\"headerlink\" title=\"L1 and L2 Loss\"></a>L1 and L2 Loss</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L1</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L1 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum(abs(yhat - y))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L2</span><span class=\"params\">(yhat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    yhat -- vector of size m (predicted labels)</span></span><br><span class=\"line\"><span class=\"string\">    y -- vector of size m (true labels)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    loss -- the value of the L2 loss function defined above</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    loss = np.sum((y - yhat) ** <span class=\"number\">2</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> loss</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性函数\"><a href=\"#线性函数\" class=\"headerlink\" title=\"线性函数\"></a>线性函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_forward</span><span class=\"params\">(A, W, b)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear part of a layer's forward propagation.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    Z -- the input of the activation function, also called pre-activation parameter</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    Z = np.dot(W, A) + b</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(Z.shape == (W.shape[<span class=\"number\">0</span>], A.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (A, W, b)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> Z, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_backward</span><span class=\"params\">(dZ, cache)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    A_prev, W, b = cache</span><br><span class=\"line\">    m = A_prev.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    dW = <span class=\"number\">1</span> / m * np.dot(dZ, A_prev.T)</span><br><span class=\"line\">    db = <span class=\"number\">1</span> / m * np.sum(dZ, axis=<span class=\"number\">1</span>, keepdims=<span class=\"keyword\">True</span>)</span><br><span class=\"line\">    dA_prev = np.dot(W.T, dZ)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (dW.shape == W.shape)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (db.shape == b.shape)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"线性到激活层\"><a href=\"#线性到激活层\" class=\"headerlink\" title=\"线性到激活层\"></a>线性到激活层</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_forward</span><span class=\"params\">(A_prev, W, b, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class=\"line\"><span class=\"string\">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    A -- the output of the activation function, also called the post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";</span></span><br><span class=\"line\"><span class=\"string\">             stored for computing the backward pass efficiently</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = sigmoid(Z)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class=\"line\">        A, activation_cache = relu(Z)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> (A.shape == (W.shape[<span class=\"number\">0</span>], A_prev.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    cache = (linear_cache, activation_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> A, cache</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linear_activation_backward</span><span class=\"params\">(dA, cache, activation)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    dA -- post-activation gradient for current layer l</span></span><br><span class=\"line\"><span class=\"string\">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class=\"line\"><span class=\"string\">    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class=\"line\"><span class=\"string\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class=\"line\"><span class=\"string\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    linear_cache, activation_cache = cache</span><br><span class=\"line\">    <span class=\"keyword\">if</span> activation == <span class=\"string\">\"relu\"</span>:</span><br><span class=\"line\">        dZ = relu_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">elif</span> activation == <span class=\"string\">\"sigmoid\"</span>:</span><br><span class=\"line\">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class=\"line\">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>\n<h2 id=\"L层前馈网络\"><a href=\"#L层前馈网络\" class=\"headerlink\" title=\"L层前馈网络\"></a>L层前馈网络</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_forward</span><span class=\"params\">(X, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- output of initialize_parameters_deep()</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- last post-activation value</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    caches = []</span><br><span class=\"line\">    A = X</span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>                  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Implement [LINEAR -&gt; RELU]*(L-1). Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        A_prev = A</span><br><span class=\"line\">        A, cache = linear_activation_forward(A_prev, parameters[<span class=\"string\">'W'</span> + str(l)], parameters[<span class=\"string\">'b'</span> + str(l)], <span class=\"string\">\"relu\"</span>)</span><br><span class=\"line\">        caches.append(cache)</span><br><span class=\"line\">    <span class=\"comment\"># Implement LINEAR -&gt; SIGMOID. Add \"cache\" to the \"caches\" list.</span></span><br><span class=\"line\">    AL, cache = linear_activation_forward(A, parameters[<span class=\"string\">'W'</span> + str(L)], parameters[<span class=\"string\">'b'</span> + str(L)], <span class=\"string\">\"sigmoid\"</span>)    <span class=\"comment\"># 注意这里是 A</span></span><br><span class=\"line\">    caches.append(cache)</span><br><span class=\"line\">    <span class=\"keyword\">assert</span>(AL.shape == (<span class=\"number\">1</span>, X.shape[<span class=\"number\">1</span>]))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> AL, caches</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_model_backward</span><span class=\"params\">(AL, Y, caches)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class=\"line\"><span class=\"string\">    caches -- list of caches containing:</span></span><br><span class=\"line\"><span class=\"string\">                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class=\"line\"><span class=\"string\">                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    grads -- A dictionary with the gradients</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dA\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"dW\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">             grads[\"db\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    grads = &#123;&#125;</span><br><span class=\"line\">    L = len(caches)  <span class=\"comment\"># the number of layers</span></span><br><span class=\"line\">    <span class=\"comment\"># m = AL.shape[1]</span></span><br><span class=\"line\">    Y = Y.reshape(AL.shape)  <span class=\"comment\"># after this line, Y is the same shape as AL</span></span><br><span class=\"line\">    <span class=\"comment\"># Initializing the backpropagation</span></span><br><span class=\"line\">    dAL = - (np.divide(Y, AL) - np.divide(<span class=\"number\">1</span> - Y, <span class=\"number\">1</span> - AL))</span><br><span class=\"line\">    <span class=\"comment\"># Lth layer (SIGMOID -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">    current_cache = caches[L - <span class=\"number\">1</span>]</span><br><span class=\"line\">    grads[<span class=\"string\">\"dA\"</span> + str(L)], grads[<span class=\"string\">\"dW\"</span> + str(L)], grads[<span class=\"string\">\"db\"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class=\"string\">'sigmoid'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> reversed(range(L - <span class=\"number\">1</span>)):</span><br><span class=\"line\">        <span class=\"comment\"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class=\"line\">        current_cache = caches[l]</span><br><span class=\"line\">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">2</span>)], caches[l], <span class=\"string\">'relu'</span>)</span><br><span class=\"line\">        grads[<span class=\"string\">\"dA\"</span> + str(l + <span class=\"number\">1</span>)] = dA_prev_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)] = dW_temp</span><br><span class=\"line\">        grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)] = db_temp</span><br><span class=\"line\">    <span class=\"keyword\">return</span> grads</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数初始化\"><a href=\"#参数初始化\" class=\"headerlink\" title=\"参数初始化\"></a>参数初始化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">initialize_parameters_deep</span><span class=\"params\">(layer_dims)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":</span></span><br><span class=\"line\"><span class=\"string\">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class=\"line\"><span class=\"string\">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    np.random.seed(<span class=\"number\">3</span>)</span><br><span class=\"line\">    parameters = &#123;&#125;</span><br><span class=\"line\">    L = len(layer_dims)            <span class=\"comment\"># number of layers in the network</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, L):</span><br><span class=\"line\">        parameters[<span class=\"string\">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]) * np.sqrt(<span class=\"number\">2</span> / layer_dims[l - <span class=\"number\">1</span>])  <span class=\"comment\"># He initialization</span></span><br><span class=\"line\">        parameters[<span class=\"string\">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l - <span class=\"number\">1</span>]))</span><br><span class=\"line\">        <span class=\"keyword\">assert</span>(parameters[<span class=\"string\">'b'</span> + str(l)].shape == (layer_dims[l], <span class=\"number\">1</span>))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"参数更新（梯度下降）\"><a href=\"#参数更新（梯度下降）\" class=\"headerlink\" title=\"参数更新（梯度下降）\"></a>参数更新（梯度下降）</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">update_parameters</span><span class=\"params\">(parameters, grads, learning_rate)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Update parameters using gradient descent</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your parameters</span></span><br><span class=\"line\"><span class=\"string\">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- python dictionary containing your updated parameters</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"W\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">                  parameters[\"b\" + str(l)] = ...</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    L = len(parameters) // <span class=\"number\">2</span>  <span class=\"comment\"># number of layers in the neural network</span></span><br><span class=\"line\">    <span class=\"comment\"># Update rule for each parameter. Use a for loop.</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(L):</span><br><span class=\"line\">        parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"W\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"dW\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">        parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] = parameters[<span class=\"string\">\"b\"</span> + str(l + <span class=\"number\">1</span>)] - learning_rate * grads[<span class=\"string\">\"db\"</span> + str(l + <span class=\"number\">1</span>)]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">L_layer_model</span><span class=\"params\">(X, Y, layers_dims, learning_rate=<span class=\"number\">0.0075</span>, num_iterations=<span class=\"number\">3000</span>, print_cost=False)</span>:</span>  <span class=\"comment\"># lr was 0.009</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class=\"line\"><span class=\"string\">    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class=\"line\"><span class=\"string\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class=\"line\"><span class=\"string\">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class=\"line\"><span class=\"string\">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class=\"line\"><span class=\"string\">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    costs = []                         <span class=\"comment\"># keep track of cost</span></span><br><span class=\"line\">    <span class=\"comment\"># Parameters initialization.</span></span><br><span class=\"line\">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop (gradient descent)</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_iterations):</span><br><span class=\"line\">        <span class=\"comment\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class=\"line\">        AL, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">        <span class=\"comment\"># Compute cost.</span></span><br><span class=\"line\">        cost = compute_cost(AL, Y)</span><br><span class=\"line\">        <span class=\"comment\"># Backward propagation.</span></span><br><span class=\"line\">        grads = L_model_backward(AL, Y, caches)</span><br><span class=\"line\">        <span class=\"comment\"># Update parameters.</span></span><br><span class=\"line\">        parameters = update_parameters(parameters, grads, learning_rate=<span class=\"number\">0.0075</span>)</span><br><span class=\"line\">        <span class=\"comment\"># Print the cost every 100 training example</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Cost after iteration %i: %f\"</span> % (i, cost))</span><br><span class=\"line\">        <span class=\"keyword\">if</span> print_cost <span class=\"keyword\">and</span> i % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            costs.append(cost)</span><br><span class=\"line\">    <span class=\"comment\"># plot the cost</span></span><br><span class=\"line\">    plt.plot(np.squeeze(costs))</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">'cost'</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">'iterations (per tens)'</span>)</span><br><span class=\"line\">    plt.title(<span class=\"string\">\"Learning rate =\"</span> + str(learning_rate))</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> parameters</span><br></pre></td></tr></table></figure>\n<h2 id=\"预测\"><a href=\"#预测\" class=\"headerlink\" title=\"预测\"></a>预测</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(X, y, parameters)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    This function is used to predict the results of a  L-layer neural network.</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Arguments:</span></span><br><span class=\"line\"><span class=\"string\">    X -- data set of examples you would like to label</span></span><br><span class=\"line\"><span class=\"string\">    parameters -- parameters of the trained model</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    Returns:</span></span><br><span class=\"line\"><span class=\"string\">    p -- predictions for the given dataset X</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    m = X.shape[<span class=\"number\">1</span>]</span><br><span class=\"line\">    p = np.zeros((<span class=\"number\">1</span>, m))</span><br><span class=\"line\">    <span class=\"comment\"># Forward propagation</span></span><br><span class=\"line\">    probas, caches = L_model_forward(X, parameters)</span><br><span class=\"line\">    <span class=\"comment\"># convert probas to 0/1 predictions</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, probas.shape[<span class=\"number\">1</span>]):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> probas[<span class=\"number\">0</span>, i] &gt; <span class=\"number\">0.5</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">1</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            p[<span class=\"number\">0</span>, i] = <span class=\"number\">0</span></span><br><span class=\"line\">    print(<span class=\"string\">\"Accuracy: \"</span> + str(np.sum((p == y) / m)))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p</span><br></pre></td></tr></table></figure>\n"},{"title":"贞洁禁忌","date":"2018-07-21T07:31:50.000Z","_content":"## 现象\n\n**处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。**史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。\n\n## 解释\n\n### 对血的恐惧\n\n贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。\n\n### 对新事物的恐惧\n\n正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。\n\n### 贞洁禁忌是整个性生活的一部分\n\n不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。\n\n只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果--卸载紧张、引发身体疲软--或许是男人惧怕女人的原型。\n\n## 结论\n\n在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。\n\n十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。\n","source":"_posts/贞洁禁忌.md","raw":"---\ntitle: 贞洁禁忌\ndate: 2018-07-21 15:31:50\ntags: 爱情心理学\ncategories: 心理学\n---\n## 现象\n\n**处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。**史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。\n\n## 解释\n\n### 对血的恐惧\n\n贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。\n\n### 对新事物的恐惧\n\n正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。\n\n### 贞洁禁忌是整个性生活的一部分\n\n不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。\n\n只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果--卸载紧张、引发身体疲软--或许是男人惧怕女人的原型。\n\n## 结论\n\n在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。\n\n十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。\n","slug":"贞洁禁忌","published":1,"updated":"2018-08-31T03:55:27.511Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjlhgwh650032zkvo4xlt3bwd","content":"<h2 id=\"现象\"><a href=\"#现象\" class=\"headerlink\" title=\"现象\"></a>现象</h2><p><strong>处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。</strong>史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。</p>\n<h2 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h2><h3 id=\"对血的恐惧\"><a href=\"#对血的恐惧\" class=\"headerlink\" title=\"对血的恐惧\"></a>对血的恐惧</h3><p>贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。</p>\n<h3 id=\"对新事物的恐惧\"><a href=\"#对新事物的恐惧\" class=\"headerlink\" title=\"对新事物的恐惧\"></a>对新事物的恐惧</h3><p>正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。</p>\n<h3 id=\"贞洁禁忌是整个性生活的一部分\"><a href=\"#贞洁禁忌是整个性生活的一部分\" class=\"headerlink\" title=\"贞洁禁忌是整个性生活的一部分\"></a>贞洁禁忌是整个性生活的一部分</h3><p>不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。</p>\n<p>只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果–卸载紧张、引发身体疲软–或许是男人惧怕女人的原型。</p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。</p>\n<p>十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"现象\"><a href=\"#现象\" class=\"headerlink\" title=\"现象\"></a>现象</h2><p><strong>处女不能将贞洁保留给新郎或未来的伴侣，习俗上要求新郎避开使处女失贞这一行为。</strong>史实记载在澳大利亚的Dieri部落,女孩到青春期破处是普遍的习俗。</p>\n<h2 id=\"解释\"><a href=\"#解释\" class=\"headerlink\" title=\"解释\"></a>解释</h2><h3 id=\"对血的恐惧\"><a href=\"#对血的恐惧\" class=\"headerlink\" title=\"对血的恐惧\"></a>对血的恐惧</h3><p>贞洁禁忌与普遍存在的月经禁忌有关。面对每月流血这一令人迷惑的现象，原始人无法把它与施虐观点相连，而被解释为某种鬼怪咬了女孩。</p>\n<h3 id=\"对新事物的恐惧\"><a href=\"#对新事物的恐惧\" class=\"headerlink\" title=\"对新事物的恐惧\"></a>对新事物的恐惧</h3><p>正如精神分析理论所研究的焦虑神经症一样，原始人也长期受到潜在忧虑的影响。在不同寻常的场合，这种忧虑尤其强烈，包括遇到新事物或预料之外的情况、无法理解或神秘之事。这也是各种仪式的根源，后来被宗教广泛采用。威胁焦虑者的危险只会在他自己期待中生动上演，而非在真实的危险情境中。因此，婚姻中首行性事之前做些预防措施十分重要。</p>\n<h3 id=\"贞洁禁忌是整个性生活的一部分\"><a href=\"#贞洁禁忌是整个性生活的一部分\" class=\"headerlink\" title=\"贞洁禁忌是整个性生活的一部分\"></a>贞洁禁忌是整个性生活的一部分</h3><p>不仅与女人的第一次性交是禁忌，而且性交总体上就是个禁忌。在多数情况下，原始人的性生活被各种禁止强有力地约束着，并不像在文明社会中已达到的较高水平。当原始人从事重要活动，如出发探索周围环境，狩猎或参加战役时，就必须远离自己的妻子，尤其不能与她性交。</p>\n<p>只要原始人设立禁忌，就意味着惧怕事物，不容争议的是，所有回避女人的规定都表达了对女人整体的恐惧。或许这种恐惧建立在男女不同的事实上，即女人永远都无法令人理解、神秘且陌生，因此明显地与男人敌对。男人害怕受女性气质的感染，害怕变得像女人一样脆弱无能。性交的效果–卸载紧张、引发身体疲软–或许是男人惧怕女人的原型。</p>\n<h2 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h2><p>在文明社会中，“失贞”不仅会长久地把女性束缚在男性身上，还会从女性身上释放出对男性的敌意反应。女性对男性的敌意反应可以采取病态形式，并常常表现为婚后对性生活的抑制，同时，我们也可以把第二段婚姻比第一段婚姻更美满的原因归结于此。</p>\n<p>十分有趣的是，精神分析家还会遇到这样的女人：她们心中同时存在着归属于敌意两种冲动，而且着两种冲动彼此间的联系十分紧密。她们可能看起来完全离开了丈夫，但心里还会受到丈夫的影响。当她试图爱上别的男人时，前任的形象就会冒出来干扰，对她的新爱情产生抑制效果。分析表明，这类女人确实还与前任有联结，尽管这不是一种情感联结。她们之所以离不开前任，是因为自己还没有完成复仇计划。</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjlhgwh0q0000zkvor5zlsqia","category_id":"cjlhgwh1e0004zkvo52fipkqj","_id":"cjlhgwh29000ezkvoqixytbiy"},{"post_id":"cjlhgwh150002zkvo5xrne7jg","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh2j000kzkvoq6e5y7p8"},{"post_id":"cjlhgwh22000dzkvojp09hnqb","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh2q000ozkvo7krzsmnc"},{"post_id":"cjlhgwh2c000hzkvo2nqvzqlu","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh2v000rzkvoitjq8p8g"},{"post_id":"cjlhgwh1m0006zkvopbc79g5w","category_id":"cjlhgwh2a000fzkvoeie1esf2","_id":"cjlhgwh30000vzkvolbebf7r8"},{"post_id":"cjlhgwh2h000jzkvoj92ty2ho","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh3j000yzkvo1oq3dswb"},{"post_id":"cjlhgwh1q0007zkvoha9k7jtz","category_id":"cjlhgwh2a000fzkvoeie1esf2","_id":"cjlhgwh3p0013zkvooklgefn1"},{"post_id":"cjlhgwh2y000uzkvopzwrhw7y","category_id":"cjlhgwh1e0004zkvo52fipkqj","_id":"cjlhgwh3t0016zkvo9ef10mh1"},{"post_id":"cjlhgwh1t0008zkvoibplber8","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh3y001azkvolwpyzp4i"},{"post_id":"cjlhgwh3m0011zkvomzak4ys6","category_id":"cjlhgwh1e0004zkvo52fipkqj","_id":"cjlhgwh42001ezkvorwik6gn1"},{"post_id":"cjlhgwh1y000czkvo3sbh0wnu","category_id":"cjlhgwh3k000zzkvopenw2xna","_id":"cjlhgwh47001izkvo7w006svp"},{"post_id":"cjlhgwh3w0019zkvozm965mug","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh4b001mzkvonieuvroz"},{"post_id":"cjlhgwh2o000nzkvo48z7y4wk","category_id":"cjlhgwh3w0018zkvoo8nvly4q","_id":"cjlhgwh4h001qzkvoni36jqmc"},{"post_id":"cjlhgwh49001lzkvo1qz0qam9","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh4o001wzkvo75firtaa"},{"post_id":"cjlhgwh2s000qzkvoulr3be2r","category_id":"cjlhgwh3k000zzkvopenw2xna","_id":"cjlhgwh4t001zzkvopnpy03a6"},{"post_id":"cjlhgwh31000xzkvoxs8df41n","category_id":"cjlhgwh3w0018zkvoo8nvly4q","_id":"cjlhgwh4z0023zkvou05t16rr"},{"post_id":"cjlhgwh4m001vzkvor5qf3p8u","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh540026zkvodrrz92u6"},{"post_id":"cjlhgwh4q001yzkvonjkru0pe","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh58002azkvoln96qk2b"},{"post_id":"cjlhgwh3r0015zkvo68hfkm7c","category_id":"cjlhgwh4p001xzkvoydm8tglv","_id":"cjlhgwh5d002ezkvo180hipds"},{"post_id":"cjlhgwh4x0022zkvob6xght3c","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh5h002izkvon4v0qhzk"},{"post_id":"cjlhgwh510025zkvok9x6lwgd","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh5l002mzkvoq3dgqee8"},{"post_id":"cjlhgwh40001dzkvozvwz8kqf","category_id":"cjlhgwh4z0024zkvos6tyxq4w","_id":"cjlhgwh5p002qzkvoxzi2qsvz"},{"post_id":"cjlhgwh5b002dzkvoa1339jfq","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh5v002tzkvon3fyuk11"},{"post_id":"cjlhgwh4d001ozkvoksssjkum","category_id":"cjlhgwh5a002bzkvo5cg8wr05","_id":"cjlhgwh5z002wzkvo2yxwnqvl"},{"post_id":"cjlhgwh5f002hzkvohf9e76a1","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh630030zkvo87ebrq6x"},{"post_id":"cjlhgwh5j002lzkvowgp5nb6k","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh6b0033zkvo9u4nectv"},{"post_id":"cjlhgwh5n002pzkvopy2jr5jc","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh6d0035zkvolcdotkpd"},{"post_id":"cjlhgwh4j001tzkvoa21ov879","category_id":"cjlhgwh5i002kzkvoj73roypo","_id":"cjlhgwh6e0037zkvoowotxl50"},{"post_id":"cjlhgwh5s002szkvoyghay2nv","category_id":"cjlhgwh5i002kzkvoj73roypo","_id":"cjlhgwh6g0039zkvoi71scrm6"},{"post_id":"cjlhgwh5w002vzkvo3hxzy07d","category_id":"cjlhgwh4p001xzkvoydm8tglv","_id":"cjlhgwh6h003bzkvo76o33h1r"},{"post_id":"cjlhgwh61002zzkvodd3d1q3b","category_id":"cjlhgwh1v0009zkvoffdyomm8","_id":"cjlhgwh6i003dzkvoj0k6ee7u"},{"post_id":"cjlhgwh650032zkvo4xlt3bwd","category_id":"cjlhgwh5i002kzkvoj73roypo","_id":"cjlhgwh6k003fzkvotg7ei9vz"}],"PostTag":[{"post_id":"cjlhgwh0q0000zkvor5zlsqia","tag_id":"cjlhgwh1l0005zkvooinjwndv","_id":"cjlhgwh1x000bzkvoqf2620wj"},{"post_id":"cjlhgwh150002zkvo5xrne7jg","tag_id":"cjlhgwh1w000azkvotnsg5isr","_id":"cjlhgwh2g000izkvow3v87d9m"},{"post_id":"cjlhgwh1m0006zkvopbc79g5w","tag_id":"cjlhgwh2a000gzkvo3csq28tp","_id":"cjlhgwh2r000pzkvoar1zv2a2"},{"post_id":"cjlhgwh1q0007zkvoha9k7jtz","tag_id":"cjlhgwh2k000mzkvopyztgkxr","_id":"cjlhgwh31000wzkvo1vfh5c2v"},{"post_id":"cjlhgwh2y000uzkvopzwrhw7y","tag_id":"cjlhgwh1l0005zkvooinjwndv","_id":"cjlhgwh3l0010zkvo24359o29"},{"post_id":"cjlhgwh31000xzkvoxs8df41n","tag_id":"cjlhgwh1l0005zkvooinjwndv","_id":"cjlhgwh3q0014zkvo4gxhgfgi"},{"post_id":"cjlhgwh1t0008zkvoibplber8","tag_id":"cjlhgwh2x000tzkvo2zrdncev","_id":"cjlhgwh3v0017zkvokz68n83o"},{"post_id":"cjlhgwh3m0011zkvomzak4ys6","tag_id":"cjlhgwh1l0005zkvooinjwndv","_id":"cjlhgwh40001czkvofofzdsp9"},{"post_id":"cjlhgwh1y000czkvo3sbh0wnu","tag_id":"cjlhgwh3o0012zkvoq5tfw9xy","_id":"cjlhgwh43001fzkvooii5a9wc"},{"post_id":"cjlhgwh3w0019zkvozm965mug","tag_id":"cjlhgwh2x000tzkvo2zrdncev","_id":"cjlhgwh48001kzkvovesk1oqs"},{"post_id":"cjlhgwh22000dzkvojp09hnqb","tag_id":"cjlhgwh3z001bzkvoq6ear6h2","_id":"cjlhgwh4c001nzkvohnv3k8lj"},{"post_id":"cjlhgwh49001lzkvo1qz0qam9","tag_id":"cjlhgwh3z001bzkvoq6ear6h2","_id":"cjlhgwh4i001rzkvolanqz0ck"},{"post_id":"cjlhgwh2c000hzkvo2nqvzqlu","tag_id":"cjlhgwh48001jzkvo11le0xgh","_id":"cjlhgwh4m001uzkvoeou1bdd7"},{"post_id":"cjlhgwh2h000jzkvoj92ty2ho","tag_id":"cjlhgwh4i001szkvonzutke3q","_id":"cjlhgwh4w0021zkvokroabt53"},{"post_id":"cjlhgwh4x0022zkvob6xght3c","tag_id":"cjlhgwh2x000tzkvo2zrdncev","_id":"cjlhgwh560028zkvoz0ryu9mo"},{"post_id":"cjlhgwh2o000nzkvo48z7y4wk","tag_id":"cjlhgwh4v0020zkvo9it9hfmd","_id":"cjlhgwh5b002czkvozp9yqbdi"},{"post_id":"cjlhgwh510025zkvok9x6lwgd","tag_id":"cjlhgwh2x000tzkvo2zrdncev","_id":"cjlhgwh5f002gzkvo9ioog9yr"},{"post_id":"cjlhgwh560029zkvo8cwnvc14","tag_id":"cjlhgwh2x000tzkvo2zrdncev","_id":"cjlhgwh5i002jzkvo14ysh149"},{"post_id":"cjlhgwh3r0015zkvo68hfkm7c","tag_id":"cjlhgwh550027zkvojqa4l64o","_id":"cjlhgwh5m002nzkvosw77jegb"},{"post_id":"cjlhgwh40001dzkvozvwz8kqf","tag_id":"cjlhgwh5e002fzkvodlqipjmz","_id":"cjlhgwh5r002rzkvopknom5lr"},{"post_id":"cjlhgwh5j002lzkvowgp5nb6k","tag_id":"cjlhgwh3z001bzkvoq6ear6h2","_id":"cjlhgwh5w002uzkvoiqttosq3"},{"post_id":"cjlhgwh5n002pzkvopy2jr5jc","tag_id":"cjlhgwh2x000tzkvo2zrdncev","_id":"cjlhgwh61002yzkvogfl3pg18"},{"post_id":"cjlhgwh43001gzkvoibct7v8u","tag_id":"cjlhgwh5n002ozkvodi9u6hzp","_id":"cjlhgwh640031zkvo4zmwtkw6"},{"post_id":"cjlhgwh4d001ozkvoksssjkum","tag_id":"cjlhgwh5z002xzkvozs1kre6r","_id":"cjlhgwh6d0036zkvo0wqg0gx2"},{"post_id":"cjlhgwh4j001tzkvoa21ov879","tag_id":"cjlhgwh6c0034zkvodi3w0hhi","_id":"cjlhgwh6g003azkvo74p1b0ah"},{"post_id":"cjlhgwh4m001vzkvor5qf3p8u","tag_id":"cjlhgwh6f0038zkvodap05ev9","_id":"cjlhgwh6j003ezkvo1s7o85mr"},{"post_id":"cjlhgwh4q001yzkvonjkru0pe","tag_id":"cjlhgwh6f0038zkvodap05ev9","_id":"cjlhgwh6l003hzkvowqrlpvzv"},{"post_id":"cjlhgwh5b002dzkvoa1339jfq","tag_id":"cjlhgwh6k003gzkvoheu8yg98","_id":"cjlhgwh6m003jzkvoct7cvhnl"},{"post_id":"cjlhgwh5f002hzkvohf9e76a1","tag_id":"cjlhgwh6m003izkvo5uxznh7r","_id":"cjlhgwh6n003lzkvo1149tmsb"},{"post_id":"cjlhgwh5s002szkvoyghay2nv","tag_id":"cjlhgwh6c0034zkvodi3w0hhi","_id":"cjlhgwh6o003nzkvoi6l2dt83"},{"post_id":"cjlhgwh5w002vzkvo3hxzy07d","tag_id":"cjlhgwh6o003mzkvo2fvczaq1","_id":"cjlhgwh6p003pzkvobe4wmysv"},{"post_id":"cjlhgwh61002zzkvodd3d1q3b","tag_id":"cjlhgwh6p003ozkvoqtvs8guo","_id":"cjlhgwh6r003rzkvoqczmtmk2"},{"post_id":"cjlhgwh650032zkvo4xlt3bwd","tag_id":"cjlhgwh6c0034zkvodi3w0hhi","_id":"cjlhgwh6s003szkvon4vrukt6"}],"Tag":[{"name":"python","_id":"cjlhgwh1l0005zkvooinjwndv"},{"name":"DNN","_id":"cjlhgwh1w000azkvotnsg5isr"},{"name":"进化算法","_id":"cjlhgwh2a000gzkvo3csq28tp"},{"name":"遗传算法","_id":"cjlhgwh2k000mzkvopyztgkxr"},{"name":"优化算法","_id":"cjlhgwh2x000tzkvo2zrdncev"},{"name":"hexo","_id":"cjlhgwh3o0012zkvoq5tfw9xy"},{"name":"CNN","_id":"cjlhgwh3z001bzkvoq6ear6h2"},{"name":"CNN, VGG","_id":"cjlhgwh48001jzkvo11le0xgh"},{"name":"dropout","_id":"cjlhgwh4i001szkvonzutke3q"},{"name":"git","_id":"cjlhgwh4v0020zkvo9it9hfmd"},{"name":"路遥","_id":"cjlhgwh550027zkvojqa4l64o"},{"name":"动态规划","_id":"cjlhgwh5e002fzkvodlqipjmz"},{"name":"策略游戏","_id":"cjlhgwh5n002ozkvodi9u6hzp"},{"name":"物理","_id":"cjlhgwh5z002xzkvozs1kre6r"},{"name":"爱情心理学","_id":"cjlhgwh6c0034zkvodi3w0hhi"},{"name":"数据","_id":"cjlhgwh6f0038zkvodap05ev9"},{"name":"模型估计","_id":"cjlhgwh6k003gzkvoheu8yg98"},{"name":"正则化","_id":"cjlhgwh6m003izkvo5uxznh7r"},{"name":"现代诗","_id":"cjlhgwh6o003mzkvo2fvczaq1"},{"name":"神经网络","_id":"cjlhgwh6p003ozkvoqtvs8guo"}]}}