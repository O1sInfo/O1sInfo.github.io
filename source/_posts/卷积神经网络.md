---
title: 卷积神经网络
date: 2018-08-26 17:20:05
tags: CNN
categories: 深度学习
mathjax: true
---

## Convolution neural network(CNN)

是一种专门用来处理**具有类似网格结构的数据**的神经网络。例如时间序列数据(可以认为在时间轴上有规律的采样形成的一维网格)和图像数据(可以看作二维的像素网格)。

## 卷积运算

数学定义
$$f(t) = f_1(t) \ast f_2(t) = \int_{-\infty}^{\infty} f_1(\tau)f_2(t - \tau)d\tau$$

$$y(k) = f(k) \ast h(k) = \sum_{i = -\infty}^{\infty} f(i) h(k - i)$$

二维图像的卷积表示
$$S(i, j) = I(i, j) \ast K(i, j) = \sum_{m}\sum_{n}I(m, n)K(i - m, j - n)$$

神经网络中实现的卷积运算实际上是**互相关函数**
$$S(i, j) = I(i, j) \ast K(i, j) = \sum_{m}\sum_{n}I(i + m, j + n)K(m, n)$$

## 三个重要思想

### 稀疏交互(sparse interactions)

**在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。**

![](/images/dl_pic9_2.jpg)

传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中的每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而卷积网络具有稀疏交互的特征，这是使核的大小远小于输入的大小来达到的。当处理一张图像时，输入的图像可能包含成千上万个像素点，但我们可以通过只占用几十到几百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。

### 参数共享(parameter sharing)

特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。**即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。**

在传统的神经网络中，当计算一层的输出时，权重矩阵的每个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。在卷积神经网络中，核的每一个元素都作用在输入的每一个位置上。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对每一个位置都需要学习一个单独的参数集合。

![](/images/dl_pic9_5.jpg)

### 等变表示(equivarient representations)

等变的数学概念
$$如果函数f(x), g(x)满足 f(g(x)) = g(f(x)) 我们就说f(x)对于变换g具有等变性 $$

对于卷积来说，**如果令g是输入的任意平移函数，那么卷积函数对于g具有等变性。**
在图像处理中，卷积产生了一个二维映射来表明某些特征在输入中出现的位置。如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。

## 池化(pooling)

卷积网络中一个典型层包含三级
![](/images/dl_pic9_7.jpg)

**池化层**的作用是在卷积后很好地聚合了特征，通过降维来减少运算量, 缩减模型的大小，提高计算速度，同时减小噪声提高所提取特征的稳健性。

**池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。** 例如最大池化函数给出相邻区域内的最大值。

**不管采用什么样的池化函数，当输入做出少量平移时，池化能够帮助输入的表示近似不变**。局部平移不变性是一个很有用的性质，尤其当我们关心某个特征是否出现而不关心它出现的具体位置时。

在很多任务中，池化对于处理不同大小的输入具有重要作用。例如我们想对不同大小的图像进行分类时，分类层的输入必须是固定大小，而这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到相同数量的统计特征而不管最初的输入大小。例如最终的池化层可能会输入4组综合统计特征，每组对于着图像的一个象限。

## 卷积与池化作为一种无限强的先验

>先验概率分布。这是一个模型参数的概率分布，它刻画了我们在看到数据之前认为什么样的模型是合理的信念。先验被认为强或者弱取决于先验中概率密度的集中程度。一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值。

我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求那些处于隐藏单元的小的空间连续的接受域内的权重以外，其余权重都为零。

类似地使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。

## 填充(Padding)

假设输入图片的大小为 $n \times n$，而滤波器的大小为 $f \times f$，则卷积后的输出图片大小为 $(n-f+1) \times (n-f+1)$。

这样就有两个问题：

* 每次卷积运算后，输出图片的尺寸缩小；
* 原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息。

为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行 **填充（Padding）**，以增加矩阵的大小。通常将 0 作为填充值。

![](/images/Padding.jpg)

设每个方向扩展像素点数量为 $p$，则填充后原始图片的大小为 $(n+2p) \times (n+2p)$，滤波器大小保持 $f \times f$不变，则输出图片大小为 $(n+2p-f+1) \times (n+2p-f+1)$。

因此，在进行卷积运算时，我们有两种选择：

* **Valid 卷积**：不填充，直接卷积。结果大小为 $(n-f+1) \times (n-f+1)$；
* **Same 卷积**：进行填充，并使得卷积后结果大小与输入一致，这样 $p = \frac{f-1}{2}$。

在计算机视觉领域，$f$通常为奇数。原因包括 Same 卷积中 $p = \frac{f-1}{2}$ 能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。

## 卷积步长(Stride)

卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置 **步长（Stride）** 来压缩一部分信息。

步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：

![](/images/Stride.jpg)

设步长为 $s$，填充长度为 $p$，输入图片大小为 $n \times n$，滤波器大小为 $f \times f$，则卷积后图片的尺寸为：

$$\biggl\lfloor \frac{n+2p-f}{s}+1   \biggr\rfloor \times \biggl\lfloor \frac{n+2p-f}{s}+1 \biggr\rfloor$$

## 高维卷积

如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。

![](/images/Convolutions-on-RGB-image.png)

设输入图片的尺寸为 $n \times n \times n_c$（$n_c$为通道数），滤波器尺寸为 $f \times f \times n_c$，则卷积后的输出图片尺寸为 $(n-f+1) \times (n-f+1) \times n^{'}_c$，$n^{'}_c$为滤波器组的个数。

### 符号总结

设 $l$ 层为卷积层：

* $f^{[l]}$：**滤波器的高（或宽）**
* $p^{[l]}$：**填充长度**
* $s^{[l]}$：**步长**
* $n^{[l]}_c$：**滤波器组的数量**

* **输入维度**：$n^{[l-1]}_H \times n^{[l-1]}_W \times n^{[l-1]}_c$ 。其中 $n^{[l-1]}_H$表示输入图片的高，$n^{[l-1]}_W$表示输入图片的宽。之前的示例中输入图片的高和宽都相同，但是实际中也可能不同，因此加上下标予以区分。

* **输出维度**：$n^{[l]}_H \times n^{[l]}_W \times n^{[l]}_c$ 。其中

$$n^{[l]}_H = \biggl\lfloor \frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$

$$n^{[l]}_W = \biggl\lfloor \frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1   \biggr\rfloor$$

* **每个滤波器组的维度**：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c$ 。其中$n^{[l-1]}_c$ 为输入图片通道数（也称深度）。
* **权重维度**：$f^{[l]} \times f^{[l]} \times n^{[l-1]}_c \times n^{[l]}_c$
* **偏置维度**：$1 \times 1 \times 1 \times n^{[l]}_c$
