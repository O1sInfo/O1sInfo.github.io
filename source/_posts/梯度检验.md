---
title: 梯度检验
date: 2018-07-20 16:31:29
tags: 梯度检验
categories: 深度学习的实用层面
mathjax: true
---
## 梯度检验（Gradient checking）

### 梯度的数值逼近

使用双边误差的方法去逼近导数，精度要高于单边误差。

* 单边误差：

![one-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/one-sided-difference.png)

$$f'(\theta) = \lim\_{\varepsilon\to 0} = \frac{f(\theta + \varepsilon) - (\theta)}{\varepsilon}$$

误差：$O(\varepsilon)$

* 双边误差求导（即导数的定义）：

![two-sided-difference](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/two-sided-difference.png)

$$f'(\theta) = \lim\_{\varepsilon\to 0} = \frac{f(\theta + \varepsilon) - (\theta - \varepsilon)}{2\varepsilon}$$

误差：$O(\varepsilon^2)$

当 ε 越小时，结果越接近真实的导数，也就是梯度值。可以使用这种方法来判断反向传播进行梯度下降时，是否出现了错误。

### 梯度检验的实施

#### 连接参数

将 $W^{[1]}$，$b^{[1]}$，...，$W^{[L]}$，$b^{[L]}$全部连接出来，成为一个巨型向量 θ。这样，

$$J(W^{[1]}, b^{[1]}, ..., W^{[L]}，b^{[L]}) = J(\theta)$$

同时，对 $dW^{[1]}$，$db^{[1]}$，...，$dW^{[L]}$，$db^{[L]}$执行同样的操作得到巨型向量 dθ，它和 θ 有同样的维度。

![dictionary_to_vector](https://raw.githubusercontent.com/bighuang624/Andrew-Ng-Deep-Learning-notes/master/docs/Improving_Deep_Neural_Networks/dictionary_to_vector.png)

现在，我们需要找到 dθ 和代价函数 J 的梯度的关系。

#### 进行梯度检验

求得一个梯度逼近值

$$d\theta_{approx}[i] ＝ \frac{J(\theta\_1, \theta\_2, ..., \theta\_i+\varepsilon, ...) - J(\theta\_1, \theta\_2, ..., \theta\_i-\varepsilon, ...)}{2\varepsilon}$$

应该

$$\approx{d\theta[i]} = \frac{\partial J}{\partial \theta_i}$$

因此，我们用梯度检验值

$$\frac{||d\theta\_{approx} - d\theta||\_2}{||d\theta\_{approx}||\_2+||d\theta||\_2}$$

检验反向传播的实施是否正确。其中，

$${||x||}\_2 = \sum^N\_{i=1}{|x_i|}^2$$

表示向量 x 的 2-范数（也称“欧几里德范数”）。

如果梯度检验值和 ε 的值相近，说明神经网络的实施是正确的，否则要去检查代码是否存在 bug。

### 在神经网络实施梯度检验的实用技巧和注意事项

1. 不要在训练中使用梯度检验，它只用于调试（debug）。使用完毕关闭梯度检验的功能；
2. 如果算法的梯度检验失败，要检查所有项，并试着找出 bug，即确定哪个 dθapprox[i] 与 dθ 的值相差比较大；
3. 当成本函数包含正则项时，也需要带上正则项进行检验；
4. 梯度检验不能与 dropout 同时使用。因为每次迭代过程中，dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的成本函数 J。建议关闭 dropout，用梯度检验进行双重检查，确定在没有 dropout 的情况下算法正确，然后打开 dropout；