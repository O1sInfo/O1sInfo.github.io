---
title: Deep Residual Learning for Image Recognition
date: 2018-09-25 15:02:35
tags: ResNet
categories: 深度学习
mathjax: true
---
## 摘要

更深的神经网络往往更难以训练，我们提出一个 **残差学习框架**，使训练比以往更深的网络变得更加轻松。我们明确地将这些层重新规划为 **学习与层的输入有关的残差函数**，而不是学习不相关的函数。我们提供了非常全面的实验数据来证明，残差网络更容易优化，并且可以在深度增加的情况下让精度也增加。在ImageNet的数据集上我们评测了一个深度152层（是VGG的8倍）的残差网络，但依旧拥有比VGG更低的复杂度。这些残差网络在ImageNet测试集达到了3.57%的错误率，这个结果获得了ILSVRC2015的分类任务第一名，我们还用CIFAR-10数据集分析了100层和1000层的网络。

对于很多视觉识别任务来说，**表示的深度** 是至关重要的。我们极深的网络让我们在COCO的目标检测数据集上得到了28%的相对提升。同时，深度残差网络也是提交参加ILSVRC和COCO2015比赛的基础。我们还赢得了ImageNet目标检测、ImageNet目标定位、COCO目标检测和COCO图像分割等任务的第一名。

## 简介

深度卷积神经网络给图像分类问题上的研究带来了很多突破，深层网络自然地将 **低/中/高级的特征和分类器集成到端到端的多层方式中**，而特征的“层次”可以通过叠加层(深度)的数量来丰富。最近的证据表明，网络深度是至关重要的，而具有挑战性的ImageNet数据集的主要结果都利用了“非常深”的模型，深度为16到30。其他一些计算机视觉的问题也受益于非常深的网络模型。

受到深度的重要性的驱动，出现了这样一个问题：**学习更好的网络就像堆叠更多的层一样容易吗?** 这个问题的一大障碍就是臭名昭著的梯度消失/爆炸问题，它从网络结构的一开始就阻碍收敛。然而这个问题，很大程度上可以通过 **归一化的初始化** 和 **中间层的归一化** 解决，这个方法确保几十层的网络能够在使用 **随机梯度下降（SGD）** 的后向传播过程中收敛。

当更深的网络能够开始收敛时，一个 **退化问题** 就暴露出来了: **随着网络深度的增加，准确度就会饱和(这可能不足为奇)，然后就会迅速下降。出人意料的，退化问题并不是过拟合导致的，并且增加更多的层反而会导致更大的训练误差**，就像文章[11,42]中说的那样，通过我们的实验也得到证实。图1展示了一个典型的例子。

![](/images/resnet_f1.jpg)

训练精度的退化表明，不是所有的系统都同样容易优化。让我们考虑一个浅层网络和与之对应的增加了更多层的深层网络。有一个方法来构建该深层网络：**额外添加的层都是 恒等映射，其他层则是从训练好的浅层网络复制过来**。这种构造解的存在性表明，更深的网络应该不会比浅层网络产生更高的训练误差。但实验结果表明，我们手头上有的方案都找不到更好或者同样好的解（或者是无法在可接受的时间里做完）。

在本文中，我们通过引入一个 **深度残差学习** 框架解决这个退化问题。我们 **不期望每几个叠加的层直接拟合一个映射，而是明确的让这些层去拟合 残差映射**。更正式的说法是，这里用 $H(X)$ 来表示 **想要得到的潜在映射**，但我们让堆叠的非线性层去拟合另一个映射 $F(X):=H(X)-X$，此时原潜在映射 $H(X)$ 就可以改写成 $F(X)+X$，我们 **假设残差映射跟原映射相比更容易被优化**。现在我们考虑极端情况，如果恒等映射是最优解，那么可以 **相对于用堆叠的非线性层去拟合恒等映射将残差置0更容易些**。

$F(X)+X$ 的公式可以通过在前馈网络中做一个“**快捷连接**”来实现（如图2）。快捷连接跳过一个或多个层。在我们的例子中，快捷连接简单的执行恒等映射，它们的输出被添加到堆叠层的输出中。恒等映射快捷连接既不会增加额外的参数也不会增加计算复杂度。整个网络依然可以通过SGD和反向传播进行端到端训练，并且可以用常见的深度学习库来实现（比如Caffe）无需修改求解器。

![](/images/resnet_f2.PNG)

我们目前用ImageNet的数据集做了很多综合实验，来证实退化问题和评估我们的方法。我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差。2）我们的深度残差网络可以从增加的深度中轻松提高准确性，生成的结果实质上比以前的网络更好。

类似的现象在CIFAR-10数据集的实验中也一样，这表明了优化的困难以及我们的方法不是仅对特定的数据集起作用。我们在这个数据集上应用了成功训练的超过100层的模型，并探索了超过1000层的模型。

在ImageNet对象分类数据集上，我们用深度残差网络获得了很棒的结果，我们152层的残差网络是ImageNet的参赛网络中最深的，然而却拥有比VGG更低的复杂度。我们的模型集合在ImageNet测试集上有3.57% top-5的错误率，并在ILSVRC 2015分类比赛中获得了第一名。**极深的表示在其它识别任务中也有极好的泛化性能**，使我们进一步赢得了多个比赛的第一名包括ILSVRC & COCO 2015竞赛中的ImageNet检测，ImageNet目标定位，COCO目标检测和COCO图像分割，坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。

## 相关工作

**残差表示**。VLAD是一种通过关于字典的残差向量进行编码的表示形式。**Fisher矢量** 可以认为是VLAD的概率版本。它们都是图像检索和图像分类中强大的浅层表示。对于矢量量化，**编码残差矢量被证明比编码原始矢量更有效**。

在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的 **Multigrid方法** 将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。Multigrid的替代方法是 **层次化基础预处理**，它依赖于表示两个尺度之间残差向量的变量。[3,45,46]已经证明比起不知道解的残差性质的标准求解器，这些求解器收敛得更快。这些方法表明好的重构或预处理可以简化优化过程。

**快捷连接**。快捷连接的实践和理论已经被研究了很长时间。训练多层感知机（MLPs）的早期实践是添加一个线性层来连接网络的输入和输出。在[44,24]中，一些中间层直接连接到辅助分类器，用于解决梯度消失/爆炸问题。论文[39,38,31,47]提出了通过快捷连接实现层间响应，梯度和传播误差的方法。在[44]中，一个“inception”层由一个快捷分支和一些更深的分支组成。

和我们同时进行的工作，高速路网络提出了门控函数的快捷连接。这些门依赖数据且有参数，与我们没有参数的恒等快捷连接相反。当门控快捷连接“关闭”（接近零）时，高速路网络中的层表示非残差函数。相反，我们的公式总是学习残差函数；我们的恒等快捷连接永远不会关闭，所有的信息总是通过，还有额外的残差函数要学习。此外，高速路网络还没有证实极度增加的深度（例如，超过100个层）能够提高准确性。

## 深度残差学习

### 残差学习

我们考虑 $H(x)$ 作为几个堆叠层（不必是整个网络）要拟合的潜在映射，$x$ 表示这些层中第一层的输入。假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即 $H(x)−x$ (假设输入输出维度相同)。因此，我们明确让这些层近似残差函数 $F(x):= H(x)−x$，而不是期望堆叠层近似 $H(x)$。因此原始函数变为 $F(x)+x$。尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度可能是不同的。

这种重构是受到了反直觉的退化问题的激发（图1左）。正如我们在第一部分介绍中讨论的那样，如果添加的层可以被构建为恒等映射，更深模型的训练误差应该不大于它对应的更浅版本。**退化问题表明求解器通过多个非线性层来近似恒等映射可能存在困难**。通过残差学习的重构，如果恒等映射是最优解，求解器可能简单地将多个非线性层的权重置零使其得到恒等映射。

在实际情况下，恒等映射不太可能是最优解，但是我们的重构可能有助于对问题进行预处理。**如果最优函数比零映射更接近于恒等映射，则求解器应该更容易找到关于恒等映射的扰动，而不是将该函数作为新函数来学习**。我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。

![](/images/resnet_f7.PNG)

### 快捷恒等映射

我们每隔几个堆叠层采用残差学习。一个构建块如图2所示。在本文中我们考虑构建块正式定义为：

$y=F(x, \{W_i\})+x \tag{1}$

$x$和$y$是考虑的层的输入和输出向量。函数 $F(x,\{W_i\})$ 表示要学习的残差映射。对于图2中的有两层的例子来说，$F = W_2 \sigma (W_1x)$，其中 $\sigma$ 表示ReLU，为了简化表达忽略偏置项。$F+x$ 操作通过快捷连接和各个元素相加来执行。在相加之后我们采取了第二个非线性 $\sigma(y)$（图2）。

方程(1)中的快捷连接既没有引入额外参数又没有增加计算复杂度。这不仅在实践中有吸引力，而且在简单网络和残差网络的比较中也很重要。我们可以公平地比较同时具有相同数量的参数，相同深度，宽度和计算成本的简单/残差网络（除了不可忽略的元素加法之外）。

方程(1)中$x$和$F$的维度必须是相等的。如果不是这种情况（例如，当更改输入/输出通道时），我们可以通过快捷连接执行线性投影$W_s$来匹配维度：

$y=F(x, \{W_i\})+W_sx \tag{2}$

我们也可以在方程(1)中使用方阵$W_s$。但是我们将通过实验表明，恒等映射足以解决退化问题，并且是合算的，因此$W_s$仅在匹配维度时使用。

残差函数$F$的形式是可变的。本文中的实验包括有两层或三层（图5）的函数$F$，也可以有更多的层。但如果$F$只有一层，方程(1)类似于线性层：$y=W_1x+x$，这就没有优势了。

![](/images/resnet_f5.PNG)

我们还注意到尽管上述表达式是关于全连接层的，但它们同样适用于卷积层。函数 $F(x，W_i)$ 可以表示多个卷积层。元素加法在两个特征图上逐通道进行。

### 网络架构

我们测试了各种简单/残差网络，并观察到了一致的现象。为了给讨论举例，我们描述ImageNet的两个模型如下。

![](/images/resnet_f3.jpg)

简单网络。 我们简单网络的基准（图3，中间）主要受到VGG网络（图3，左图）的设计哲学启发。卷积层主要有3×3的滤波器，并遵循两个简单的设计规则：（i）对于相同的输出特征图尺寸，层具有相同数量的滤波器；（ii）如果特征图尺寸减半，则滤波器数量加倍，以便保持每层的时间复杂度。我们通过步长为2的卷积层直接执行下采样。网络以全局平均池化层和具有softmax的1000维全连接层结束。图3（中间）的权重层总数为34。

值得注意的是我们的模型与VGG网络（图3左）相比，有更少的滤波器和更低的复杂度。我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。

残差网络。基于上述的简单网络，我们插入快捷连接（图3，右），将网络转换为其对应的残差版本。当输入和输出具有相同的维度时（图3中的实线快捷连接）时，可以直接使用恒等快捷连接（方程（1））。当维度增加（图3中的虚线快捷连接）时，我们考虑两个选项：（A）快捷连接仍然执行恒等映射，额外填充零输入以增加维度。此选项不会引入额外的参数；（B）方程（2）中的投影快捷连接用于匹配维度（由1×1卷积完成）。对于这两个选项，当快捷连接跨越两种尺寸的特征图时，它们执行时步长为2。

![](/images/resnet_t1.PNG)
### 实现

ImageNet中我们的实现遵循[21，41]的实践。调整图像大小，其较短的边在[256,480]之间进行随机采样，用于尺度增强。使用224×224裁剪从图像或其水平翻转中随机采样，并逐像素减去均值。使用了论文[21]中的标准颜色增强。在每个卷积之后和激活之前，我们采用批量归一化（BN）[16]。我们按照[13]的方法初始化权重，从零开始训练所有的简单/残差网络。我们使用批大小为256的SGD方法。学习速度从0.1开始，当误差稳定时学习率除以10，并且模型训练高达$60×10^4$次迭代。我们使用的权重衰减为0.0001，动量为0.9。根据[16]的实践，我们不使用dropout算法。

在测试阶段，为了比较研究，我们采用标准的10-crop测试[21]。对于最好的结果，我们采用如[41, 13]中的全卷积形式，并在多尺度上对分数进行平均（对图像尺寸缩放，使短边范围在{224, 256, 384, 480, 640}中）。
