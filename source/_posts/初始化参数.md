---
title: 初始化参数
date: 2018-07-22 13:22:45
tags: 初始化参数
categories: 深度学习的实用层面
---

## Initialization

Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  

A well chosen initialization can:
- Speed up the convergence of gradient descent
- Increase the odds of gradient descent converging to a lower training (and generalization) error 

## Random initialization

```python
parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 0.01
parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
```

## He initialization

```python
parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])
parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
```


**What you should remember from this artical**:
- Different initializations lead to different results
- Random initialization is used to break symmetry and make sure different hidden units can learn different things
- Don't intialize to values that are too large
- He initialization works well for networks with ReLU activations. 